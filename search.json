[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "audio-algebra",
    "section": "",
    "text": "Experimental research code exploring MIR / signal processing tasks with latent audio representations (“embeddings”)"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "audio-algebra",
    "section": "Install",
    "text": "Install\npip install git+https://github.com/drscotthawley/audio-algebra.git\n(Do not rely on out-of-date binaries on PyPI/conda.)"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "audio-algebra",
    "section": "How to use",
    "text": "How to use\nApart from the “Destructo” demo notebook, most of this repository is still under very active development / work in progress. So, for now only “Destructo” is usable; refer to it directly for instructions."
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "datasets",
    "section": "",
    "text": "This is patterned after and relies upon aeiou.datasets, but includes some differences such as the use of audiomentations and probably Pedalboard sometime.\nEventually changes from this file will be merged into aeiou.datasets. …But not today!"
  },
  {
    "objectID": "datasets.html#testing-dualeffectsdataset",
    "href": "datasets.html#testing-dualeffectsdataset",
    "title": "datasets",
    "section": "Testing DualEffectsDataset",
    "text": "Testing DualEffectsDataset\nQuick checks to catch minor errors and explore\n(note that CI will not execute the following cells)\n\ndata_path = '../aeiou/examples/'\ndataset = DualEffectsDataset(data_path)\ndata = dataset.__getitem__(0)\nprint(data)\n\naugs = Stereo(), PhaseFlipper()\neffects_list =  ['Gain', 'BandPassFilter', 'BandStopFilter', 'HighPassFilter', 'LowPassFilter']\nAudioDataset:2 files found.\n{'a': tensor([[-0.0661, -0.0648, -0.0633,  ...,  0.0558,  0.0524,  0.0495],\n        [-0.0034, -0.0034, -0.0034,  ..., -0.0239, -0.0192, -0.0166]]), 'b': tensor([[ 0.0185,  0.0073, -0.0046,  ..., -0.0108, -0.0156, -0.0141],\n        [-0.0403, -0.0522, -0.0583,  ..., -0.0241, -0.0216, -0.0179]]), 'a1': tensor([[-0.0661, -0.0661, -0.0660,  ...,  0.0689,  0.0668,  0.0644],\n        [-0.0034, -0.0034, -0.0034,  ..., -0.0514, -0.0458, -0.0401]]), 'b1': tensor([[ 0.0185,  0.0184,  0.0177,  ..., -0.0062, -0.0065, -0.0070],\n        [-0.0403, -0.0405, -0.0411,  ..., -0.0140, -0.0170, -0.0193]]), 'a2': tensor([[-0.0661, -0.0651, -0.0645,  ...,  0.0875,  0.0861,  0.0853],\n        [-0.0034, -0.0034, -0.0034,  ..., -0.0180, -0.0190, -0.0212]]), 'b2': tensor([[ 0.0185,  0.0081, -0.0016,  ..., -0.0053, -0.0090, -0.0063],\n        [-0.0403, -0.0514, -0.0555,  ..., -0.0057, -0.0005,  0.0051]]), 'e1': 'LowPassFilter', 'e2': 'BandStopFilter'}\n\n\nTest how the DataLoader behaves in dict pipeline mode:\n\ndataset = DualEffectsDataset(data_path)\ntrain_dl = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\nbatch = next(iter(train_dl))\nprint(\"batch =\\n\",batch)\n\naugs = Stereo(), PhaseFlipper()\neffects_list =  ['Gain', 'BandPassFilter', 'BandStopFilter', 'HighPassFilter', 'LowPassFilter']\nAudioDataset:2 files found.\nbatch =\n {'a': tensor([[[-3.0239e-04, -3.8517e-04, -6.0043e-04,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [-3.0239e-04, -3.8517e-04, -6.0043e-04,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00]],\n\n        [[ 2.9149e-01,  2.2990e-01,  1.7710e-01,  ..., -2.4063e-02,\n          -2.3992e-02, -2.1247e-02],\n         [ 1.1003e-04,  1.6797e-04,  1.3461e-04,  ..., -5.7370e-03,\n          -5.6048e-03, -5.5120e-03]]]), 'b': tensor([[[-0.0003, -0.0004, -0.0006,  ...,  0.0000,  0.0000,  0.0000],\n         [-0.0003, -0.0004, -0.0006,  ...,  0.0000,  0.0000,  0.0000]],\n\n        [[ 0.1027,  0.0918,  0.0796,  ..., -0.0584, -0.0534, -0.0412],\n         [ 0.0017, -0.0006, -0.0032,  ...,  0.1276,  0.1195,  0.1094]]]), 'a1': tensor([[[-3.0239e-04, -3.5309e-04, -4.4016e-04,  ..., -0.0000e+00,\n          -0.0000e+00, -0.0000e+00],\n         [-3.0239e-04, -3.5309e-04, -4.4016e-04,  ..., -0.0000e+00,\n          -0.0000e+00, -0.0000e+00]],\n\n        [[ 2.9149e-01,  2.4293e-01,  2.2399e-01,  ..., -4.3179e-02,\n          -4.1475e-02, -3.8351e-02],\n         [ 1.1003e-04,  1.5571e-04,  1.0806e-04,  ..., -5.5205e-03,\n          -5.4740e-03, -5.4839e-03]]]), 'b1': tensor([[[-0.0003, -0.0004, -0.0004,  ...,  0.0000,  0.0000,  0.0000],\n         [-0.0003, -0.0004, -0.0004,  ...,  0.0000,  0.0000,  0.0000]],\n\n        [[ 0.1027,  0.0933,  0.0856,  ..., -0.0665, -0.0634, -0.0530],\n         [ 0.0017, -0.0003, -0.0020,  ..., -0.0320, -0.0415, -0.0496]]]), 'a2': tensor([[[-1.3810e-04, -1.7590e-04, -2.7421e-04,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [-1.3810e-04, -1.7590e-04, -2.7421e-04,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00]],\n\n        [[ 1.1102e-16, -5.3172e-02, -8.3176e-02,  ..., -7.6607e-03,\n          -5.7607e-03, -1.7675e-03],\n         [ 2.7105e-20,  5.0022e-05,  6.5531e-06,  ...,  1.7718e-04,\n           2.2987e-04,  2.2429e-04]]]), 'b2': tensor([[[-2.1308e-04, -2.7141e-04, -4.2310e-04,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00],\n         [-2.1308e-04, -2.7141e-04, -4.2310e-04,  ...,  0.0000e+00,\n           0.0000e+00,  0.0000e+00]],\n\n        [[-6.9389e-17, -1.0133e-02, -1.9777e-02,  ..., -5.8415e-02,\n          -6.6108e-02, -6.5935e-02],\n         [-1.0842e-18, -2.1146e-03, -4.1841e-03,  ...,  4.9976e-03,\n          -6.5876e-03, -1.8445e-02]]]), 'e1': ['BandStopFilter', 'BandStopFilter'], 'e2': ['Gain', 'HighPassFilter']}\n\n\n\nbatch = next(iter(train_dl))\na,b, a1,b1, a2, b2, e1, e2 = batch.values()\nprint(\"clean\")\nplayable_spectrogram(a[0], output_type='live')\n\n\nprint(e1[0])\nplayable_spectrogram(a1[0], output_type='live')\n\n\nprint(e2[0])\nplayable_spectrogram(a2[0], output_type='live')\n\n\ndiff = a2[0] - a1[0]\nplayable_spectrogram(diff, output_type='live')"
  },
  {
    "objectID": "aa-mixer-toy.html",
    "href": "aa-mixer-toy.html",
    "title": "aa_mixer_toy",
    "section": "",
    "text": "NOTE: to view this notebook properly, open it via NBViewer or Colab. GitHub won’t show you everything.\nWe stick an autoencoder in the middle of some “given” encoder(-decoder) system, and we’ll try to get our new system to preserve linearity with respect to the (original) inputs."
  },
  {
    "objectID": "aa-mixer-toy.html#basic-idea-terminology-and-goals",
    "href": "aa-mixer-toy.html#basic-idea-terminology-and-goals",
    "title": "aa_mixer_toy",
    "section": "Basic idea, Terminology, and Goals",
    "text": "Basic idea, Terminology, and Goals\nBasic Idea: Inputs \\(x\\) -> Given Model encodings \\(y\\) -> AA embeddings \\(z\\)\nFlowchart: \n\nTerminology\n\nencoder: The “given” map \\(f_\\theta(x) : x\\mapsto y\\) is called the “(given) encoder”.\nstem: The inputs \\(x_i\\) are also called “stems”\ngiven encodings/representations: The encodings \\(y_i\\) are called “(given) representations” or encodings. …sometimes embeddings but we’re going to save that for the \\(z_i\\).\nmix: A sum of stems is called a “mix”, e.g. mix = \\(x_1 + x_2\\)\nfaders: Any scalar coefficients applied to stems are called “faders”. So in the mix \\(a_1 x_1 + a_2 x_2\\) for scalar \\(a_i\\), the \\(a_i\\)’s are faders.\na-a model: Our new “audio algebra” map \\(h_\\phi(y): y\\mapsto z\\) is sometimes called the “projector” or “expander” in the literature. We’ll refer to to as the “a-a model”. ;-)\nembeddings: The mapped values \\(z_i\\) are called “embeddings”\ngiven decoder/inverse: The given encoder \\(f_\\theta\\) may or may not have an inverse or “decoder” \\(f^{-1}\\), but since we’re only interested in systems that can re-synthesize things in the input space (e.g., audio, images, etc), we’ll assume some kind of decoder exists, even if it’s not a true inverse (e.g. maybe it’s some kind of generative model).\n\n\n\nGoals\nWe want this new “audio algebra” map \\(h\\) to have two properties, which we’ll write as loss functions: 1. “Mix Loss”: Our similarity condition will be to make it so “the embedding of the mix equals the sum of the embeddings”, that is to say \\[ h(f(x_1 + x_2)) = h(f(x_1) + h(f(x_2) \\] i.e. \\[ \"zmix = zsum\" \\] So the mix loss will be\nmix_loss = MSE( zmix - zsum ).\n\n“(AA) Reconstruction Loss”: We also want the a-a “projector” model \\(h_\\phi\\) to be (approximately) invertible), i.e. we want \\(h^{-1}(z): z\\mapsto y\\) to exist. So for both the stems and the mix, we use an MSE loss on the reconstruction of the given representations \\(y\\). This the aa recon loss will be\n\naa_recon_loss = MSE ( ymix - y_recon ) + MSE( y_i - y_i_recon ),\nwhere the “recon” variables are in terms of \\(h^{-1}\\) applied to the z’s, e.g. ymix_recon := \\(h^{-1}\\)(zmix) ).\n\n\nChallenge: Avoiding Collapse\n…the challenge is that the “mix loss” leads to “collapse”: all the points either constrict to the origin, or towards some constant vector. We’ll use the VICReg method to fix these problems. More on VICReg further below.\n\n\nBut this is a toy model\nInstead of real audio, in order to keep things simple and visualize-able, we’ll just work in terms of 2D data points. The real problem we want so solve eventually involves huge numbers of dimensions that we can’t visualize. The hope is to check our work and understanding and gain intuition using this simple “toy model”.\n\n# install a few non-standard dependencies (for colab)\n\nNote: you may need to restart the kernel to use updated packages.\n\n\nGet/set a few “environment” related variables\n\ndevice = get_device()\nprint(\"device = \",device)\n\n    \nSCRATCH_DIR = \"/scratch\" if os.path.isdir('/scratch') else \"/tmp\"\nSCRATCH_DIR += '/aa'  # give audio alg scratch its own spot\nif not os.path.exists(SCRATCH_DIR):\n    os.makedirs(SCRATCH_DIR)\nprint(\"SCRATCH_DIR =\",SCRATCH_DIR)\n\ndevice =  cuda\nSCRATCH_DIR = /tmp/aa\n\n\n\n# generate a unique 2-character code for the run\nsuffix_len = 2\nRUN_SUFFIX = '_'+''.join(random.choices(string.ascii_lowercase, k=suffix_len))\nprint(\"RUN_SUFFIX =\",RUN_SUFFIX)\n\nRUN_SUFFIX = _ao"
  },
  {
    "objectID": "aa-mixer-toy.html#make-up-some-data",
    "href": "aa-mixer-toy.html#make-up-some-data",
    "title": "aa_mixer_toy",
    "section": "Make up some data",
    "text": "Make up some data\nFor starters, just a bunch of random numbers\n\nseed = 5   \ntorch.manual_seed(seed)\nbatch_size = 1024\nn_train_points, train_val_split  = batch_size*2000,  0.8 # we're going to grab random data anyway\nn_points = int(n_train_points*1.25) \nin_dims = 2 # number of dimensions the input data will live in, e.g. 2 or 3\nemb_dims = 2 # number of dimensions for embeddings \n\ntrain_val_split = 0.8 \ntrain_len, val_len = round(n_points*train_val_split), round(n_points*(1-train_val_split))\n\nclass RandVecDataset(torchdata.Dataset):\n    \"very simple dataset\"\n    def __init__(self, length, dims):\n        super().__init__()\n        self.data = 2*torch.rand(length, dims) - 1 \n        \n    def __getitem__(self, idx):\n        return self.data[idx]                                               \n        \n    def __len__(self): \n         return self.data.shape[0]  \n                                                        \ntrain_dataset = RandVecDataset(train_len, in_dims)\nval_dataset = RandVecDataset(val_len, in_dims)\nv = train_dataset.__getitem__(0)\nprint(train_dataset.__len__(), v.shape)\nprint(val_dataset.__len__(), v.shape)\n\n2048000 torch.Size([2])\n512000 torch.Size([2])\n\n\n\ntrain_dl = torchdata.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dl = torchdata.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\nval_iter = iter(val_dl)\n\nDEMO BATCH: Vizualization demo batch (same each time)\n\n# plot what the demo batch looks like in input space -- you're going to be seeing this alot!\nfig, ax = plt.subplots(1,3, figsize=(12,4))\n\ndemo_batch = get_demo_batch(val_iter, debug=True)\nax[0].plot(demo_batch[:,0], demo_batch[:,1], marker='.', linestyle='None')\nax[0].set_title('single \"lines+dots\" stem')\nprint(\"\")\n\ndemo_mix = demo_batch + get_demo_batch(val_iter, demo_line=True, debug=True) # add another 'line stem'\nax[1].plot(demo_mix[:,0], demo_mix[:,1], marker='.', linestyle='None')\nax[1].set_title('mix of two \"lines+dots\" stems')\n\ndemo_mix = demo_batch + get_demo_batch(val_iter, demo_line=False, debug=True) # random noise will swamp lines\nax[2].plot(demo_mix[:,0], demo_mix[:,1], marker='.', linestyle='None')\nax[2].set_title('mix of \"lines+dots\" + unit noise')\n\n\n\n\nText(0.5, 1.0, 'mix of \"lines+dots\" + unit noise')\n\n\n\n\n\n…so in general our mix of random numbers – even if one of the stems has a nice pattern – will just be a bunch of random numbers.\nWe’ll use the far right for “demo_mix” since that’s what it’ll typically look like, whereas the middle graph is contrived."
  },
  {
    "objectID": "aa-mixer-toy.html#generic-building-blocks",
    "href": "aa-mixer-toy.html#generic-building-blocks",
    "title": "aa_mixer_toy",
    "section": "Generic Building Blocks",
    "text": "Generic Building Blocks\n\n# test that\nemb_test = EmbedBlock(2, 2, use_bn=True, requires_grad=False)\n#res_test = ResBlock(2, 2)"
  },
  {
    "objectID": "aa-mixer-toy.html#the-given-autoencoder-f_thetax-x-mapsto-y",
    "href": "aa-mixer-toy.html#the-given-autoencoder-f_thetax-x-mapsto-y",
    "title": "aa_mixer_toy",
    "section": "The Given (Auto)Encoder: \\(f_\\theta(x): x \\mapsto y\\)",
    "text": "The Given (Auto)Encoder: \\(f_\\theta(x): x \\mapsto y\\)\nThis is a stand-in for whatever the main encoder is to be, i.e. for which the audio-algebra is going be inserted in the middle of.\nThis could be an actual audio encoder or,…just something random.\nNow, for word embeddings these are typically just weights from a linear transformation, but we’re going to assume that there’s maybe some set of nonlinear tranformations that led us to this point. I made up a twisty nonlinear model for the toy model to use.\nOh, first I’m going to define a couple nonlinearities that might be used in the model.\n…and we’ll just quickly visualize them.\n\nx = torch.linspace(-2.5,2.5,500)\nfig, ax = plt.subplots(figsize=(9,4))\nax.plot(x.cpu().numpy(), torch.tanh(3*x).cpu().numpy(), label='tanh')\nax.plot(x.cpu().numpy(), friendly_tanh(x,a=3).cpu().numpy(), label='friendly_tanh')\nax.plot(x.cpu().numpy(), compressor(x).cpu().numpy(), label='\"compressor\"')\nax.legend()\n\n<matplotlib.legend.Legend>"
  },
  {
    "objectID": "aa-mixer-toy.html#visualize-effects-of-given_model.encode-on-inputs",
    "href": "aa-mixer-toy.html#visualize-effects-of-given_model.encode-on-inputs",
    "title": "aa_mixer_toy",
    "section": "Visualize Effects of given_model.encode on Inputs",
    "text": "Visualize Effects of given_model.encode on Inputs\n\ntorch.manual_seed(seed)\n#given_model = SimpleAutoEncoder(in_dims=in_dims, emb_dims=emb_dims).to(device)\ngiven_model = TwistAndScrunch()\n\ngiven_model = given_model.to(device)\n\n\nprint(\"for single stem:\")\nviz_given_batch(demo_batch, given_model, debug=True)\n\nfor single stem:\nabsmax of val_batch =  0.9996815919876099\nabsmax of emb_batch =  1.141883134841919\n\n\n\n\n\n\nprint(\"Also operate on the demo mix:\")\nviz_given_batch(demo_mix, given_model, debug=True)\n\nAlso operate on the demo mix:\nabsmax of val_batch =  1.9301079511642456\nabsmax of emb_batch =  1.2925496101379395"
  },
  {
    "objectID": "aa-mixer-toy.html#train-aa_model-with-vicreg",
    "href": "aa-mixer-toy.html#train-aa_model-with-vicreg",
    "title": "aa_mixer_toy",
    "section": "Train aa_model with VICReg",
    "text": "Train aa_model with VICReg\n\nwandb.login()\n\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\nwandb: You can find your API key in your browser here: https://wandb.ai/authorize\nwandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n\n\n ········\n\n\nwandb: Appending key for api.wandb.ai to your netrc file: /home/ec2-user/.netrc\n\n\nTrue\n\n\n\nif train_new_aa:\n    train_aa_vicreg()\n\nwandb: Currently logged in as: drscotthawley. Use `wandb login --relogin` to force relogin\n\n\ntotal_steps = 80000\n\n\nTracking run with wandb version 0.13.7\n\n\nRun data is saved locally in /home/ec2-user/SageMaker/audio-algebra/wandb/run-20230104_095749-2khoulne\n\n\nSyncing run likely-sea-52 to Weights & Biases (docs)\n\n\nwandb: WARNING Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n\n\nNew run name = h64_bs1024_lr0.002_ao\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s see how we did…\n\ntorch.manual_seed(seed)\ndemo_stems, demo_faders, val_iter = get_stems_faders(demo_batch, val_iter, val_dataset, maxstems=2)\ndemo_zsum, demo_zmix, demo_archive = do_mixing(demo_stems, demo_faders, given_model, aa_model, device)\nviz_aa_demo(demo_zsum, demo_zmix, demo_archive, aa_model)\n\n\n\n\n^^ If the green markers in the bottom right and bottom middle panels are covered up, then we’re good!\nBTW, if you want to make a movie of those images over the course of training, see the instructions by Scott Condon of Weights and Biases, except you should use this here Colab link to my modification of his code. Here’s a movie I made:\n\nfrom IPython.display import Video  \nvideo_url = \"https://hedges.belmont.edu/aa_1280.mp4\"\nVideo(video_url, width=720)\n# NOTE that GitHub won't display this video; you have to execute the above code\n\n\n      Your browser does not support the video element."
  },
  {
    "objectID": "aa-mixer-toy.html#algebra-1-king---man-woman-queen",
    "href": "aa-mixer-toy.html#algebra-1-king---man-woman-queen",
    "title": "aa_mixer_toy",
    "section": "Algebra #1: “king - man + woman = queen”",
    "text": "Algebra #1: “king - man + woman = queen”\nok, first some utility routines…\n\n“Forward”: Do math with x’s, then map to z’s. Guess and check?\n\n# vectors in input space\nxs = 2*torch.rand(4,2, requires_grad=False)-1 # \nxs[3] = xs[1] - xs[0] + xs[2]  # queen = king - man + woman\n\nplot_kmwq_demo(xs)\n\norange dot shows the 'guess'\n\n\n\n\n\n\n\n“Backward” summation\n\n# vectors in input space\nzs = 2*torch.rand(4,2, requires_grad=False)-1 \nzs[3] = zs[1] - zs[0] + zs[2] \nplot_kmwq_demo(zs, forward=False)\n\norange dot shows the 'guess'"
  },
  {
    "objectID": "aa-mixer-toy.html#algebra-2-demix-removing-things-from-a-mix",
    "href": "aa-mixer-toy.html#algebra-2-demix-removing-things-from-a-mix",
    "title": "aa_mixer_toy",
    "section": "Algebra #2: “Demix” removing things from a mix",
    "text": "Algebra #2: “Demix” removing things from a mix\nThis is kind of the same thing as we just did but more of a (theoretical) “music” example.\nPretend we’re mixing 4 stems. But then we want to remove the last one. We can do the math for that in the input space. But can we also do it in the aa embedding space?\n\ndef demix_fun(labels = ['guitar','drums','bass','vocal']):\n    i_rm = random.randint(0,len(labels)-1)\n\n    # input space\n    xs = 2*torch.rand(4,2, requires_grad=False) -1\n    mix = xs.sum(dim=0).unsqueeze(0)\n    mix_rm = mix - xs[i_rm,:]  # subtract a stem in input space\n    print(f\"mix_remove_{labels[i_rm]} = {mix_rm}\")\n\n\n    # do the op in embedding space\n    with torch.no_grad():\n        zs   = aa_model.encode( given_model.encode(xs.to(device)) )\n        zmix = aa_model.encode( given_model.encode(mix.to(device)) )\n\n        zmix_rm = zmix - zs[i_rm,:]  # subtract a stem in aa embedding space!\n        guess = given_model.decode( aa_model.decode(zmix_rm) ).cpu()  # convert to input space\n        print(f\"guess =          {guess}\")\n\n\n    #plot what we got, in input space \n    fig, ax = plt.subplots(figsize=(7,7))\n\n    ax.text(0,0 ,f\"origin\")\n    for i in range(4):         # plot all the stems and put labels on them \n        ax.arrow(0,0, xs[i,0], xs[i,1], length_includes_head=True, head_width=0.05, head_length=0.1)  # plot vectors for stems\n        ax.annotate(\"\", xy=(xs[i,0], xs[i,1]), xytext=(0, 0), arrowprops=dict(arrowstyle=\"->\"))\n        ax.text(xs[i,0],xs[i,1],f\"{labels[i]}\")\n\n    plt.plot(mix[:,0],mix[:,1], marker='o', markersize=10)  # also plot and label the mix\n    ax.text(mix[:,0],mix[:,1], \"mix\")\n\n    # connect mix and removed value\n    dx = mix_rm - mix \n    ax.arrow(mix[0][0], mix[0][1], dx[0][0], dx[0][1], length_includes_head=True, head_width=0.05, head_length=0.1)  \n    ax.annotate(\"\", xy=(xs[i,0], xs[i,1]), xytext=(0, 0), arrowprops=dict(arrowstyle=\"->\"))\n\n\n    ax.plot(mix_rm[:,0],mix_rm[:,1], marker='o', label=f'mix_remove_{labels[i_rm]}', linestyle='None', markersize=10) # plot the point in real space\n    ax.text(mix_rm[:,0],mix_rm[:,1], f'mix_remove_{labels[i_rm]}')\n\n\n    ax.plot(guess[:,0],guess[:,1], marker='o', label='guess from aa', linestyle='None', markersize=10) # guess = input recon of what was done in aa emb space\n\n    plt.axis('square')\n    ax.set_aspect('equal', adjustable='box')\n    ax.set_title(\"input space\")\n    ax.legend(fancybox=True, framealpha=0.6, prop={'size': 10})\n    plt.show()\n    \ndemix_fun()\n\nmix_remove_drums = tensor([[0.4061, 0.8386]])\nguess =          tensor([[0.4127, 0.8588]])\n\n\n\n\n\nIf the green dot is covering the orange dot, then it worked!"
  },
  {
    "objectID": "aa-mixer.html",
    "href": "aa-mixer.html",
    "title": "aa_mixer",
    "section": "",
    "text": "install = False  # can set to false to skip this part, e.g. for re-running in same session\nif install:     # ffmpeg is to add MP3 support to Colab\n    !yes | sudo apt install ffmpeg \n    !pip install -Uqq einops gdown \n    !pip install -Uqq git+https://github.com/drscotthawley/aeiou\n    !pip install -Uqq git+https://github.com/drscotthawley/audio-algebra"
  },
  {
    "objectID": "aa-mixer.html#set-up-the-given-autoencoder-models",
    "href": "aa-mixer.html#set-up-the-given-autoencoder-models",
    "title": "aa_mixer",
    "section": "Set up the Given [Auto]Encoder Model(s)",
    "text": "Set up the Given [Auto]Encoder Model(s)\nNote that initially we’re only going to be using the encoder part. The decoder – with all of its sampling code, etc. – will be useful eventualy, and we’ go ahead and define it. But fyi it won’t be used at all while training the AA mixer model.\nDownload the checkpoint file for the dvae\n\non_colab = os.path.exists('/content')\nif on_colab:\n    from google.colab import drive\n    drive.mount('/content/drive/') \n    ckpt_file = '/content/drive/MyDrive/AI/checkpoints/epoch=53-step=200000.ckpt'\nelse:\n    ckpt_file = 'checkpoint.ckpt'\n    if not os.path.exists(ckpt_file):\n        url = 'https://drive.google.com/file/d/1C3NMdQlmOcArGt1KL7pH32KtXVCOfXKr/view?usp=sharing'\n        # downloading large files from GDrive requires special treatment to bypass the dialog button it wants to throw up\n        id = url.split('/')[-2]\n        cmd = f'wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \\'https://docs.google.com/uc?export=download&id={id}\\' -O- | sed -rn \\'s/.*confirm=([0-9A-Za-z_]+).*/\\1\\\\n/p\\')&id={id}\" -O {ckpt_file} && rm -rf /tmp/cookies.txt'\n        print(\"cmd = \\n\",cmd)\n        subprocess.run(cmd, shell=True, check=True)\n\n\ngiven_model = DiffusionDVAE.load_from_checkpoint(ckpt_file, global_args=global_args)\ngiven_model.eval() # disable randomness, dropout, etc...\n\n# attach some arg values to the model \ngiven_model.demo_samples = global_args.sample_size \ngiven_model.quantized = global_args.num_quantizers > 0\ngiven_model.to(device)\nfreeze(given_model)  # freeze the weights for inference\nprint(\"Given Autoencoder is ready to go!\")"
  },
  {
    "objectID": "aa-mixer.html#the-aa-mixer-model",
    "href": "aa-mixer.html#the-aa-mixer-model",
    "title": "aa_mixer",
    "section": "The AA-mixer model",
    "text": "The AA-mixer model\nTest that:\n\nbatch = next(train_iter) \nstems, faders, val_iter = get_stems_faders(batch, train_iter, train_dl, maxstems=2)\nprint(\"len(faders) = \",len(faders))\n\n# artificially max out these stems! \nfor i in range(len(faders)):\n    faders[i] = 1/torch.abs(stems[i][0]).max() \n\nplayable_spectrogram( stems[0][0]*faders[0], output_type='live')  #  this is just the batch\n\n\nplayable_spectrogram( stems[1][0]*faders[1], output_type='live')  # thisis something new"
  },
  {
    "objectID": "aa-mixer.html#mix-and-apply-models",
    "href": "aa-mixer.html#mix-and-apply-models",
    "title": "aa_mixer",
    "section": "Mix and apply models",
    "text": "Mix and apply models\n\naa_use_bn = False  # batch norm? \naa_use_resid = True # use residual connections? (doesn't make much difference tbh)\nemb_dims = global_args.latent_dim # input size to aa model\nhidden_dims = 64   # number of hidden dimensions in aa model. usually was 64\ntrivial = False  # aa_model is a no-op when this is true\ndebug = True \nprint(\"emb_dims = \",emb_dims)\n\n# untrained aa model\ntorch.manual_seed(seed+2)\n#stems, faders, val_iter = get_stems_faders(batch, val_iter, val_dl)\n\naa_model = AudioAlgebra(dims=emb_dims, hidden_dims=hidden_dims, use_bn=aa_use_bn, resid=aa_use_resid, trivial=trivial).to(device) \nwith torch.no_grad():\n    zsum, zmix, archive = do_mixing(stems, faders, given_model, aa_model, device, debug=debug)\n    \nprint(\"mix:\")\nplayable_spectrogram( archive['mix'][0], output_type='live')\n\n\nFirst, the effects of the given encoder \\(f\\)\n\ndef plot_emb_spectrograms(qs, labels, skip_ys=True):\n    fig, ax = plt.subplots( 3 , 1, figsize=(10, 9))\n    for i, (q, name) in enumerate(zip(qs, labels)):\n        if i>2 and skip_ys: break\n        row, col = i % 3, i//3\n        im = tokens_spectrogram_image(q, mark_batches=True)\n        newsize = (np.array(im.size) *800/im.size[0]).astype(int)\n        im.resize(newsize)\n        ax[row].imshow(im)\n        ax[row].axis('off')\n        ax[row].set_title(labels[i])\n\n    plt.tight_layout()\n    plt.show()\n    \n    \nys, ymix, ysum = archive['ys'], archive['ymix'], archive['ysum']\ndiff = ysum - ymix\nqs      = [ ymix,   ysum,  diff, ys[0], ys[1]]\nlabels =  ['ymix', 'ysum','diff := ysum - ymix', 'y0', 'y1', ]\nprint(\"ymix.shape = \",ymix.shape)\nplot_emb_spectrograms(qs, labels)\n\n….So at least using the data I can see right now, ymix and ysum can differ by what looks to be 50% in places.\n\nfor i, (q, name) in enumerate(zip(qs, labels)):\n    if i>2: break\n    print(f\"{name}:\")\n    show_pca_point_cloud(q, mode='lines+markers')\n\n\n\nNow the z’s (note the model is untrained at this point)\n\n\nReconstruction /demo\n\n\nDefine Losses"
  },
  {
    "objectID": "aa-mixer.html#training-loop",
    "href": "aa-mixer.html#training-loop",
    "title": "aa_mixer",
    "section": "Training loop",
    "text": "Training loop\n\ntrain_aa_model(debug=True)\n\n\nif use_wandb: wandb.finish()"
  },
  {
    "objectID": "destructo.html",
    "href": "destructo.html",
    "title": "Destructo. 💣💥🤯 v0.1",
    "section": "",
    "text": "Open In Colab\nAuthorship: notebook by Scott H. Hawley ….using audio-diffusion by Zach Evans; based on v-diffusion codes by Katherine Crowson, and incorporating audio-diffusion-pytorch by Flavio Schneider. …with notebook-styling ideas from David Marx’s “Video_Killed_The_Radio_Star_Defusion” notebook. ;-)"
  },
  {
    "objectID": "destructo.html#load-the-model-checkpoint",
    "href": "destructo.html#load-the-model-checkpoint",
    "title": "Destructo. 💣💥🤯 v0.1",
    "section": "Load the Model Checkpoint",
    "text": "Load the Model Checkpoint\nThe checkpoint file is 4 GB.\nThe notebook will download the checkpoint file if you’re not on Colab. (This is slow).\nOn Colab, you can manually mount the checkpoint directly from Google Drive. (This is fast). To do that, click this Google Drive link, then choose “Add to My Drive”.\n\n#@title Mount or Download Checkpoint\non_colab = os.path.exists('/content')\nif on_colab:\n    from google.colab import drive\n    drive.mount('/content/drive/') \n    ckpt_file = '/content/drive/'+model_choices[mc]['gdrive_path']\n    while not os.path.exists(ckpt_file):\n        print(f\"\\nPROBLEM: Expected to find the checkpoint file at {ckpt_file} but it's not there.\\nWhere is it? (Go to the File system in the left sidebar and find it)\")\n        ckpt_file = input('Enter location of checkpoint file: ')\nelse:\n    ckpt_file = 'checkpoint.ckpt'\n    if not os.path.exists(ckpt_file):\n        url = model_choices[mc]['ckpt_url']\n        # downloading large files from GDrive requires special treatment to bypass the dialog button it wants to throw up\n        id = url.split('/')[-2]\n        #cmd = f'wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \\'https://docs.google.com/uc?export=download&id={id}\\' -O- | sed -rn \\'s/.*confirm=([0-9A-Za-z_]+).*/\\1\\\\n/p\\')&id={id}\" -O {ckpt_file} && rm -rf /tmp/cookies.txt'\n        #subprocess.run(cmd, shell=True, check=True) \n        !gdown -O {ckpt_file} {id}\n\n        print(f\"\\nSecurity: checking hash on checkpoint file...\")\n        new_hash = subprocess.run(['shasum', '-a','256',ckpt_file], stdout=subprocess.PIPE).stdout.decode('utf-8').split(' ')[0]\n        #new_hash = subprocess.run(['md5sum',ckpt_file], stdout=subprocess.PIPE).stdout.decode('utf-8')\n        assert new_hash == model_choices[mc]['ckpt_hash'], \"Hashes don't match. STOP THE NOTEBOOK. DO NOT EXECUTE.\"\n        print(\"[Hash is fast,] Hash is cool.  Ready to go!\")\n    else:\n        print(\"Checkpoint found!\")\n\nCheckpoint found!\n\n\nThis next part actually loads the checkpoint:\n\n#@title \nprint(f\"Now loading checkpoint from {ckpt_file}\")\n\nargs_dict = {'num_quantizers':0, 'sample_size': 65536, 'sample_rate':48000, 'latent_dim': 64, 'pqmf_bands':1, 'ema_decay':0.995, 'num_quantizers':0}\nglobal_args = namedtuple(\"global_args\", args_dict.keys())(*args_dict.values())\n\nmodel = DiffusionDVAE.load_from_checkpoint(ckpt_file, global_args=global_args)\nmodel.eval() # Inference mode\nmodel = model.to(device)\n\nNow loading checkpoint from checkpoint.ckpt\n\n\n # \\(2.\\) 🔈 Supply Your Audio We need the filename (full path) to some audio already on your hard drive if you’re running locally, or else something that you upload if you’re running this on a cloud service. * On Colab: To upload, you can click on the the file icon to the left, and find the upload icon, which is looks like a piece of paper with an upward arrow on it. * On Kaggle: Go to File > Upload Data. It’ll make you create a “dataset”; afterward your file will be /kaggle/input/<your_dataset_name>/<your_file>\n\n#@markdown Provide the full path of your *short* (3 to 10 sec) audio file: \n\naudio_file = 'examples/guitar_dry.mp3'  #@param{type:\"string\"}\n\naudio_in = load_audio(audio_file, sr=global_args.sample_rate)\nif audio_in.shape[0] == 1:     # our models expect stereo\n    audio_in = torch.vstack((audio_in, audio_in))\n\nprint(f\"audio_in.shape =\",audio_in.shape)\nplayable_spectrogram(audio_in, output_type='live', specs='wave_mel')\n\n # \\(3.\\) 🗜 Encode Audio into Neural Embeddings\n\n#@title Make batches and encode\n# First, we need to chop up the audio into batches \ndemo_reals = batch_it_crazy(audio_in, global_args.sample_size)\n\nmax_batch_size = 8\nif demo_reals.size()[0] > max_batch_size:\n    print(f\"Warning: Due to CUDA memory limits, we're cutting you off at a batch size of {max_batch_size}\")\n    demo_reals = demo_reals[0:max_batch_size,:,:]\n\nprint(f\"demo_reals.shape = {demo_reals.shape}\\ni.e. {demo_reals.shape[0]} batches\")\n\n\n# TODO: move most of this stuff to an import \ndef encode_it(demo_reals, module):\n    encoder_input = demo_reals\n    \n    if module.pqmf_bands > 1:\n        encoder_input = module.pqmf(demo_reals)\n    \n    encoder_input = encoder_input.to(module.device)\n    demo_reals = demo_reals.to(module.device)\n    noise = torch.randn([demo_reals.shape[0], 2, module.demo_samples]).to(module.device)\n    \n    with torch.no_grad():\n        embeddings = module.encoder_ema(encoder_input)\n        if module.quantized:\n            #Rearrange for Memcodes\n            embeddings = rearrange(embeddings, 'b d n -> b n d')\n            embeddings, _= module.quantizer_ema(embeddings)\n            embeddings = rearrange(embeddings, 'b n d -> b d n')\n        \n        embeddings = torch.tanh(embeddings)\n        return embeddings, noise\n\n# attach some arg values to the model \nmodel.demo_samples = global_args.sample_size \nmodel.quantized = global_args.num_quantizers > 0\n\nembeddings, noise = encode_it(demo_reals, model) # ENCODING! \n\nprint(f\"Encoded to embeddings.shape =\",embeddings.shape)\nprint(f\"            and noise.shape =\",noise.shape)\n\ndemo_reals.shape = torch.Size([6, 2, 65536])\ni.e. 6 batches\nEncoded to embeddings.shape = torch.Size([6, 64, 512])\n            and noise.shape = torch.Size([6, 2, 65536])\n\n\n\n#@title ## 🔊 Check: Decode Unaltered Embeddings\n#@markdown Perform reconstruction using  \\_\\_this many\\_\\_ diffusion inference steps:\n\ndemo_steps = 35 #@param {type:\"slider\", min:10, max:100, step:1}\n\nfake_batches = sample(model.diffusion_ema, noise, demo_steps, 0, embeddings)\n\naudio_out = rearrange(fake_batches, 'b d n -> d (b n)') # Put the demos together\nplayable_spectrogram(audio_out, output_type='live')\n\n….Not a perfect reconstruction, right? But pretty close. We’re working on increasing the reconstruction quality, but for now that’s the best you’ll get. That’s not the interest part though! Keep going…\n # \\(4.\\)🌪 Mathemangle the Embeddings ..and reap the whirlwind.\nFirst let’s print out a little bit of info so you can see what your’re working with:\n\n#@title  \nz = embeddings   # shorthand variable name \"z\"\nprint(f\"Embeddings array has shape z.shape = {z.shape}: [b, d, n] = {z.shape[0]} batches, {z.shape[1]} dimensions, {z.shape[-1]} time-samples.\\nEach time-sample exists in a {z.shape[1]}-dimensional latent space\")\n\nEmbeddings array has shape z.shape = torch.Size([6, 64, 512]): [b, d, n] = 6 batches, 64 dimensions, 512 time-samples.\nEach time-sample exists in a 64-dimensional latent space\n\n\n\n# @title Perform math operations in embedding space: \"Embedding-Based Audio Effects\"\n\n#@markdown Random number to use with some calculations:\nrand_fac = 0.5   #@param {type:\"number\"}\n\nz = embeddings.clone() # make a copy before doing math\n\n\n# 'MATH OPTIONS' below for messing with the sound....\n\n\n#@markdown ###Effects Presets:\n\n#@markdown (These 'chain' if mutliple are enabled. To reorder them you can highlight and drag-and-drop in the code itself)\n\ncall_and_response = False #@param {type:\"boolean\"}\nif call_and_response: z = -z + rand_fac*z*(2*torch.rand_like(z)-1)\n\nhurt_drums = False #@param {type:\"boolean\"}\nif hurt_drums: z = (1-rand_fac)*embeddings + rand_fac*z*(2*torch.rand_like(z)-1) \n\nswap_emb_dims = True #@param {type:\"boolean\"}\nif swap_emb_dims: z = z.flip(dims=[1])     # Swap across the dimensions\n\nDestructo = True #@param {type:\"boolean\"}\nif Destructo: z = torch.max(z)*(torch.sign(z) - z) # Scott calls this one 'Destructo'\n\nDestructo2 = False #@param {type:\"boolean\"}\nif Destructo2: z = torch.max(torch.abs(z)) - z\n\nbig_changes = False #@param {type:\"boolean\"}\nif big_changes: z = 2*z                         # 'big changes afoot'\n\nwavy = False #@param {type:\"boolean\"}\nif wavy: z = z*torch.cos(torch.linspace(0,4*6.28,z.shape[-1])).to(device)             # slow sine wave of embeddings? IDK\n\ntime_reverse = False #@param {type:\"boolean\"}\nif time_reverse: z = torch.flip(z,[2])          # time reversal of tokens\n\nflippy = False #@param {type:\"boolean\"}\nif flippy: z = z.clone() + torch.flip(z,[-1])  # IDK what to calll it\n\nkill_half = False #@param {type:\"boolean\"}\nif kill_half: z[:,33:-1,:] = 0.\n\nreverb_time = 0 #@param {type:\"number\"}\nif reverb_time != 0:\n    for i in range(z.shape[-1]):   # exp. weighted moving average\n        z = z + math.exp(-i/reverb_time)*torch.nn.functional.pad(z,(i+1,0,0,0,0,0), mode='constant')[:,:,0:z.shape[-1]]\n\noverdrive_factor = 1 #@param {type:\"number\"}  # 1=do nothing\nif overdrive_factor!=1: z = torch.max(z)*torch.tanh(z*overdrive_factor)   # overdrive? \n# \n\n#@markdown WRITE YOUR OWN MATH OP! (use \"z\" as embedding variable)\nop = \"z = 1.0*z\" #@param {type:\"string\"}\nif op != '': exec(op)\n\nembeddings2 = z  # for backwards-compatibility with old version of notebook \n\nprint(\"Just a quick pre-post masurement to see how much you futzed things up:\")\nprint(f\"Before mathemangling, embeddings min & max values were {torch.min(embeddings).cpu().numpy()}, {torch.max(embeddings).cpu().numpy()}, respectively.\")\nprint(f\"After mathemangling,  embeddings min & max values were {torch.min(z).cpu().numpy()}, {torch.max(z).cpu().numpy()}, respectively.\")\n\nJust a quick pre-post masurement to see how much you futzed things up:\nBefore mathemangling, embeddings min & max values were -0.6636519432067871, 0.5791661739349365, respectively.\nAfter mathemangling,  embeddings min & max values were -0.5791651606559753, 0.5791658759117126, respectively."
  },
  {
    "objectID": "destructo.html#check-visualize-prepost-embeddings",
    "href": "destructo.html#check-visualize-prepost-embeddings",
    "title": "Destructo. 💣💥🤯 v0.1",
    "section": "🌌 Check: Visualize pre/post embeddings",
    "text": "🌌 Check: Visualize pre/post embeddings\n\nprint(\"Before:\")\ndisplay(tokens_spectrogram_image(embeddings))\n\nBefore:\n\n\n\n\n\n\nprint(\"After:\")\ndisplay(tokens_spectrogram_image(z))\n\nAfter:\n\n\n\n\n\n3D Point Clouds:\n\nprint(\"Before:\")\nshow_pca_point_cloud(embeddings, mode='lines+markers')\n\nBefore:\n\n\n\n\n\n\n                                                \n\n\n\nprint(\"After:\")\nshow_pca_point_cloud(z, mode='lines+markers')\n\nAfter:\n\n\n\n                                                \n\n\n # \\(5.\\) 🔊 Decode and Listen Now we decode & listen to the embeddings that were the result of the mathemangling operation(s)\n\ndemo_steps = 35 #@param {type:\"slider\", min:10, max:100, step:1}\n\nfake_batches2 = sample(model.diffusion_ema, noise, demo_steps, 0, z)\naudio_out2 = rearrange(fake_batches2, 'b d n -> d (b n)').cpu() # un-batch: Put the demos together\nif np.abs(audio_out2.flatten()).max() >= 1.0:                   # let's go ahead and rescale to prevent clipping \n    audio_out2 = 0.99*audio_out2/np.abs(audio_out2.flatten()).max() \nplayable_spectrogram(audio_out2, output_type='live')\n\nOriginal Input Audio was (for comparison):\n\nplayable_spectrogram(audio_in, output_type='live')\n\nLet’s look at spectrograms alone:\n\nprint(\"Before:\")\nspec_graph = audio_spectrogram_image(audio_out.cpu(), justimage=False, db=False, db_range=[-60,20])\ndisplay(spec_graph)\n\nBefore:\n\n\n/fsx/shawley/envs/shazbot/lib/python3.8/site-packages/torchaudio/functional/functional.py:571: UserWarning:\n\nAt least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (513) may be set too low.\n\n\n\n\n\n\n\nprint(\"After:\")\nspec_graph2 = audio_spectrogram_image(audio_out2, justimage=False, db=False, db_range=[-60,20])\ndisplay(spec_graph2)\n\nAfter:\n\n\n\n\n\n\n# @title (Optional) Re-amp your output?\n#@markdown With re-amping, you create a feedback loop so you can go back up to \"Mathemangle\" to add more to your signal chain.\n\n# @markdown (Note that this will overwrite your input embeddings, so we'll save a backup just in case you want to \"undo\")\n\nre_amp = False  #@param {type:\"boolean\"}\nif re_amp:\n    backup, embeddings = embeddings.clone(), z\n\n # \\(6.\\) ↗️ Share Your New Sounds …either alone as part of a song you made! Harmonai Play Discord has a #show-and-tell channel where you can post your Destructo creations!\n # \\(7.\\) ⚖️ Licence Sounds are all yours, period.\nAs for the code, it is:\nLicensed under the MIT License\nCopyright (c) 2022 Scott H. Hawley\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n # \\(8.\\) ❔ Afterword: Researching Audio-Algebra for Effects?"
  },
  {
    "objectID": "destructo.html#can-we-1-shot-learn-audio-effects-using-these-embeddings-spoiler-no-but-lets-try",
    "href": "destructo.html#can-we-1-shot-learn-audio-effects-using-these-embeddings-spoiler-no-but-lets-try",
    "title": "Destructo. 💣💥🤯 v0.1",
    "section": "Can We 1-Shot-Learn Audio Effects using These Embeddings? (Spoiler: No but let’s try…)",
    "text": "Can We 1-Shot-Learn Audio Effects using These Embeddings? (Spoiler: No but let’s try…)\nDo embeddings store anything important, and can vectors in embedding-space be meaningfully used to model audio effects?\nLanguage models showed that operations on embeddings of words (for well-trained masked language models) could produce “semantically meaningful” results, such as “king - man + woman = queen” or “japan - tokyo = germany - berlin”.\nCan our model(s) do similar things for audio? Here we’ll subtract two (new) embeddings and add them to a third (your original input embedding). What do we get?\nIn what follows, you’ll upload two “versions” of the same audio, with (“wet”) and without (“dry”) an effect applied (e.g. distortion).\n\nWill we get e.g. “wet_piano - dry_piano + dry_guitar = wet_guitar”?\n\nIn the examples provided in examples/, we used an overdrive effect for “wet”. This is a nonlinear effect, so we might not expect a “linear addition rule” to work for this (spoiler: and we’d be correct), but let’s try it anyway:\n\neffect = 'reverb'\n\n\n#@markdown Provide the full path of your audio files:\n\ndry_name = 'examples/piano_dry.mp3'  #@param{type:\"string\"}\ndry = load_audio(dry_name, sr=global_args.sample_rate)\nif dry.shape[0] == 1: dry = torch.vstack((dry, dry))\n\nwet_name = f'examples/piano_wet_{effect}.mp3' #@param{type:\"string\"}\nwet = load_audio(wet_name, sr=global_args.sample_rate)\nif wet.shape[0] == 1: wet = torch.vstack((wet, wet))\n\n# let's make them the same size \nmin_len = min(dry.shape[-1], wet.shape[-1])\ndry, wet = dry[:,:min_len], wet[:,:min_len]\n\nprint(f\"dry.shape = {dry.shape}, wet.shape = {wet.shape}\")\nprint(\"dry:\")\nplayable_spectrogram(dry, output_type='live')\n\n\nprint(\"wet:\")  \nplayable_spectrogram(wet, output_type='live')\n\nIn raw-audio space, the difference between these is:\n\nprint(\"wet - dry (difference taken in raw audio domain):\")  \nplayable_spectrogram(wet - dry, output_type='live')\n\n….That includes a lot of the original “content”. One might hope that in embedding space, by “subtracting out the content” we might leave only the “audio effect” that makes up their difference.\n\n#@title Encode wet & dry, compute the difference\n\ndry_reals = batch_it_crazy(dry, global_args.sample_size)\nwet_reals = batch_it_crazy(wet, global_args.sample_size)\n\nmax_batch_size = 8\nif dry_reals.size()[0] > max_batch_size:\n    print(f\"Warning: Due to CUDA memory limits, we're cutting you off at a batch size of {max_batch_size}\")\n    dry_reals = dry_reals[0:max_batch_size,:,:]\n    wet_reals = wet_reals[0:max_batch_size,:,:]\n\nprint(f\"dry_reals.shape = {demo_reals.shape}, wet_reals.shape = {wet_reals.shape}\")\n\ndry_embeddings, dry_noise = encode_it(dry_reals, model) \nwet_embeddings, wet_noise = encode_it(wet_reals, model) \ndiff = wet_embeddings - dry_embeddings     ## this is \"king - man\"\n\nprint(f\"Difference encoded to diff.  diff.shape =\",diff.shape,\", and noise.shape =\",noise.shape)\n\ndry_reals.shape = torch.Size([6, 2, 65536]), wet_reals.shape = torch.Size([4, 2, 65536])\nDifference encoded to diff.  diff.shape = torch.Size([4, 64, 512]) , and noise.shape = torch.Size([6, 2, 65536])\n\n\n\n# @title 🔊 Appy difference to input audio, decode\n\n# @markdown Time-average the difference in embeddings?\ntime_avg = False  #@param {type:\"boolean\"}\n\n# @markdown Number of steps for diffusion reconstruction: \ndemo_steps = 35 #@param {type:\"slider\", min:10, max:100, step:1}\n\nz = embeddings.clone() # backup.clone()    # just in case we re-amped, go back to an earlier version\n\nif time_avg: \n    diff = diff.mean(axis=-1)\nelse:\n    # make sure embeddings and diff have the same length\n    length_difference = z.shape[-1] - diff.shape[-1]\n    if length_difference > 0:  # zero-pad the end of diff\n        diff = torch.nn.functional.pad(diff,(length_difference,0,0,0,0,0), mode='constant')\n    elif length_difference < 0: # truncate diff \n        diff = diff[:,:,:z.shape[-1]]\n    \n    diff = diff.mean(0) # average diff over batches, to allow broadcasting\n\n\n# now apply the \"learned effect\"\nz = z + diff   # woman + (king - man)  = ? (queen?)\n\n# and decode\nfake_batches2 = sample(model.diffusion_ema, noise, demo_steps, 0, z)\naudio_out3 = rearrange(fake_batches2, 'b d n -> d (b n)').cpu() # un-batch: Put the demos together\nif np.abs(audio_out3.flatten()).max() >= 1.0:                   # let's go ahead and rescale to prevent clipping \n    audio_out3 = 0.99*audio_out3/np.abs(audio_out3.flatten()).max() \nplayable_spectrogram(audio_out3, output_type='live')\n\n…not much of a success. Not much change to the guitar, and some of the piano content is leaking in.\nIf the same (real) effect is applied to the guitar (for realsies), we’d expect to have gotten this:\n\nreal_wet_guitar = load_audio(f'examples/guitar_wet_{effect}.mp3')\nplayable_spectrogram(real_wet_guitar, output_type='live')\n\n\nConclusions?\nSo, for this particular embedding model, we did not observe a “vector space” relation for a particular (time-dependent) audio effect. Perhaps a different model would allow us to perform such an operation.\nEven with this model, we might still try to compute an “average” displacement (or perhaps somehow a nonlinear effect) in embedding space for certain effects, and see how well that works. This is an active goal of the Audio Algebra research program. We can try that with a dataset in another notebook.\n\n\nWhat about using ‘the trivial embedding’?\nRegarding the raw audio itself as a trivial embedding, what happens if we just do algebra on the audio samples? Well, are samples aren’t quite the same size but we can try…\n\ndiff = (wet - dry).cpu()\nminshape = min(diff.shape[-1], audio_out.shape[-1])\ntry_guitar_wet = diff[:,:minshape] + audio_out.cpu()[:,:minshape]\nplayable_spectrogram(try_guitar_wet, output_type='live')\n\nlots more piano in that that when we used the embeddings. So… hard to say what’s going on. Maybe using more examples, or a different effect than distortion, would be helpful."
  },
  {
    "objectID": "given-models.html",
    "href": "given-models.html",
    "title": "given_models",
    "section": "",
    "text": "Currently included: - Baselines: Spectrogram (i.e. complex STFT), MagSpectrogram, MagDPhaseSpectrogram, MelSpectrogram - Diffusion models: DVAE (unmasked) - VAEs: RAVE\nTo be added: - Diffusion models: masked DVAE , archinet’s - Others groups’: JukeBox (as “IceBox”)"
  },
  {
    "objectID": "given-models.html#zaptrems-1d-diffusion-unet",
    "href": "given-models.html#zaptrems-1d-diffusion-unet",
    "title": "given_models",
    "section": "@zaptrem’s 1D diffusion unet",
    "text": "@zaptrem’s 1D diffusion unet\nNote that several of the routines imported require zaptrem’s special fork of Flavio Schneider’s repo(s):\npip install -U git+https://github.com/Sonauto/audio-diffusion-pytorch.git git+https://github.com/Sonauto/audio-encoders-pytorch.git https://github.com/Sonauto/a-unet/archive/tiled-attention.zip pyloudnorm\n\nsource\n\nDMAE1d\n\n DMAE1d (debug=False)\n\nThis provides an (optional) ‘shorthand’ structure for (some) given_models"
  },
  {
    "objectID": "given-models.html#test-our-autoencoder-options",
    "href": "given-models.html#test-our-autoencoder-options",
    "title": "given_models",
    "section": "Test our autoencoder options",
    "text": "Test our autoencoder options\nFirst prepare a waveform and instantiate the various models\n\nprint(\"waveform.shape =      \",waveform.shape)\n\nwaveform_pad = given_model.zero_pad_po2(waveform)\nprint(\"waveform_pad.shape =  \",waveform_pad.shape)\n\n# optional: to batch or not to batch?\nwaveform_batch = waveform_pad.unsqueeze(0)\nprint(f\"waveform_batch.shape = {waveform_batch.shape}, dtype = {waveform.dtype}\")\n\n# use reverse order to put most recently-written models diffusion models first, leave out MelSpec\ngiven_models = [SpectrogramAE(), MagSpectrogramAE(), MagDPhaseSpectrogramAE(),  DVAEWrapper(), DMAE1d(),]\n_ = [x.setup() for x in given_models]\n\nwaveform.shape =       torch.Size([2, 55728])\nwaveform_pad.shape =   torch.Size([2, 65536])\nwaveform_batch.shape = torch.Size([1, 2, 65536]), dtype = torch.float32\nDVAE: attempting to load checkpoint ~/checkpoints/dvae_checkpoint.ckpt\nCheckpoint found!\nDMAE1d: attempting to load checkpoint /Users/shawley/checkpoints/dmae1d_checkpoint.ckpt\nCheckpoint found!\nSorry, exception = Error(s) in loading state_dict for DMAE1d:\n    Missing key(s) in state_dict: \"resample_encode.kernel\", \"resample_decode.kernel\". . Going with random weights\n\n\nNow run the waveform through the models:\n\nfor given_model in given_models:\n    display(HTML('<hr>'))\n    print(\"given_model.name = \",given_model.name)\n    given_model = given_model.to(device)\n    reps = given_model.encode(waveform_batch.to(device))\n    recon = given_model.decode(reps) \n    recon = recon.squeeze().cpu()\n    if len(reps.shape) < 4: reps = reps.unsqueeze(0) # for viz purposes\n    print(f\"For model {given_model.name}, reps.shape = {reps.shape} and dtype = {reps.dtype}. recon.shape = {recon.shape}\")\n    if given_model.name in ['DVAEWrapper', 'DMAE1d']:\n        title, cmap = 'Embeddings', 'coolwarm'\n        vals = reps[:,0,:,:]\n    else:\n        title, cmap =f'{given_model.name}: 10Log10(abs(Embeddings)**2)', 'viridis'\n        vals = 10*torch.log10(torch.abs(reps[:,0,:,:])**2+1e-6)\n\n    display(Audio(recon, rate=48000))\n    display(tokens_spectrogram_image(vals.cpu(), title=title, mark_batches=True, symmetric=False, cmap=cmap))\n    #THIS NEXT LINE MAKES notebook filesize huge: \n    # display(playable_spectrogram(recon.cpu(), specs=\"all\", output_type='live'))\n    diff = waveform_pad - recon\n    for thing,name in zip([waveform, recon, diff], ['input','recon','diff']):\n        plt.plot(thing[0,:].numpy(), alpha=0.5, label=name) # just left channel for now\n    plt.legend()\n    plt.show()\n\n\n\n\ngiven_model.name =  SpectrogramAE\nFor model SpectrogramAE, reps.shape = torch.Size([1, 2, 513, 257]) and dtype = torch.complex64. recon.shape = torch.Size([2, 65536])\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n\n\n\n\n\ngiven_model.name =  MagSpectrogramAE\nFor model MagSpectrogramAE, reps.shape = torch.Size([1, 2, 513, 257]) and dtype = torch.float32. recon.shape = torch.Size([2, 65536])\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n\n\n\n\n\ngiven_model.name =  MagDPhaseSpectrogramAE\nFor model MagDPhaseSpectrogramAE, reps.shape = torch.Size([2, 2, 513, 257]) and dtype = torch.float32. recon.shape = torch.Size([2, 65536])\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n\n\n\n\n\ngiven_model.name =  DVAEWrapper\n\n\n  0%|                                                                                            | 0/50 [00:00<?, ?it/s]/Users/shawley/envs/aa/lib/python3.10/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n100%|███████████████████████████████████████████████████████████████████████████████████| 50/50 [03:23<00:00,  4.08s/it]\n\n\nFor model DVAEWrapper, reps.shape = torch.Size([1, 1, 64, 512]) and dtype = torch.float32. recon.shape = torch.Size([2, 65536])\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n\n\n\n\n\ngiven_model.name =  DMAE1d\n\n\nSampling (noise=0.00): 100%|████████████████████████████████████████████████████████████| 50/50 [00:07<00:00,  6.42it/s]\n\n\nFor model DMAE1d, reps.shape = torch.Size([1, 1, 32, 64]) and dtype = torch.float32. recon.shape = torch.Size([2, 65536])\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "aa-effects.html",
    "href": "aa-effects.html",
    "title": "aa_effects",
    "section": "",
    "text": "install = False  # can set to false to skip this part, e.g. for re-running in same session\nif install:     # ffmpeg is to add MP3 support to Colab\n    !yes | sudo apt install ffmpeg \n    !pip install -Uqq einops gdown \n    !pip install -Uqq git+https://github.com/drscotthawley/aeiou\n    !pip install -Uqq git+https://github.com/drscotthawley/audio-algebra"
  },
  {
    "objectID": "aa-effects.html#set-up-the-given-autoencoder-models",
    "href": "aa-effects.html#set-up-the-given-autoencoder-models",
    "title": "aa_effects",
    "section": "Set up the Given [Auto]Encoder Model(s)",
    "text": "Set up the Given [Auto]Encoder Model(s)\nNote that initially we’re only going to be using the encoder part. The decoder – with all of its sampling code, etc. – will be useful eventualy, and we’ go ahead and define it. But fyi it won’t be used at all while training the AA mixer model.\nDownload the checkpoint file for the dvae\n\non_colab = os.path.exists('/content')\nif on_colab:\n    from google.colab import drive\n    drive.mount('/content/drive/') \n    ckpt_file = '/content/drive/MyDrive/AI/checkpoints/epoch=53-step=200000.ckpt'\nelse:\n    ckpt_file = '/fsx/shawley/checkpoints/dvae_checkpoint.ckpt'\n    if not os.path.exists(ckpt_file):\n        url = 'https://drive.google.com/file/d/1C3NMdQlmOcArGt1KL7pH32KtXVCOfXKr/view?usp=sharing'\n        # downloading large files from GDrive requires special treatment to bypass the dialog button it wants to throw up\n        id = url.split('/')[-2]\n        cmd = f'wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \\'https://docs.google.com/uc?export=download&id={id}\\' -O- | sed -rn \\'s/.*confirm=([0-9A-Za-z_]+).*/\\1\\\\n/p\\')&id={id}\" -O {ckpt_file} && rm -rf /tmp/cookies.txt'\n        print(\"cmd = \\n\",cmd)\n        subprocess.run(cmd, shell=True, check=True)\n\n\ngiven_model = DiffusionDVAE.load_from_checkpoint(ckpt_file, global_args=global_args)\ngiven_model.eval() # disable randomness, dropout, etc...\n\n# attach some arg values to the model \ngiven_model.demo_samples = global_args.sample_size \ngiven_model.quantized = global_args.num_quantizers > 0\ngiven_model.to(device)\nfreeze(given_model)  # freeze the weights for inference\nprint(\"Given Autoencoder is ready to go!\")\n\nGiven Autoencoder is ready to go!"
  },
  {
    "objectID": "aa-effects.html#the-aa-model",
    "href": "aa-effects.html#the-aa-model",
    "title": "aa_effects",
    "section": "The AA model",
    "text": "The AA model"
  },
  {
    "objectID": "aa-effects.html#mix-and-apply-models",
    "href": "aa-effects.html#mix-and-apply-models",
    "title": "aa_effects",
    "section": "Mix and apply models",
    "text": "Mix and apply models\n\naa_use_bn = False  # batch norm? \naa_use_resid = True # use residual connections? (doesn't make much difference tbh)\nemb_dims = global_args.latent_dim # input size to aa model\nhidden_dims = 64   # number of hidden dimensions in aa model. usually was 64\ntrivial = False  # aa_model is a no-op when this is true\ndebug = True \nprint(\"emb_dims = \",emb_dims)\n\n# untrained aa model\ntorch.manual_seed(seed+2)\n#stems, faders, val_iter = get_stems_faders(batch, val_iter, val_dl)\n\naa_model = AudioAlgebra(dims=emb_dims, hidden_dims=hidden_dims, use_bn=aa_use_bn, resid=aa_use_resid, trivial=trivial).to(device) \nwith torch.no_grad():\n    archive = do_mixing(batch, given_model, aa_model, device)\n\nemb_dims =  64\n\n\n\nFirst, the effects of the given encoder \\(f\\)\n\ndef plot_emb_spectrograms(qs, labels, skip_ys=True):\n    fig, ax = plt.subplots( 3 , 1, figsize=(10, 9))\n    for i, (q, name) in enumerate(zip(qs, labels)):\n        if i>2 and skip_ys: break\n        row, col = i % 3, i//3\n        im = tokens_spectrogram_image(q, mark_batches=True)\n        newsize = (np.array(im.size) *800/im.size[0]).astype(int)\n        im.resize(newsize)\n        ax[row].imshow(im)\n        ax[row].axis('off')\n        ax[row].set_title(labels[i])\n\n    plt.tight_layout()\n    plt.show()\n    \n#ys, ymix, ysum = archive['ys'], archive['ymix'], archive['ysum']\n#diff = ysum - ymix\n#qs      = [ ymix,   ysum,  diff, ys[0], ys[1]]\n#labels =  ['ymix', 'ysum','diff := ysum - ymix', 'y0', 'y1', ]\n#print(\"ymix.shape = \",ymix.shape)\n#plot_emb_spectrograms(qs, labels)\n\n….So at least using the data I can see right now, ymix and ysum can differ by what looks to be 50% in places.\n\n#for i, (q, name) in enumerate(zip(qs, labels)):\n#    if i>2: break\n#    print(f\"{name}:\")\n#    show_pca_point_cloud(q, mode='lines+markers')\n\n\n\nNow the z’s (note the model is untrained at this point)\n\n\nReconstruction /demo\n\n\nDefine Losses"
  },
  {
    "objectID": "aa-effects.html#training-loop",
    "href": "aa-effects.html#training-loop",
    "title": "aa_effects",
    "section": "Training loop",
    "text": "Training loop\n\ntrain_aa_model(debug=True)\n\ntotal_steps = 23000\nSetting up AA model using device: cuda\n\n\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: Currently logged in as: drscotthawley. Use `wandb login --relogin` to force relogin\n\n\nwandb version 0.13.9 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.13.8\n\n\nRun data is saved locally in /fsx/shawley/code/audio-algebra/wandb/run-20230116_072622-lb0sa9bz\n\n\nSyncing run autumn-wind-10 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/drscotthawley/aa-effects\n\n\n View run at https://wandb.ai/drscotthawley/aa-effects/runs/lb0sa9bz\n\n\nEpoch 1/40: 100%|██████████| 576/576 [05:37<00:00,  1.71batch/s, loss=12.2]\nEpoch 2/40:  31%|███▏      | 180/576 [01:49<03:39,  1.80batch/s, loss=11.7]\n\n\n\nwandb.finish()"
  },
  {
    "objectID": "diffusiondvae.html",
    "href": "diffusiondvae.html",
    "title": "DiffusionDVAE",
    "section": "",
    "text": "See Zach Evans’ official audio-diffusion for updates.\nOther parts of audio-algebra may use Scott Hawley’s “Python Packaging Fork” of this: https://github.com/drscotthawley/audio-diffusion. That fork is also “out of date” w.r.t. @zqevans’ research; we’ll sync back up someday! See “LICENSE(S)” at the bottom of the notebook.\n\nsource\n\nget_alphas_sigmas\n\n get_alphas_sigmas (t)\n\nReturns the scaling factors for the clean image (alpha) and for the noise (sigma), given a timestep.\n\nsource\n\n\nget_crash_schedule\n\n get_crash_schedule (t)\n\n\nsource\n\n\nalpha_sigma_to_t\n\n alpha_sigma_to_t (alpha, sigma)\n\nReturns a timestep, given the scaling factors for the clean image and for the noise.\n\nsource\n\n\nsample\n\n sample (model, x, steps, eta, logits)\n\nDraws samples from a model given starting noise.\n\nsource\n\n\nDiffusionDVAE\n\n DiffusionDVAE (global_args)\n\nHooks to be used in LightningModule.\nTesting that:\n\nargs_dict = {'num_quantizers':0, 'sample_size': 65536, 'sample_rate':48000, 'latent_dim': 64, 'pqmf_bands':1, 'ema_decay':0.995, 'num_quantizers':0}\nglobal_args = namedtuple(\"global_args\", args_dict.keys())(*args_dict.values())\nmodel = DiffusionDVAE(global_args=global_args)\n\n\n\nLICENSE(S)"
  }
]