{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp StackedDiffAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StackedDiffAE\n",
    "\n",
    "> This is a version of one of Zach Evans' \"stacked\" diffusion-based autoencoders, for use with audio-algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: For the 'Wrapper' version of this, see given_models.ipynb**\n",
    "\n",
    "See Zach Evans' official [audio-diffusion](https://github.com/zqevans/audio-diffusion) for updates. \n",
    "\n",
    "Other parts of `audio-algebra` may use Scott Hawley's \"Python Packaging Fork\" of this: [https://github.com/drscotthawley/audio-diffusion](https://github.com/drscotthawley/audio-diffusion).  That fork is also \"out of date\" w.r.t. @zqevans' research; we'll sync back up someday!  See \"LICENSE(S)\" at the bottom of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "# old imports from DVAEWrapper\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import os, sys\n",
    "import subprocess\n",
    "from collections import namedtuple\n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import optim, nn, Tensor\n",
    "from torch import multiprocessing as mp\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data as torchdata\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.distributed import rank_zero_only\n",
    "from tqdm import trange \n",
    "\n",
    "from einops import rearrange\n",
    "from nwt_pytorch import Memcodes\n",
    "\n",
    "# audio-diffusion imports\n",
    "from diffusion.pqmf import CachedPQMF as PQMF # may require some manual labor/ symlinking directories\n",
    "from encoders.encoders import AttnResEncoder1D\n",
    "from autoencoders.soundstream import SoundStreamXLEncoder\n",
    "from dvae.residual_memcodes import ResidualMemcodes\n",
    "from decoders.diffusion_decoder import DiffusionAttnUnet1D\n",
    "from diffusion.model import ema_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# new imports \n",
    "\n",
    "#standard codes\n",
    "from contextlib import contextmanager\n",
    "from copy import deepcopy\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from torch.nn.parameter import Parameter\n",
    "from tqdm import trange\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.distributed import rank_zero_only\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# harmonai imports \n",
    "\n",
    "from diffusion.pqmf import CachedPQMF as PQMF\n",
    "from autoencoders.soundstream import SoundStreamXLEncoder, SoundStreamXLDecoder\n",
    "from autoencoders.models import AudioAutoencoder\n",
    "from audio_encoders_pytorch import Encoder1d\n",
    "from ema_pytorch import EMA\n",
    "#from audio_diffusion_pytorch import UNetConditional1d, T5Embedder\n",
    "\n",
    "from decoders.diffusion_decoder import DiffusionAttnUnet1D\n",
    "from diffusion.model import ema_update\n",
    "from aeiou.viz import embeddings_table, pca_point_cloud, audio_spectrogram_image, tokens_spectrogram_image\n",
    "from aeiou.datasets import AudioDataset\n",
    "from dataset.dataset import SampleDataset\n",
    "from blocks.utils import InverseLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Define the noise schedule and sampling loop\n",
    "def get_alphas_sigmas(t):\n",
    "    \"\"\"Returns the scaling factors for the clean image (alpha) and for the\n",
    "    noise (sigma), given a timestep.\"\"\"\n",
    "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
    "\n",
    "def alpha_sigma_to_t(alpha, sigma):\n",
    "    \"\"\"Returns a timestep, given the scaling factors for the clean image and for\n",
    "    the noise.\"\"\"\n",
    "    return torch.atan2(sigma, alpha) / math.pi * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.no_grad()\n",
    "def sample(model, x, steps, eta, **extra_args):\n",
    "    \"\"\"Draws samples from a model given starting noise.\"\"\"\n",
    "    ts = x.new_ones([x.shape[0]])\n",
    "\n",
    "    # Create the noise schedule\n",
    "    t = torch.linspace(1, 0, steps + 1)[:-1]\n",
    "\n",
    "    alphas, sigmas = get_alphas_sigmas(t)\n",
    "\n",
    "    # The sampling loop\n",
    "    for i in trange(steps):\n",
    "\n",
    "        # Get the model output (v, the predicted velocity)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            v = model(x, ts * t[i], **extra_args).float()\n",
    "\n",
    "        # Predict the noise and the denoised image\n",
    "        pred = x * alphas[i] - v * sigmas[i]\n",
    "        eps = x * sigmas[i] + v * alphas[i]\n",
    "\n",
    "        # If we are not on the last timestep, compute the noisy image for the\n",
    "        # next timestep.\n",
    "        if i < steps - 1:\n",
    "            # If eta > 0, adjust the scaling factor for the predicted noise\n",
    "            # downward according to the amount of additional noise to add\n",
    "            ddim_sigma = eta * (sigmas[i + 1]**2 / sigmas[i]**2).sqrt() * \\\n",
    "                (1 - alphas[i]**2 / alphas[i + 1]**2).sqrt()\n",
    "            adjusted_sigma = (sigmas[i + 1]**2 - ddim_sigma**2).sqrt()\n",
    "\n",
    "            # Recombine the predicted noise and predicted denoised image in the\n",
    "            # correct proportions for the next step\n",
    "            x = pred * alphas[i + 1] + eps * adjusted_sigma\n",
    "\n",
    "            # Add the correct amount of fresh noise\n",
    "            if eta:\n",
    "                x += torch.randn_like(x) * ddim_sigma\n",
    "\n",
    "    # If we are on the last timestep, output the denoised image\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LatentAudioDiffusionAutoencoder(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    This is the main code we will use with the autoencoder\n",
    "    source: /fsx/zqevans/code/audio-diffusion/train_stacked_latent_dvae_ss_adp_cond.py\n",
    "    \"\"\"\n",
    "    def __init__(self, autoencoder: AudioAutoencoder):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        self.latent_dim = autoencoder.latent_dim\n",
    "                \n",
    "        self.second_stage_latent_dim = 32\n",
    "\n",
    "        factors = [2, 2, 2, 2]\n",
    "\n",
    "        self.latent_downsampling_ratio = np.prod(factors)\n",
    "        \n",
    "        self.downsampling_ratio = autoencoder.downsampling_ratio * self.latent_downsampling_ratio\n",
    "\n",
    "        self.latent_encoder = Encoder1d(\n",
    "            in_channels=self.latent_dim, \n",
    "            out_channels = self.second_stage_latent_dim,\n",
    "            channels = 128,\n",
    "            multipliers = [1, 2, 4, 8, 8],\n",
    "            factors =  factors,\n",
    "            num_blocks = [8, 8, 8, 8],\n",
    "        )\n",
    "\n",
    "        self.latent_encoder_ema = deepcopy(self.latent_encoder)\n",
    "\n",
    "        self.diffusion = DiffusionAttnUnet1D(\n",
    "            io_channels=self.latent_dim, \n",
    "            cond_dim = self.second_stage_latent_dim,\n",
    "            n_attn_layers=0, \n",
    "            c_mults=[512] * 10,\n",
    "            depth=10\n",
    "        )\n",
    "\n",
    "        self.diffusion_ema = deepcopy(self.diffusion)\n",
    "\n",
    "        self.autoencoder = autoencoder\n",
    "\n",
    "        self.autoencoder.requires_grad_(False)\n",
    "        \n",
    "        self.rng = torch.quasirandom.SobolEngine(1, scramble=True)\n",
    "\n",
    "    def encode(self, reals):\n",
    "        first_stage_latents = self.autoencoder.encode(reals)\n",
    "\n",
    "        second_stage_latents = self.latent_encoder(first_stage_latents)\n",
    "\n",
    "        second_stage_latents = torch.tanh(second_stage_latents)\n",
    "\n",
    "        return second_stage_latents\n",
    "\n",
    "    def decode(self, latents, steps=100, device=\"cuda\"):\n",
    "        first_stage_latent_noise = torch.randn([latents.shape[0], self.latent_dim, latents.shape[2]*self.latent_downsampling_ratio]).to(device)\n",
    "\n",
    "        first_stage_sampled = sample(self.diffusion, first_stage_latent_noise, steps, 0, cond=latents)\n",
    "        decoded = self.autoencoder.decode(first_stage_sampled)\n",
    "        return decoded\n",
    "\n",
    "    def load_ema_weights(self, ema_state_dict):\n",
    "        own_state = self.state_dict()\n",
    "        for name, param in ema_state_dict.items():\n",
    "            if name.startswith(\"latent_encoder_ema.\"):\n",
    "                new_name = name.replace(\"latent_encoder_ema.\", \"latent_encoder.\")\n",
    "                if isinstance(param, Parameter):\n",
    "                    # backwards compatibility for serialized parameters\n",
    "                    param = param.data\n",
    "                own_state[new_name].copy_(param)\n",
    "            if name.startswith(\"diffusion_ema.\"):\n",
    "                new_name = name.replace(\"diffusion_ema.\", \"diffusion.\")\n",
    "                if isinstance(param, Parameter):\n",
    "                    # backwards compatibility for serialized parameters\n",
    "                    param = param.data\n",
    "                own_state[new_name].copy_(param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "class StackedAELatentDiffusionCond(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Note: this part is only needed for the 'generative' model.  \n",
    "    source: /fsx/zqevans/code/audio-diffusion/train_stacked_latent_dvae_ss_adp_cond.py\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_ae: LatentAudioDiffusionAutoencoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_ae.second_stage_latent_dim\n",
    "        self.downsampling_ratio = latent_ae.downsampling_ratio\n",
    "\n",
    "        #embedding_max_len = 64\n",
    "\n",
    "        embedding_max_len = 128\n",
    "\n",
    "        self.embedder = T5Embedder(model='t5-base', max_length=embedding_max_len).requires_grad_(False)\n",
    "\n",
    "        self.embedding_features = 768\n",
    "\n",
    "        # self.diffusion = UNetConditional1d(\n",
    "        #     in_channels = self.latent_dim, \n",
    "        #     context_embedding_features = self.embedding_features,\n",
    "        #     context_embedding_max_length= embedding_max_len,\n",
    "        #     channels = 256,\n",
    "        #     patch_blocks = 1,\n",
    "        #     patch_factor = 1,\n",
    "        #     resnet_groups = 8,\n",
    "        #     kernel_multiplier_downsample = 2,\n",
    "        #     multipliers = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "        #     factors = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "        #     num_blocks = [3, 3, 3, 3, 3, 3, 4, 4, 4, 4],\n",
    "        #     attentions = [0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3],\n",
    "        #     attention_heads = 16,\n",
    "        #     attention_features = 64,\n",
    "        #     attention_multiplier = 4,\n",
    "        #     attention_use_rel_pos=False,\n",
    "        #     use_nearest_upsample = False,\n",
    "        #     use_skip_scale = True,\n",
    "        #     use_context_time = True,\n",
    "        #     use_magnitude_channels = False\n",
    "        # )\n",
    "\n",
    "        self.diffusion = UNetConditional1d(\n",
    "            in_channels = self.latent_dim, \n",
    "            context_embedding_features = self.embedding_features,\n",
    "            context_embedding_max_length= embedding_max_len,\n",
    "            channels = 256,\n",
    "            patch_blocks = 1,\n",
    "            patch_factor = 1,\n",
    "            resnet_groups = 8,\n",
    "            kernel_multiplier_downsample = 2,\n",
    "            multipliers = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "            factors = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "            num_blocks = [3, 3, 3, 3, 3, 3, 4, 4, 4, 4],\n",
    "            attentions = [0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3],\n",
    "            attention_heads = 16,\n",
    "            attention_features = 64,\n",
    "            attention_multiplier = 4,\n",
    "            attention_use_rel_pos=False,\n",
    "            use_nearest_upsample = False,\n",
    "            use_skip_scale = True,\n",
    "            use_context_time = True,\n",
    "            use_magnitude_channels = False\n",
    "        )\n",
    "        \n",
    "        self.diffusion_ema = EMA(\n",
    "            self.diffusion,\n",
    "            beta = 0.9999,\n",
    "            power=3/4,\n",
    "            update_every = 1,\n",
    "            update_after_step = 1\n",
    "        )\n",
    "\n",
    "        self.autoencoder = latent_ae\n",
    "\n",
    "        self.autoencoder.requires_grad_(False)\n",
    "        \n",
    "        self.rng = torch.quasirandom.SobolEngine(1, scramble=True)\n",
    "\n",
    "    def encode(self, reals):\n",
    "        return self.autoencoder.encode(reals)\n",
    "\n",
    "    def decode(self, latents, steps=100):\n",
    "        return self.autoencoder.decode(latents, steps, device=self.device)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #return optim.Adam([*self.diffusion.parameters()], lr=1e-4)\n",
    "        optimizer = optim.Adam([*self.diffusion.parameters()], lr=1e-4)\n",
    "\n",
    "        #scheduler = InverseLR(optimizer, inv_gamma=50000, power=1/2, warmup=0.9)\n",
    "\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        reals, infos = batch\n",
    "\n",
    "        filenames = infos[\"path\"]\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                latents = self.encode(reals)\n",
    "                embeddings = self.embedder(filenames)\n",
    "\n",
    "        # Draw uniformly distributed continuous timesteps\n",
    "        t = self.rng.draw(reals.shape[0])[:, 0].to(self.device)\n",
    "\n",
    "        # Calculate the noise schedule parameters for those timesteps\n",
    "        alphas, sigmas = get_alphas_sigmas(t)\n",
    "\n",
    "        # Combine the ground truth images and the noise\n",
    "        alphas = alphas[:, None, None]\n",
    "        sigmas = sigmas[:, None, None]\n",
    "        noise = torch.randn_like(latents)\n",
    "        noised_latents = latents * alphas + noise * sigmas\n",
    "        targets = noise * alphas - latents * sigmas\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # 0.1 CFG dropout\n",
    "            v = self.diffusion(noised_latents, t, embedding=embeddings, embedding_mask_proba = 0.1)\n",
    "            mse_loss = F.mse_loss(v, targets)\n",
    "            loss = mse_loss\n",
    "\n",
    "        log_dict = {\n",
    "            'train/loss': loss.detach(),\n",
    "            'train/mse_loss': mse_loss.detach(),\n",
    "            'train/lr': self.lr_schedulers().get_last_lr()[0]\n",
    "        }\n",
    "\n",
    "        self.log_dict(log_dict, prog_bar=True, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def on_before_zero_grad(self, *args, **kwargs):\n",
    "        self.diffusion_ema.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.7.4 to v1.9.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../checkpoints/stacked-diffae-more-310k.ckpt`\n"
     ]
    }
   ],
   "source": [
    "first_stage_config = {\"capacity\": 64, \"c_mults\": [2, 4, 8, 16, 32], \"strides\": [2, 2, 2, 2, 2], \"latent_dim\": 32}\n",
    "first_stage_autoencoder = AudioAutoencoder( **first_stage_config ).requires_grad_(False)\n",
    "\n",
    "ckpt_path = '/fsx/shawley/checkpoints/stacked-diffae-more-310k.ckpt'\n",
    "\n",
    "latent_diffae = LatentAudioDiffusionAutoencoder.load_from_checkpoint(ckpt_path, \n",
    "                    autoencoder=first_stage_autoencoder, strict=True).requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LICENSE(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|export\n",
    "'''\n",
    "Besides the main LICENSE for this library overall, this particular file uses code by \n",
    "Zach Evans, who used some of Phil Wang's codes.  The licenses for those are as follows:\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2022,2023 Zach Evans\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2022 Phil Wang\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "'''\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "aa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
