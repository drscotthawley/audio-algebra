{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| default_exp StackedAELatentDiffusionCond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StackedAELatentDiffusionCond\n",
    "\n",
    "> This is a version of one of Zach Evans' \"stacked\" conditioned diffusion-based autoencoders, for use with audio-algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: For the 'Wrapper' version of this, see given_models.ipynb**\n",
    "\n",
    "See Zach Evans' official [audio-diffusion](https://github.com/zqevans/audio-diffusion) for updates. \n",
    "\n",
    "Other parts of `audio-algebra` may use Scott Hawley's \"Python Packaging Fork\" of this: [https://github.com/drscotthawley/audio-diffusion](https://github.com/drscotthawley/audio-diffusion).  That fork is also \"out of date\" w.r.t. @zqevans' research; we'll sync back up someday!  See \"LICENSE(S)\" at the bottom of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "# old imports from DVAEWrapper\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import os, sys\n",
    "import subprocess\n",
    "from collections import namedtuple\n",
    "import numpy as np \n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch\n",
    "\n",
    "import torchaudio\n",
    "from torch import optim, nn, Tensor\n",
    "from torch import multiprocessing as mp\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import trange \n",
    "\n",
    "from einops import rearrange\n",
    "from nwt_pytorch import Memcodes\n",
    "\n",
    "# audio-diffusion imports\n",
    "from diffusion.pqmf import CachedPQMF as PQMF # may require some manual labor/ symlinking directories\n",
    "from encoders.encoders import AttnResEncoder1D\n",
    "#from autoencoders.soundstream import SoundStreamXLEncoder\n",
    "#from dvae.residual_memcodes import ResidualMemcodes\n",
    "from decoders.diffusion_decoder import DiffusionAttnUnet1D\n",
    "from diffusion.model import ema_update\n",
    "\n",
    "from autoencoders.models import AudioAutoencoder\n",
    "from audio_encoders_pytorch import Encoder1d\n",
    "from ema_pytorch import EMA\n",
    "from audio_diffusion_pytorch.modules import UNetCFG1d\n",
    "from audio_diffusion_pytorch import NumberEmbedder  # pip install audio-diffusion-pytorch==0.0.97\n",
    "#from audio_diffusion_pytorch.modules import UNetCFG1d\n",
    "\n",
    "import laion_clap \n",
    "from laion_clap.training.data import get_audio_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# not exported because only used for testing\n",
    "from aeiou.core import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "# Define the noise schedule and sampling loop\n",
    "def get_alphas_sigmas(t):\n",
    "    \"\"\"Returns the scaling factors for the clean image (alpha) and for the\n",
    "    noise (sigma), given a timestep.\"\"\"\n",
    "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
    "\n",
    "def alpha_sigma_to_t(alpha, sigma):\n",
    "    \"\"\"Returns a timestep, given the scaling factors for the clean image and for\n",
    "    the noise.\"\"\"\n",
    "    return torch.atan2(sigma, alpha) / math.pi * 2\n",
    "\n",
    "#def get_crash_schedule(t):\n",
    "#    sigma = torch.sin(t * math.pi / 2) ** 2\n",
    "#    alpha = (1 - sigma ** 2) ** 0.5\n",
    "#    return alpha_sigma_to_t(alpha, sigma)\n",
    "\n",
    "#from diffusion.utils import get_spliced_ddpm_cosine_schedul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.no_grad()\n",
    "def sample(model, x, steps, eta, step_list=None, **extra_args):\n",
    "    \"\"\"Draws samples from a model given starting noise.\"\"\"\n",
    "    print(\"sample: x.shape =\",x.shape)\n",
    "    ts = x.new_ones([x.shape[0]])\n",
    "    if step_list is None:        # Create the noise schedule\n",
    "        t = torch.linspace(1, 0, steps + 1)[:-1]\n",
    "    else:                        # Noise schedule already created & passed in \n",
    "        t = step_list\n",
    "    #print(\"sample: t = \",t)\n",
    "        \n",
    "    alphas, sigmas = get_alphas_sigmas(t)\n",
    "    steps = len(t)  # just in case len(step_list) didn't match up with steps\n",
    "\n",
    "    # The sampling loop\n",
    "    for i in trange(steps):\n",
    "\n",
    "        # Get the model output (v, the predicted velocity)\n",
    "        #with torch.cuda.amp.autocast():\n",
    "        v = model(x, ts * t[i], **extra_args).float()\n",
    "\n",
    "        # Predict the noise and the denoised image\n",
    "        pred = x * alphas[i] - v * sigmas[i]\n",
    "        eps = x * sigmas[i] + v * alphas[i]\n",
    "\n",
    "        # If we are not on the last timestep, compute the noisy image for the\n",
    "        # next timestep.\n",
    "        if i < steps - 1:\n",
    "            # If eta > 0, adjust the scaling factor for the predicted noise\n",
    "            # downward according to the amount of additional noise to add\n",
    "            ddim_sigma = eta * (sigmas[i + 1]**2 / sigmas[i]**2).sqrt() * \\\n",
    "                (1 - alphas[i]**2 / alphas[i + 1]**2).sqrt()\n",
    "            adjusted_sigma = (sigmas[i + 1]**2 - ddim_sigma**2).sqrt()\n",
    "\n",
    "            # Recombine the predicted noise and predicted denoised image in the\n",
    "            # correct proportions for the next step\n",
    "            x = pred * alphas[i + 1] + eps * adjusted_sigma\n",
    "\n",
    "            # Add the correct amount of fresh noise\n",
    "            if eta:\n",
    "                x += torch.randn_like(x) * ddim_sigma\n",
    "\n",
    "    # If we are on the last timestep, output the denoised image\n",
    "    #print(\"sample: pred.shape =\",pred.shape)\n",
    "    return pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def resample(model_fn, \n",
    "             audio,  # note this is not real audio but rather latents of init audio\n",
    "             steps=100, \n",
    "             eta=0, \n",
    "             sampler_type=\"v-ddim\", \n",
    "             noise_level=1.0, \n",
    "             device='cuda',\n",
    "             debug=True,\n",
    "             batch_size=1,\n",
    "             effective_length=2048, # TODO: make more flexible later hard-coded now for 22s runs. \n",
    "             **extra_args):\n",
    "    \"\"\"from Dance_Diffusion.ipynb: Noise the input\"\"\"\n",
    "    if debug: print(f\"resample: audio.shape = {audio.shape}, steps = {steps}, eta = {eta}, noise_level = {noise_level}\")\n",
    "    while len(audio.shape) < 3: \n",
    "        audio = audio.unsqueeze(0)  # add a batch dim and/or channel dim if needed\n",
    "    if debug: print(f\"resample: audio.shape = {audio.shape}\")\n",
    "    \n",
    "    batch_size=1\n",
    "    \n",
    "    if sampler_type.startswith(\"v-\"):\n",
    "        t = torch.linspace(0, 1, steps + 1, device=device)\n",
    "        step_list = t  # get_spliced_ddpm_cosine_schedule(t) #  get_crash_schedule(t)\n",
    "        step_list = step_list[step_list < noise_level]\n",
    "        alpha, sigma = get_alphas_sigmas(step_list[-1])\n",
    "        noised = torch.randn(audio.shape, device=device)\n",
    "\n",
    "        noised = audio.to(device) * alpha + noised * sigma\n",
    "        noise = noised\n",
    "        return sample(model_fn, noise, steps, eta, step_list=step_list.flip(0)[:-1], **extra_args)\n",
    "    else:\n",
    "        raise ValueError(f\"Sorry, sampler {sampler_type} not implemented yet\") \n",
    "        return None\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "class LatentAudioDiffusionAutoencoder(pl.LightningModule):\n",
    "    def __init__(self, autoencoder: AudioAutoencoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = autoencoder.latent_dim\n",
    "\n",
    "        self.second_stage_latent_dim = 32\n",
    "\n",
    "        factors = [2, 2, 2, 2]\n",
    "\n",
    "        self.latent_downsampling_ratio = np.prod(factors)\n",
    "\n",
    "        self.downsampling_ratio = autoencoder.downsampling_ratio * self.latent_downsampling_ratio\n",
    "\n",
    "        self.latent_encoder = Encoder1d(\n",
    "            in_channels=self.latent_dim,\n",
    "            out_channels = self.second_stage_latent_dim,\n",
    "            channels = 128,\n",
    "            multipliers = [1, 2, 4, 8, 8],\n",
    "            factors =  factors,\n",
    "            num_blocks = [8, 8, 8, 8],\n",
    "        )\n",
    "\n",
    "        self.latent_encoder_ema = deepcopy(self.latent_encoder)\n",
    "\n",
    "        self.diffusion = DiffusionAttnUnet1D(\n",
    "            io_channels=self.latent_dim,\n",
    "            cond_dim = self.second_stage_latent_dim,\n",
    "            n_attn_layers=0,\n",
    "            c_mults=[512] * 10,\n",
    "            depth=10\n",
    "        )\n",
    "\n",
    "        self.diffusion_ema = deepcopy(self.diffusion)\n",
    "\n",
    "        self.diffusion_ema.requires_grad_(False)\n",
    "        self.latent_encoder_ema.requires_grad_(False)\n",
    "\n",
    "        self.autoencoder = autoencoder\n",
    "\n",
    "        self.autoencoder.requires_grad_(False).eval()\n",
    "\n",
    "        self.rng = torch.quasirandom.SobolEngine(1, scramble=True)\n",
    "\n",
    "    def encode(self, reals):\n",
    "        first_stage_latents = self.autoencoder.encode(reals)\n",
    "\n",
    "        second_stage_latents = self.latent_encoder(first_stage_latents)\n",
    "\n",
    "        second_stage_latents = torch.tanh(second_stage_latents)\n",
    "\n",
    "        return second_stage_latents\n",
    "\n",
    "    def decode(self, latents, steps=100, device=\"cuda\", init_audio=None, init_strength=0.4, debug=True):\n",
    "        first_stage_latent_noise = torch.randn([latents.shape[0], self.latent_dim, latents.shape[2]*self.latent_downsampling_ratio]).to(device)\n",
    "        if init_audio is None: \n",
    "            first_stage_sampled = sample(self.diffusion, first_stage_latent_noise, steps, 0, cond=latents)\n",
    "        else:\n",
    "            first_stage_sampled = resample(self.diffusion, init_audio, steps, 0, cond=latents, noise_level=(1.0-init_strength))\n",
    "        first_stage_sampled = first_stage_sampled.clamp(-1, 1)\n",
    "        if debug: print(\"LatentAudioDiffusionAutoencoder: calling self.autoencoder.decode\")\n",
    "        decoded = self.autoencoder.decode(first_stage_sampled)\n",
    "        if debug: print(\"LatentAudioDiffusionAutoencoder: decoded.shape =\",decoded.shape)\n",
    "        return decoded\n",
    "\n",
    "    def load_ema_weights(self, ema_state_dict):\n",
    "        own_state = self.state_dict()\n",
    "        for name, param in ema_state_dict.items():\n",
    "            if name.startswith(\"latent_encoder_ema.\"):\n",
    "                new_name = name.replace(\"latent_encoder_ema.\", \"latent_encoder.\")\n",
    "                if isinstance(param, Parameter):\n",
    "                    # backwards compatibility for serialized parameters\n",
    "                    param = param.data\n",
    "                own_state[new_name].copy_(param)\n",
    "            if name.startswith(\"diffusion_ema.\"):\n",
    "                new_name = name.replace(\"diffusion_ema.\", \"diffusion.\")\n",
    "                if isinstance(param, Parameter):\n",
    "                    # backwards compatibility for serialized parameters\n",
    "                    param = param.data\n",
    "                own_state[new_name].copy_(param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "class StackedAELatentDiffusionCond(pl.LightningModule):\n",
    "    def __init__(self, latent_ae: LatentAudioDiffusionAutoencoder, clap_module: laion_clap.CLAP_Module):\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_ae.second_stage_latent_dim\n",
    "        self.downsampling_ratio = latent_ae.downsampling_ratio\n",
    "\n",
    "        embedding_max_len = 1\n",
    "\n",
    "        self.embedder = clap_module\n",
    "\n",
    "        self.embedding_features = 512\n",
    "\n",
    "       # self.timestamp_embedder = NumberEmbedder(features=self.embedding_features)\n",
    "\n",
    "        '''# for samples:\n",
    "        self.diffusion = UNetCFG1d(\n",
    "            in_channels = self.latent_dim, \n",
    "            context_embedding_features = self.embedding_features,\n",
    "            context_embedding_max_length = embedding_max_len, # + 2, #2 for timestep embeds\n",
    "            channels = 256,\n",
    "            resnet_groups = 8,\n",
    "            kernel_multiplier_downsample = 2,\n",
    "            multipliers = [2, 3, 4, 5],\n",
    "            factors = [1, 2, 4],\n",
    "            num_blocks = [3, 3, 3],\n",
    "            attentions = [1, 1, 1, 1],\n",
    "            attention_heads = 16,\n",
    "            attention_features = 64,\n",
    "            attention_multiplier = 4,\n",
    "            attention_use_rel_pos=True,\n",
    "            attention_rel_pos_max_distance=512,\n",
    "            attention_rel_pos_num_buckets=128,\n",
    "            use_nearest_upsample = False,\n",
    "            use_skip_scale = True,\n",
    "            use_context_time = True,\n",
    "        )'''\n",
    "        #for songs:\n",
    "        self.diffusion = UNetCFG1d(\n",
    "            in_channels = self.latent_dim, \n",
    "            context_embedding_features = self.embedding_features,\n",
    "            context_embedding_max_length = embedding_max_len, # + 2, #2 for timestep embeds\n",
    "            channels = 256,\n",
    "            resnet_groups = 8,\n",
    "            kernel_multiplier_downsample = 2,\n",
    "            multipliers = [2, 3, 4, 4, 4, 4],\n",
    "            factors = [1, 2, 2, 4, 4],\n",
    "            num_blocks = [3, 3, 3, 3, 3],\n",
    "            attentions = [0, 0, 2, 2, 2, 2],\n",
    "            attention_heads = 16,\n",
    "            attention_features = 64,\n",
    "            attention_multiplier = 4,\n",
    "            attention_use_rel_pos=True,\n",
    "            attention_rel_pos_max_distance=2048,\n",
    "            attention_rel_pos_num_buckets=256,\n",
    "            use_nearest_upsample = False,\n",
    "            use_skip_scale = True,\n",
    "            use_context_time = True,\n",
    "        )\n",
    "\n",
    "        self.diffusion_ema = EMA(\n",
    "            self.diffusion,\n",
    "            beta = 0.9999,\n",
    "            power=3/4,\n",
    "            update_every = 1,\n",
    "            update_after_step = 1\n",
    "        )\n",
    "\n",
    "        self.autoencoder = latent_ae\n",
    "\n",
    "        self.autoencoder.requires_grad_(False)\n",
    "\n",
    "        self.rng = torch.quasirandom.SobolEngine(1, scramble=True)\n",
    "\n",
    "    def encode(self, reals):\n",
    "        while len(reals.shape) < 3: reals = reals.unsqueeze(0)  # add batch and/or channel dims\n",
    "        return self.autoencoder.encode(reals)\n",
    "\n",
    "    def decode(self, latents, steps=100, init_audio=None, init_strength=0.4):\n",
    "        return self.autoencoder.decode(latents, steps, device=self.device, init_audio=init_audio, init_strength=init_strength)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #optimizer = optim.Adam([*self.diffusion.parameters(), *self.timestamp_embedder.parameters()], lr=4e-5)\n",
    "        optimizer = optim.Adam([*self.diffusion.parameters()], lr=4e-5)\n",
    "\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500, eta_min=1e-6)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        reals, jsons, timestamps = batch\n",
    "        reals = reals[0]\n",
    "\n",
    "        #timestamps = [[timestamp[0].item(), timestamp[1].item()] for timestamp in timestamps]\n",
    "        mono_reals = reals.mean(dim=1)\n",
    "\n",
    "        #with torch.cuda.amp.autocast():\n",
    "        #timestamp_embeddings = self.timestamp_embedder(timestamps)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            latents = self.encode(reals)\n",
    "            audio_embeddings = self.embedder.get_audio_embedding_from_data(mono_reals, use_tensor=True)\n",
    "            audio_embeddings = audio_embeddings.unsqueeze(1).to(self.device)\n",
    "            #print(audio_embeddings)\n",
    "            #print(audio_embeddings.shape)\n",
    "\n",
    "        embeddings = audio_embeddings #torch.cat([text_embeddings, timestamp_embeddings], dim=1)\n",
    "\n",
    "        # Draw uniformly distributed continuous timesteps\n",
    "        t = self.rng.draw(reals.shape[0])[:, 0].to(self.device)\n",
    "\n",
    "        # Calculate the noise schedule parameters for those timesteps\n",
    "        alphas, sigmas = get_alphas_sigmas(t)\n",
    "\n",
    "        # Combine the ground truth images and the noise\n",
    "        alphas = alphas[:, None, None]\n",
    "        sigmas = sigmas[:, None, None]\n",
    "        noise = torch.randn_like(latents)\n",
    "        noised_latents = latents * alphas + noise * sigmas\n",
    "        targets = noise * alphas - latents * sigmas\n",
    "\n",
    "        #with torch.cuda.amp.autocast():\n",
    "        # 0.1 CFG dropout\n",
    "        v = self.diffusion(noised_latents, t, embedding=embeddings, embedding_mask_proba = 0.1)\n",
    "        mse_loss = F.mse_loss(v, targets)\n",
    "        loss = mse_loss\n",
    "\n",
    "        log_dict = {\n",
    "            'train/loss': loss.detach(),\n",
    "            'train/mse_loss': mse_loss.detach(),\n",
    "            'train/lr': self.lr_schedulers().get_last_lr()[0],\n",
    "            'train/ema_decay': self.diffusion_ema.get_current_decay()\n",
    "        }\n",
    "\n",
    "        self.log_dict(log_dict, prog_bar=True, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def on_before_zero_grad(self, *args, **kwargs):\n",
    "        self.diffusion_ema.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "first_stage_config = {\"capacity\": 64, \"c_mults\": [2, 4, 8, 16, 32], \"strides\": [2, 2, 2, 2, 2], \"latent_dim\": 32}\n",
    "first_stage_autoencoder = AudioAutoencoder(**first_stage_config).eval()\n",
    "\n",
    "latent_diffae = LatentAudioDiffusionAutoencoder(first_stage_autoencoder).eval()\n",
    "\n",
    "clap_fusion, clap_amodel = True, \"HTSAT-base\"\n",
    "device= get_device()\n",
    "clap_module = laion_clap.CLAP_Module(enable_fusion=clap_fusion, device=device, amodel=clap_amodel).requires_grad_(False).eval()\n",
    "\n",
    "ldc_model = StackedAELatentDiffusionCond(latent_ae=latent_diffae, clap_module=clap_module)\n",
    "print(\"Success!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LICENSE(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|export\n",
    "'''\n",
    "Besides the main LICENSE for this library overall, this particular file uses code by \n",
    "Zach Evans, who used some of Phil Wang's codes.  The licenses for those are as follows:\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2022,2023 Zach Evans\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2022 Phil Wang\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "'''\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "aa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
