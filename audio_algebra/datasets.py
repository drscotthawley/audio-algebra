# AUTOGENERATED! DO NOT EDIT! File to edit: ../datasets.ipynb.

# %% ../datasets.ipynb 5
from __future__ import annotations  # for type hints, in LAION code samples
import numpy as np 
import torch
import torch.nn as nn
import torchaudio
from torchaudio import transforms as T
from torchvision import transforms as VT
import random
import os
import json
from tqdm.auto import tqdm
from multiprocessing import Pool, cpu_count
from functools import partial
from aeiou.core import load_audio, get_audio_filenames, is_silence, untuple
from aeiou.viz import playable_spectrogram
from fastcore.utils import *
import webdataset as wds
import subprocess
import re
# import pedalboard  # not using pedalboard atm just cause audiomentations is really nice
from audiomentations import *   # list of effects 
from aeiou.datasets import Stereo, PhaseFlipper, PadCrop

# %% auto 0
__all__ = ['DualEffectsDataset']

# %% ../datasets.ipynb 7
class DualEffectsDataset(torch.utils.data.Dataset):
    """
    For each __getitem_, this ill grab two bits of audio and apply the same effect to both of them. 
    """
    def __init__(self, 
        paths,             # list of strings of directory (/tree) names to draw audio files from
        filenames=None,    # allow passing in the list of filenames again (e.g. for val set) to skip searching them all
        sample_rate=48000, # audio sample rate in Hz
        sample_size=65536, # how many audio samples in each "chunk"
        random_crop=True,  # take chunks from random positions within files
        load_frac=1.0,     # fraction of total dataset to load
        num_gpus=8,        # used only when `cache_training_data=True`, to avoid duplicates,
        redraw_silence=True, # a chunk containing silence will be replaced with a new one
        silence_thresh=-60,  # threshold in dB below which we declare to be silence
        max_redraws=2,        # when redrawing silences, don't do it more than this many
        augs='Stereo(), PhaseFlipper()', # list of augmentation transforms **after PadCrop**, as a string
        #effects=[],         # these are 'different' from augmentations. these are possible effects to choose 1 from (randomly)
        effects_list = [Gain, BandPassFilter, BandStopFilter, HighPassFilter, LowPassFilter],# , PitchShift, TanhDistortion],
        verbose=False,       # whether to print notices of reasampling or not
        ):
        super().__init__()
    
        print("augs =",augs)  # augs are applied to ALL samples, regardless
        print("effects_list = ",[x().__class__.__name__ for x in effects_list])  # effects are specifically controlled
        self.effects_list = [x(p=1.0) for x in effects_list]  # make probability of transform = 1
        
        # base_augs are always applied
        base_augs = 'PadCrop(sample_size, randomize=random_crop, redraw_silence=redraw_silence, silence_thresh=silence_thresh, max_redraws=max_redraws)'
        self.augs = eval(f'torch.nn.Sequential( {base_augs}, {augs} )')  if augs is not None else None 
        self.silence_thresh = silence_thresh
        self.redraw_silence = redraw_silence
        self.max_redraws = max_redraws
        self.sr = sample_rate
        self.verbose = verbose

        self.filenames = get_audio_filenames(paths) if filenames is None else filenames
        print(f"AudioDataset:{len(self.filenames)} files found.")
        self.n_files = int(len(self.filenames)*load_frac)
        self.filenames = self.filenames[0:self.n_files]

    def load_file_ind(self, file_list,i): # used when caching training data
        return load_audio(file_list[i], sr=self.sr, verbose=self.verbose).cpu()

    def __len__(self):
        return len(self.filenames)
       
    def get_next_chunk(self, 
        idx,     # the index of the file within the list of files
        ):
        "The heart of this whole dataset routine: Loads file, crops & runs other augmentations"
        audio_filename = self.filenames[idx]
        try:
            audio = load_audio(audio_filename, sr=self.sr, verbose=self.verbose)
            x = audio  
            x = self.augs(x)      # RUN AUGMENTATION PIPELINE (not other effects though)
            x = x.clamp(-1, 1)
            return x
        except Exception as e:
            print(f'AudioDataset.get_next_chunk: Error loading file {audio_filename}: {e}')
            return None
    
    def get_nonsilent_chunk(self, idx):
        "this tries to make sure we're not just getting silence"
        x = self.get_next_chunk(idx)  # x is either audio or a dict, depending on self.return_dict
        audio = x if not isinstance(x, dict) else x['inputs']
        
        # even with PadCrop set to reject silences, it could be that the whole file is silence; 
        num_redraws = 0 
        while (audio is None) or (self.redraw_silence and is_silence(audio, thresh=self.silence_thresh) \
            and (num_redraws < self.max_redraws)):
            next_idx = random.randint(0,len(self.filenames)-1)     # pick some other file at random
            x, num_redraws = self.get_next_chunk(next_idx), num_redraws+1
            audio = x if not isinstance(x, dict) else x['inputs']
    
        if self.verbose: print("__getitem__: x =",x)
        return self[random.randrange(len(self))] if (x is None) else x
    
    def apply_effect(self, audio, effect):
        return torch.from_numpy( effect( audio.numpy(), sample_rate=self.sr ).copy() )
    
    def check_size(self, a, b): # force everything to be same size as a 
        if a.shape[-1] < b.shape[-1]: # sometimes reverb makes b longer
            return b[:,:a.shape[-1]]
        return b 
  
    def __getitem__(self, 
        idx     # the index of the file within the list of files
        ):
        "two audio clips and two effects applied on each, and the names of the effects"
        # a and b are 'clean' ie non-effected
        a = self.get_nonsilent_chunk(idx)  # chunk a
        b = self.get_nonsilent_chunk(random.randint(0,len(self.filenames)-1) ) #chunk b - from some other random file
        effect1 = random.choice(self.effects_list)
        effect2 = random.choice([e for e in self.effects_list if e != effect1])  # effect2 is different from effect1
        [a1, b1]  = [ self.apply_effect(x, effect1) for x in [a,b] ]
        [a2, b2]  = [ self.apply_effect(x, effect2) for x in [a,b] ]
        [b, a1, b1, a2, b2] = [ self.check_size(a, x) for x in [b, a1, b1, a2, b2]]
        out =  dict(zip(["a","b", "a1","b1", "a2","b2", "e1", "e2"], [a,b, a1,b1, a2,b2, effect1.__class__.__name__, effect2.__class__.__name__])) 
        return out
    
