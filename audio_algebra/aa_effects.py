# AUTOGENERATED! DO NOT EDIT! File to edit: ../aa-effects.ipynb.

# %% auto 0
__all__ = ['mseloss', 'EmbedBlock', 'AudioAlgebra', 'do_mixing', 'aa_demo', 'vicreg_var_loss', 'off_diagonal', 'vicreg_cov_loss',
           'train_aa_model']

# %% ../aa-effects.ipynb 4
from prefigure.prefigure import get_all_args, push_wandb_config
from copy import deepcopy
import math
import json
import subprocess
import os, sys
import random
from IPython.display import display, Image, Audio, HTML
import matplotlib.pyplot as plt
import numpy as np

import accelerate
import torch
import torchaudio
from torch import optim, nn, Tensor
from torch import multiprocessing as mp
from torch.nn import functional as F
from torch.utils import data as torchdata

#from tqdm.auto import tqdm, trange
from tqdm import tqdm, trange
from einops import rearrange, repeat

import wandb

from aeiou.viz import embeddings_table, pca_point_cloud, show_pca_point_cloud, audio_spectrogram_image, tokens_spectrogram_image, playable_spectrogram
from aeiou.hpc import load, save, HostPrinter, freeze
#from aeiou.datasets import AudioDataset
from .datasets import DualEffectsDataset
from .aa_mixer import * 


# audio-diffusion imports
import pytorch_lightning as pl
from diffusion.pqmf import CachedPQMF as PQMF
from encoders.encoders import AttnResEncoder1D
from autoencoders.soundstream import SoundStreamXLEncoder
from dvae.residual_memcodes import ResidualMemcodes
from decoders.diffusion_decoder import DiffusionAttnUnet1D
from diffusion.model import ema_update
from audiomentations import *   # list of effects 

# %% ../aa-effects.ipynb 21
class EmbedBlock(nn.Module):
    def __init__(self, in_dims:int, out_dims:int, act=nn.GELU(), resid=True, use_bn=False, requires_grad=True, **kwargs) -> None:
        "generic little block for embedding stuff.  note residual-or-not doesn't seem to make a huge difference for a-a"
        super().__init__()
        self.in_dims, self.out_dims, self.act, self.resid = in_dims, out_dims, act, resid
        self.lin = nn.Linear(in_dims, out_dims, **kwargs)
        self.bn = nn.BatchNorm1d(out_dims) if use_bn else None # even though points in 2d, only one non-batch dim in data

        if requires_grad == False:
            self.lin.weight.requires_grad = False
            self.lin.bias.requires_grad = False

    def forward(self, xin: Tensor) -> Tensor:
        x = self.lin(xin)
        if self.act is not None: x = self.act(x)
        if self.bn is not None: x = self.bn(x)   # re. "BN before or after Activation? cf. https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md"
        return xin + x if (self.resid and self.in_dims==self.out_dims) else x 

# %% ../aa-effects.ipynb 22
class AudioAlgebra(nn.Module):
    """
    Main AudioAlgebra model. Contrast to aa-mixer code, keep this one simple & move mixing stuff outside
    """
    def __init__(self, 
                 dims=32, 
                 hidden_dims=64, 
                 act=nn.GELU(), 
                 use_bn=False, 
                 resid=True, 
                 block=EmbedBlock, 
                 trivial=False,   # ignore everything and make this an identity mapping
                ):
        super().__init__()
        self.resid, self.trivial = resid, trivial
        self.encoder = nn.Sequential(  
            block( dims,        hidden_dims, act=act,  use_bn=use_bn, resid=resid),
            block( hidden_dims, hidden_dims, act=act,  use_bn=use_bn, resid=resid),
            block( hidden_dims, hidden_dims, act=act,  use_bn=use_bn, resid=resid),
            block( hidden_dims, dims,        act=None, use_bn=use_bn, resid=resid),
        )
        self.decoder = nn.Sequential(  # same as encoder, in fact. 
            block( dims,        hidden_dims, act=act,  use_bn=use_bn, resid=resid),
            block( hidden_dims, hidden_dims, act=act,  use_bn=use_bn, resid=resid),
            block( hidden_dims, hidden_dims, act=act,  use_bn=use_bn, resid=resid),   
            block( hidden_dims, dims,        act=None, use_bn=use_bn, resid=resid),
        )
            
    def encode(self,xin):
        if self.trivial: return xin 
        x = self.encoder(xin.transpose(1,2)).transpose(1,2) # transpose is just so embeddings dim goes last for matrix mult
        return x + xin if self.resid else x

    def decode(self,xin):
        if self.trivial: return xin 
        x = self.decoder(xin.transpose(1,2)).transpose(1,2)
        return x + xin if self.resid else x

    def forward(self, 
        x   # the embedding vector from the given encoder
        ):
        xprime = self.encode(x)
        xprimeprime = self.decode(xprime)  # train system to invert itself (and hope it doesn't all collapse to nothing!)
        return xprime, xprimeprime  # encoder output,  decoder output

# %% ../aa-effects.ipynb 23
def do_mixing(batch, given_model, aa_model, device, debug=False):
    """
    here we actually encode inputs and embed them.
    """
    [a1, b1, a2, b2] = [x.to(device) for x in [batch["a1"], batch["b1"], batch["a2"], batch["b2"]]]
    ys = [given_model.encode(x) for x in [a1, b1, a2, b2]]
    zs = [aa_model.encode(x) for x in ys]
    yrecons = [aa_model.decode(x) for x in zs]
    archive = {'ys': ys, 'zs':zs, 'yrecons':yrecons} 
    return archive

# %% ../aa-effects.ipynb 32
def aa_demo(given_model, aa_model, log_dict, zsum, zmix, step, demo_steps=35, sr=48000):
    "log decoded audio for zsum and zmix"
    with torch.no_grad():
        for var,name in zip([zsum, zmix],['zsum','zmix']):
            var = aa_model.decode(var)
            fake_audio = given_model.decode_it(var, demo_steps=demo_steps)
            filename = f'{name}_{step:08}.wav'
            fake_audio = fake_audio.clamp(-1, 1).mul(32767).to(torch.int16).cpu()
            torchaudio.save(filename, fake_audio, self.sample_rate)
            log_dict[name] = wandb.Audio(filename, sample_rate=sr, caption=name)   
            #log_dict[f'{name}_spec'] = wandb.Image( tokens_spectrogram_image(var.detach()) )
    return log_dict

# %% ../aa-effects.ipynb 34
mseloss = nn.MSELoss()

#def rel_loss(y_pred: torch.Tensor, y: torch.Tensor, eps=1e-3) -> float:
#    "relative error loss   --- note we're never going to actually use this. it was just part of development"
#    e = torch.abs(y.view_as(y_pred) - y_pred) / ( torch.abs(y.view_as(y_pred)) + eps ) 
#    return torch.median(e)

def vicreg_var_loss(z, gamma=1, eps=1e-4):
    std_z = torch.sqrt(z.var(dim=0) + eps)
    return torch.mean(F.relu(gamma - std_z))   # the relu gets us the max(0, ...)

def off_diagonal(x):
    n, m = x.shape
    assert n == m
    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()

def vicreg_cov_loss(z):
    "the regularization term that is the sum of the off-diagaonal terms of the covariance matrix"
    num_features = z.shape[1]*z.shape[2]  # TODO: move this out for speed.
    cov_z = torch.cov(rearrange(z, 'b c t -> ( c t ) b'))   
    return off_diagonal(cov_z).pow_(2).sum().div(num_features)

# %% ../aa-effects.ipynb 37
def train_aa_model(debug=False):   
    "train our aa projector, uses global variables, not sorry"
    global train_dl, given_model 
    
    max_epochs = 40
    lossinfo_every, viz_demo_every =   1, 10000000   # in units of steps
    checkpoint_every = 10000
    max_lr= 0.001
    total_steps = len(train_set)//args.batch_size * max_epochs
    print("total_steps =",total_steps)  # for when I'm checking wandb

    hprint(f"Setting up AA model using device: {device}")
    #aa_model = AudioAlgebra(global_args, device, autoencoder, trivial=True)


    
    torch.manual_seed(seed) # chose this value because it shows of nice nonlinearity
    aa_model  = AudioAlgebra(dims=emb_dims, hidden_dims=hidden_dims, use_bn=aa_use_bn, resid=aa_use_resid).to(device) 
    opt       = optim.Adam([*aa_model.parameters()], lr=5e-4)  # Adam optimizer
    scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=max_lr, total_steps=total_steps)
    
    aa_model, opt, train_dl, given_model, scheduler = accelerator.prepare(aa_model, opt, train_dl, given_model, scheduler)
    
    wandb.init(project='aa-effects')
   
    # training loop
    train_iter = iter(train_dl) # this is only for use with get_stems_faders
    epoch, step = 0, 0
    torch.manual_seed(seed) # for reproducibility
    while (epoch < max_epochs) or (max_epochs < 0):  # training loop
        with tqdm(train_dl, unit="batch", disable=not accelerator.is_main_process) as tepoch:
            for batch in tepoch:   # train
                #print("step = ",step)
                opt.zero_grad()
                log_dict = {}
                #batch = batch.to(device)

                # vicreg: 1. invariance
                archive = do_mixing(batch, accelerator.unwrap_model(given_model), accelerator.unwrap_model(aa_model), device, debug=debug)
                za1, zb1, za2, zb2 = archive["zs"]
                za2_guess = za1 + (zb2 - zb1)
                zb2_guess = zb1 + (za2 - za1)
                mix_loss = mseloss(za2_guess, za2)  +  mseloss(zb2_guess, zb2)

                var_loss = (vicreg_var_loss(za2_guess) + vicreg_var_loss(zb2_guess))/2    # vicreg: 2. variance
                cov_loss = (vicreg_cov_loss(za2_guess) + vicreg_cov_loss(zb2_guess))/2    # vicreg: 3. covariance


                # reconstruction loss: inversion of aa map h^{-1}(z): z -> y,  i.e. train the aa decoder
                aa_recon_loss = mseloss(archive["yrecons"][0], archive["ys"][0])
                for i in range(1,4):
                    aa_recon_loss += mseloss(archive["yrecons"][i], archive["ys"][i]) 
           
                loss = mix_loss + var_loss + cov_loss + aa_recon_loss     # --- full loss function
                
                log_dict['train_loss'] = loss.detach()                    # --- this is the full loss 
                log_dict['mix_loss'] = mix_loss.detach() 
                log_dict['aa_recon_loss'] = aa_recon_loss.detach()
                log_dict['var_loss'] = var_loss.detach() 
                log_dict['cov_loss'] = cov_loss.detach() 
                log_dict['learning_rate'] = opt.param_groups[0]['lr']
                log_dict['epoch'] = epoch

                #if step % lossinfo_every == 0  and accelerator.is_main_process: 
                tepoch.set_description(f"Epoch {epoch+1}/{max_epochs}")
                tepoch.set_postfix(loss=loss.item())         #  TODO: use EMA for loss display? 

                accelerator.backward(loss)  #loss.backward()
                opt.step()  
                
                if accelerator.is_main_process:
                    if  False and step % viz_demo_every == 0:
                         log_dict = aa_demo(accelerator.unwrap_model(given_model), accelerator.unwrap_model(aa_model), log_dict, zsum, zmix, step)

                    if False and step % checkpoint_every == 0:
                        save_aa_checkpoint(aa_model, suffix=RUN_SUFFIX+f"_{step}")

                    wandb.log(log_dict)

                scheduler.step()   
                step += 1

        epoch += 1
    #----- training loop finished
    
    save_aa_checkpoint(accelerator.unwrap_model(aa_model), suffix=RUN_SUFFIX+f"_{step}")
    
