{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8d5003ba",
   "metadata": {},
   "source": [
    "---\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3a673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp aa_mixer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4d264b",
   "metadata": {},
   "source": [
    "# aa_mixer\n",
    "\n",
    "> Trying to map audio embeddings to vector spaces, for mixing.\n",
    "\n",
    "We try to make the sum of the embeddings of solo parts, equal(/close) to the embedding of the sum (i.e. the full mix).\n",
    "\n",
    "Based on `accelerate`-powered code by Zach Evans & Katherine Crowson, cf. https://github.com/zqevans/audio-diffusion/blob/main/train_diffgan_accel.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64916914",
   "metadata": {},
   "outputs": [],
   "source": [
    "install = False  # can set to false to skip this part, e.g. for re-running in same session\n",
    "if install:     # ffmpeg is to add MP3 support to Colab\n",
    "    !yes | sudo apt install ffmpeg \n",
    "    !pip install -Uqq einops gdown \n",
    "    !pip install -Uqq git+https://github.com/drscotthawley/aeiou\n",
    "    !pip install -Uqq git+https://github.com/drscotthawley/audio-algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ce10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from prefigure.prefigure import get_all_args, push_wandb_config\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import json\n",
    "import subprocess\n",
    "import os, sys\n",
    "import random\n",
    "from IPython.display import display, Image, Audio, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import accelerate\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import optim, nn, Tensor\n",
    "from torch import multiprocessing as mp\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data as torchdata\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "import wandb\n",
    "\n",
    "from aeiou.viz import embeddings_table, pca_point_cloud, show_pca_point_cloud, audio_spectrogram_image, tokens_spectrogram_image, playable_spectrogram\n",
    "from aeiou.hpc import load, save, HostPrinter, freeze\n",
    "from aeiou.datasets import AudioDataset\n",
    "\n",
    "# audio-diffusion imports\n",
    "import pytorch_lightning as pl\n",
    "from diffusion.pqmf import CachedPQMF as PQMF\n",
    "from encoders.encoders import AttnResEncoder1D\n",
    "from autoencoders.soundstream import SoundStreamXLEncoder\n",
    "from dvae.residual_memcodes import ResidualMemcodes\n",
    "from decoders.diffusion_decoder import DiffusionAttnUnet1D\n",
    "from diffusion.model import ema_update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9063f64",
   "metadata": {},
   "source": [
    "# Basic setup of hardware environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a08186",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = accelerate.Accelerator()\n",
    "hprint = HostPrinter(accelerator)  # this just prints only on interactive node\n",
    "device = accelerator.device\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "#if torch.backends.mps.is_available():\n",
    "#    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "print(\"device = \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74b12ef",
   "metadata": {},
   "source": [
    "Main parameters for the run/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa0e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2\n",
    "\n",
    "args_dict = {'num_quantizers':0, 'sample_size': 65536, 'sample_rate':48000, 'latent_dim': 64, 'pqmf_bands':1, 'ema_decay':0.995, 'num_quantizers':0}\n",
    "#global_args = namedtuple(\"global_args\", args_dict.keys())(*args_dict.values())\n",
    "class DictObj:\n",
    "    def __init__(self, in_dict:dict):\n",
    "        assert isinstance(in_dict, dict), \"in_dict is not a dict\"\n",
    "        for key, val in in_dict.items():\n",
    "            if isinstance(val, (list, tuple)):\n",
    "               setattr(self, key, [DictObj(x) if isinstance(x, dict) else x for x in val])\n",
    "            else:\n",
    "               setattr(self, key, DictObj(val) if isinstance(val, dict) else val)\n",
    "\n",
    "global_args = DictObj(args_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f779853",
   "metadata": {},
   "source": [
    "# Set Up Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac31b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprint(\"Setting up dataset\")\n",
    "args = global_args\n",
    "args.training_dir =  f'{os.getenv(\"HOME\")}/datasets/BDCT-0-chunk-48000'\n",
    "args.num_workers = 2\n",
    "\n",
    "\n",
    "args.batch_size = 256\n",
    "\n",
    "load_frac = 0.1\n",
    "torch.manual_seed(seed)\n",
    "train_set = AudioDataset([args.training_dir], load_frac=load_frac)\n",
    "train_dl = torchdata.DataLoader(train_set, args.batch_size, shuffle=True,\n",
    "                num_workers=args.num_workers, persistent_workers=True, pin_memory=True)\n",
    "\n",
    "# TODO: need to make val unique. for now just repeat train\n",
    "val_set = AudioDataset([args.training_dir], load_frac=load_frac/4)\n",
    "val_dl = torchdata.DataLoader(train_set, args.batch_size, shuffle=False,\n",
    "                num_workers=args.num_workers, persistent_workers=True, pin_memory=True)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "val_iter = iter(val_dl)\n",
    "train_iter = iter(train_dl)\n",
    "\n",
    "print(\"len(train_set), len(val_set) =\",len(train_set), len(val_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b08d3e3",
   "metadata": {},
   "source": [
    "And let's listen to a bit of audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_iter) \n",
    "batch = next(val_iter)  # two nexts bc i don't like the first one\n",
    "print(\"batch.shape = \",batch.shape)\n",
    "playable_spectrogram(batch[0], output_type='live') # clear this output later if you want to keep .ipynb file size small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda07ff",
   "metadata": {},
   "source": [
    "## Set up the Given [Auto]Encoder Model(s)\n",
    "\n",
    " Note that initially we're *only* going to be using the encoder part.\n",
    " The decoder -- with all of its sampling code, etc. -- will be useful eventualy, and we' go ahead and define it.  But fyi it won't be used *at all* while training the AA mixer model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad247898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#audio-diffusion stuff \n",
    "# Define the noise schedule and sampling loop\n",
    "def get_alphas_sigmas(t):\n",
    "    \"\"\"Returns the scaling factors for the clean image (alpha) and for the\n",
    "    noise (sigma), given a timestep.\"\"\"\n",
    "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
    "\n",
    "def get_crash_schedule(t):\n",
    "    sigma = torch.sin(t * math.pi / 2) ** 2\n",
    "    alpha = (1 - sigma ** 2) ** 0.5\n",
    "    return alpha_sigma_to_t(alpha, sigma)\n",
    "\n",
    "def alpha_sigma_to_t(alpha, sigma):\n",
    "    \"\"\"Returns a timestep, given the scaling factors for the clean image and for\n",
    "    the noise.\"\"\"\n",
    "    return torch.atan2(sigma, alpha) / math.pi * 2\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, x, steps, eta, logits):\n",
    "    \"\"\"Draws samples from a model given starting noise.\"\"\"\n",
    "    ts = x.new_ones([x.shape[0]])\n",
    "\n",
    "    # Create the noise schedule\n",
    "    t = torch.linspace(1, 0, steps + 1)[:-1]\n",
    "\n",
    "    t = get_crash_schedule(t)\n",
    "    \n",
    "    alphas, sigmas = get_alphas_sigmas(t)\n",
    "\n",
    "    # The sampling loop\n",
    "    for i in trange(steps):\n",
    "\n",
    "        # Get the model output (v, the predicted velocity)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            v = model(x, ts * t[i], logits).float()\n",
    "\n",
    "        # Predict the noise and the denoised image\n",
    "        pred = x * alphas[i] - v * sigmas[i]\n",
    "        eps = x * sigmas[i] + v * alphas[i]\n",
    "\n",
    "        # If we are not on the last timestep, compute the noisy image for the\n",
    "        # next timestep.\n",
    "        if i < steps - 1:\n",
    "            # If eta > 0, adjust the scaling factor for the predicted noise\n",
    "            # downward according to the amount of additional noise to add\n",
    "            ddim_sigma = eta * (sigmas[i + 1]**2 / sigmas[i]**2).sqrt() * \\\n",
    "                (1 - alphas[i]**2 / alphas[i + 1]**2).sqrt()\n",
    "            adjusted_sigma = (sigmas[i + 1]**2 - ddim_sigma**2).sqrt()\n",
    "\n",
    "            # Recombine the predicted noise and predicted denoised image in the\n",
    "            # correct proportions for the next step\n",
    "            x = pred * alphas[i + 1] + eps * adjusted_sigma\n",
    "\n",
    "            # Add the correct amount of fresh noise\n",
    "            if eta:\n",
    "                x += torch.randn_like(x) * ddim_sigma\n",
    "\n",
    "    # If we are on the last timestep, output the denoised image\n",
    "    return pred\n",
    "\n",
    "\n",
    "\n",
    "class DiffusionDVAE(pl.LightningModule):\n",
    "    def __init__(self, global_args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pqmf_bands = global_args.pqmf_bands\n",
    "\n",
    "        if self.pqmf_bands > 1:\n",
    "            self.pqmf = PQMF(2, 70, global_args.pqmf_bands)\n",
    "\n",
    "        capacity = 32\n",
    "\n",
    "        c_mults = [2, 4, 8, 16, 32]\n",
    "        \n",
    "        strides = [4, 4, 2, 2, 2]\n",
    "\n",
    "        self.encoder = SoundStreamXLEncoder(\n",
    "            in_channels=2*global_args.pqmf_bands, \n",
    "            capacity=capacity, \n",
    "            latent_dim=global_args.latent_dim,\n",
    "            c_mults = c_mults,\n",
    "            strides = strides\n",
    "        )\n",
    "        self.encoder_ema = deepcopy(self.encoder)\n",
    "\n",
    "        self.diffusion = DiffusionAttnUnet1D(\n",
    "            io_channels=2, \n",
    "            cond_dim = global_args.latent_dim, \n",
    "            pqmf_bands = global_args.pqmf_bands, \n",
    "            n_attn_layers=4, \n",
    "            c_mults=[256, 256]+[512]*12\n",
    "        )\n",
    "\n",
    "        self.diffusion_ema = deepcopy(self.diffusion)\n",
    "        self.rng = torch.quasirandom.SobolEngine(1, scramble=True)\n",
    "        self.ema_decay = global_args.ema_decay\n",
    "        \n",
    "        self.num_quantizers = global_args.num_quantizers\n",
    "        if self.num_quantizers > 0:\n",
    "            quantizer_class = ResidualMemcodes if global_args.num_quantizers > 1 else Memcodes\n",
    "            \n",
    "            quantizer_kwargs = {}\n",
    "            if global_args.num_quantizers > 1:\n",
    "                quantizer_kwargs[\"num_quantizers\"] = global_args.num_quantizers\n",
    "\n",
    "            self.quantizer = quantizer_class(\n",
    "                dim=global_args.latent_dim,\n",
    "                heads=global_args.num_heads,\n",
    "                num_codes=global_args.codebook_size,\n",
    "                temperature=1.,\n",
    "                **quantizer_kwargs\n",
    "            )\n",
    "\n",
    "            self.quantizer_ema = deepcopy(self.quantizer)\n",
    "            \n",
    "        self.demo_reals_shape = None #overwrite thie later\n",
    "\n",
    "    def encode(self, *args, **kwargs):\n",
    "        if self.training:\n",
    "            return self.encoder(*args, **kwargs)\n",
    "        return self.encoder_ema(*args, **kwargs)\n",
    "\n",
    "    def decode(self, *args, **kwargs):\n",
    "        if self.training:\n",
    "            return self.diffusion(*args, **kwargs)\n",
    "        return self.diffusion_ema(*args, **kwargs)\n",
    "    \n",
    "    def encode_it(self, demo_reals):\n",
    "        encoder_input = demo_reals\n",
    "\n",
    "        if self.pqmf_bands > 1:\n",
    "            encoder_input = self.pqmf(demo_reals)\n",
    "\n",
    "        encoder_input = encoder_input.to(self.device)\n",
    "        self.demo_reals_shape = demo_reals.shape\n",
    "        \n",
    "        # noise is only used for decoding tbh!\n",
    "        #noise = torch.randn([demo_reals.shape[0], 2, self.demo_samples]).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.encoder_ema(encoder_input)\n",
    "            if self.quantized:\n",
    "                embeddings = rearrange(embeddings, 'b d n -> b n d') # Rearrange for Memcodes\n",
    "                embeddings, _= self.quantizer_ema(embeddings)\n",
    "                embeddings = rearrange(embeddings, 'b n d -> b d n')\n",
    "\n",
    "            embeddings = torch.tanh(embeddings)\n",
    "            return embeddings#, noise\n",
    "        \n",
    "    def decode_it(self, embeddings, demo_batch_size=None, demo_steps=35):\n",
    "        if None==demo_batch_size: demo_batch_size = self.demo_reals_shape[0]\n",
    "        noise = torch.randn([self.demo_reals_shape[0], 2, self.demo_samples]).to(self.device)\n",
    "        fake_batches = sample(self.diffusion_ema, noise, demo_steps, 0, embeddings)\n",
    "        audio_out = rearrange(fake_batches, 'b d n -> d (b n)') # Put the demos together\n",
    "        return audio_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc05fe18",
   "metadata": {},
   "source": [
    "Download the checkpoint file for the dvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff6f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "on_colab = os.path.exists('/content')\n",
    "if on_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/') \n",
    "    ckpt_file = '/content/drive/MyDrive/AI/checkpoints/epoch=53-step=200000.ckpt'\n",
    "else:\n",
    "    ckpt_file = 'checkpoint.ckpt'\n",
    "    if not os.path.exists(ckpt_file):\n",
    "        url = 'https://drive.google.com/file/d/1C3NMdQlmOcArGt1KL7pH32KtXVCOfXKr/view?usp=sharing'\n",
    "        # downloading large files from GDrive requires special treatment to bypass the dialog button it wants to throw up\n",
    "        id = url.split('/')[-2]\n",
    "        cmd = f'wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \\'https://docs.google.com/uc?export=download&id={id}\\' -O- | sed -rn \\'s/.*confirm=([0-9A-Za-z_]+).*/\\1\\\\n/p\\')&id={id}\" -O {ckpt_file} && rm -rf /tmp/cookies.txt'\n",
    "        print(\"cmd = \\n\",cmd)\n",
    "        subprocess.run(cmd, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed450521",
   "metadata": {},
   "outputs": [],
   "source": [
    "given_model = DiffusionDVAE.load_from_checkpoint(ckpt_file, global_args=global_args)\n",
    "given_model.eval() # disable randomness, dropout, etc...\n",
    "\n",
    "# attach some arg values to the model \n",
    "given_model.demo_samples = global_args.sample_size \n",
    "given_model.quantized = global_args.num_quantizers > 0\n",
    "given_model.to(device)\n",
    "freeze(given_model)  # freeze the weights for inference\n",
    "print(\"Given Autoencoder is ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74d7d92",
   "metadata": {},
   "source": [
    "## The AA-mixer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeb7b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class EmbedBlock(nn.Module):\n",
    "    def __init__(self, in_dims:int, out_dims:int, act=nn.GELU(), resid=True, use_bn=False, requires_grad=True, **kwargs) -> None:\n",
    "        \"generic little block for embedding stuff.  note residual-or-not doesn't seem to make a huge difference for a-a\"\n",
    "        super().__init__()\n",
    "        self.in_dims, self.out_dims, self.act, self.resid = in_dims, out_dims, act, resid\n",
    "        self.lin = nn.Linear(in_dims, out_dims, **kwargs)\n",
    "        self.bn = nn.BatchNorm1d(out_dims) if use_bn else None # even though points in 2d, only one non-batch dim in data\n",
    "\n",
    "        if requires_grad == False:\n",
    "            self.lin.weight.requires_grad = False\n",
    "            self.lin.bias.requires_grad = False\n",
    "\n",
    "    def forward(self, xin: Tensor) -> Tensor:\n",
    "        x = self.lin(xin)\n",
    "        if self.act is not None: x = self.act(x)\n",
    "        if self.bn is not None: x = self.bn(x)   # re. \"BN before or after Activation? cf. https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md\"\n",
    "        return xin + x if (self.resid and self.in_dims==self.out_dims) else x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee18c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AudioAlgebra(nn.Module):\n",
    "    \"\"\"\n",
    "    Main AudioAlgebra model. Contrast to aa-mixer code, keep this one simple & move mixing stuff outside\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 dims=32, \n",
    "                 hidden_dims=64, \n",
    "                 act=nn.GELU(), \n",
    "                 use_bn=False, \n",
    "                 resid=True, \n",
    "                 block=EmbedBlock, \n",
    "                 trivial=False,   # ignore everything and make this an identity mapping\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.resid, self.trivial = resid, trivial\n",
    "        self.encoder = nn.Sequential(  \n",
    "            block( dims,        hidden_dims, act=act,  use_bn=use_bn, resid=resid),\n",
    "            block( hidden_dims, hidden_dims, act=act,  use_bn=use_bn, resid=resid),\n",
    "            block( hidden_dims, hidden_dims, act=act,  use_bn=use_bn, resid=resid),\n",
    "            block( hidden_dims, dims,        act=None, use_bn=use_bn, resid=resid),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(  # same as encoder, in fact. \n",
    "            block( dims,        hidden_dims, act=act,  use_bn=use_bn, resid=resid),\n",
    "            block( hidden_dims, hidden_dims, act=act,  use_bn=use_bn, resid=resid),\n",
    "            block( hidden_dims, hidden_dims, act=act,  use_bn=use_bn, resid=resid),   \n",
    "            block( hidden_dims, dims,        act=None, use_bn=use_bn, resid=resid),\n",
    "        )\n",
    "            \n",
    "    def encode(self,xin):\n",
    "        if self.trivial: return xin \n",
    "        x = self.encoder(xin.transpose(1,2)).transpose(1,2) # transpose is just so embeddings dim goes last for matrix mult\n",
    "        return x + xin if self.resid else x\n",
    "\n",
    "    def decode(self,xin):\n",
    "        if self.trivial: return xin \n",
    "        x = self.decoder(xin.transpose(1,2)).transpose(1,2)\n",
    "        return x + xin if self.resid else x\n",
    "\n",
    "    def forward(self, \n",
    "        x   # the embedding vector from the given encoder\n",
    "        ):\n",
    "        xprime = self.encode(x)\n",
    "        xprimeprime = self.decode(xprime)  # train system to invert itself (and hope it doesn't all collapse to nothing!)\n",
    "        return xprime, xprimeprime  # encoder output,  decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d1d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def get_stems_faders(batch, #  \"1 stem\" (or batch thereof) already drawn fron the dataloader (val or train)\n",
    "                     dl_iter,  # pre-made the iterator for the/a dataloader\n",
    "                     dl,       # the dataloader itself, for restarting\n",
    "                     maxstems=2,  # how many total stems will be used, i.e. draw maxstems-1 new stems from dl_iter\n",
    "                     unity_gain=False,  # this will force all faders to be +/-1 instead of random numers\n",
    "                     debug=False):\n",
    "    \"grab some more inputs and multiplies and some gain values to go with them\"\n",
    "    nstems = random.randint(2, maxstems)\n",
    "    if debug: print(\"maxstems, nstems =\",maxstems, nstems)\n",
    "    device=batch.device\n",
    "    faders = torch.sgn(2*torch.rand(nstems)-1)  # random +/- 1's\n",
    "    if not unity_gain:\n",
    "        faders += 0.5*torch.tanh(2*(2*torch.rand(nstems)-1))  # gain is now between 0.5 and 1.5\n",
    "    stems = [batch]                  # note that stems is a list\n",
    "    for i in range(nstems-1):        # in addtion to the batch of stem passed in, grab some more\n",
    "        try: \n",
    "            next_stem = next(dl_iter).to(device)    # this is just another batch of input data\n",
    "        except StopIteration:\n",
    "            dl_iter = iter(dl)       # time to restart. hoping this propagates out as a pointer\n",
    "            next_stem = next(dl_iter).to(device)\n",
    "        if debug: print(\"  next_stem.shape = \",next_stem.shape)\n",
    "        stems.append(next_stem)\n",
    "    return stems, faders.to(device), dl_iter  # also return the iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec3301",
   "metadata": {},
   "source": [
    "Test that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(train_iter) \n",
    "stems, faders, val_iter = get_stems_faders(batch, train_iter, train_dl, maxstems=2)\n",
    "print(\"len(faders) = \",len(faders))\n",
    "\n",
    "# artificially max out these stems! \n",
    "for i in range(len(faders)):\n",
    "    faders[i] = 1/torch.abs(stems[i][0]).max() \n",
    "\n",
    "playable_spectrogram( stems[0][0]*faders[0], output_type='live')  #  this is just the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8498abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "playable_spectrogram( stems[1][0]*faders[1], output_type='live')  # thisis something new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c4743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def do_mixing(stems, faders, given_model, aa_model, device, debug=False, **kwargs):\n",
    "    \"\"\"\n",
    "    here we actually mix inputs and encode them and embed them.\n",
    "    \"\"\"\n",
    "    zs, ys, zsum, ysum, yrecon_sum, fadedstems, yrecons = [], [], None, None, None, [], []\n",
    "    mix = torch.zeros_like(stems[0]).to(device)\n",
    "    #if debug: print(\"do_mixing: stems, faders =\",stems, faders)\n",
    "    for s, f in zip(stems, faders):   # iterate through list of stems, encode a bunch of stems at different fader settings\n",
    "        fadedstem = (s * f).to(device)                 # audio stem adjusted by gain fader f\n",
    "        with torch.no_grad():\n",
    "            y = given_model.encode(fadedstem)  # encode the stem\n",
    "        z, y_recon = aa_model(y)             # <-- this is the main work of the model\n",
    "        zsum = z if zsum is None else zsum + z # <---- compute the sum of all the z's so far. we'll end up using this in our (metric) loss as \"pred\"\n",
    "\n",
    "        mix += fadedstem                 # make full mix in input space\n",
    "        with torch.no_grad():\n",
    "            ymix = given_model.encode(mix)  # encode the mix in the given model\n",
    "        zmix, ymix_recon = aa_model(ymix)   #  <----- map that according to our learned re-embedding. this will be the \"target\" in the metric loss\n",
    "\n",
    "        #[y, ymix, y_recon, ymix_recon ] = [rearrange(x, 'b t e -> b e t') for x in [y, ymix, y_recon, ymix_recon ]] # put the y's back in their original order\n",
    "\n",
    "        # Sums of y are likely meaningless but one might wonder how well the given encoder does at linearity, so...\n",
    "        ysum = y if ysum is None else ysum + y   # = sum of embeddings in original model space; we don't really care about ysum except for diagnostics\n",
    "        #yrecon_sum = y_recon if yrecon_sum is None else yrecon_sum + y_recon   # = sum of embeddings in original model space; we don't really care about ysum except for diagnostics\n",
    "\n",
    "        yrecons.append(y_recon)   # for recon loss, save individual stem inverses\n",
    "        zs.append(z)              # save a list of individual z's\n",
    "        ys.append(y)            # save a list of individual y's\n",
    "        fadedstems.append(fadedstem) # safe a list of each thing that went into the mix\n",
    "        \n",
    "    archive = {'zs':zs, 'mix':mix,'ys': ys, 'ymix':ymix, 'ymix_recon':ymix_recon, 'fadedstems':fadedstems, 'yrecons':yrecons, 'ysum':ysum} \n",
    "\n",
    "    return zsum, zmix, archive  # we will try to get these two to be close to each other via loss. archive is for diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ecabc2",
   "metadata": {},
   "source": [
    "## Mix and apply models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc6253",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_use_bn = False  # batch norm? \n",
    "aa_use_resid = True # use residual connections? (doesn't make much difference tbh)\n",
    "emb_dims = global_args.latent_dim # input size to aa model\n",
    "hidden_dims = 64   # number of hidden dimensions in aa model. usually was 64\n",
    "trivial = False  # aa_model is a no-op when this is true\n",
    "debug = True \n",
    "print(\"emb_dims = \",emb_dims)\n",
    "\n",
    "# untrained aa model\n",
    "torch.manual_seed(seed+2)\n",
    "#stems, faders, val_iter = get_stems_faders(batch, val_iter, val_dl)\n",
    "\n",
    "aa_model = AudioAlgebra(dims=emb_dims, hidden_dims=hidden_dims, use_bn=aa_use_bn, resid=aa_use_resid, trivial=trivial).to(device) \n",
    "with torch.no_grad():\n",
    "    zsum, zmix, archive = do_mixing(stems, faders, given_model, aa_model, device, debug=debug)\n",
    "    \n",
    "print(\"mix:\")\n",
    "playable_spectrogram( archive['mix'][0], output_type='live')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f9624",
   "metadata": {},
   "source": [
    "### First, the effects of the given encoder $f$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ac026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emb_spectrograms(qs, labels, skip_ys=True):\n",
    "    fig, ax = plt.subplots( 3 , 1, figsize=(10, 9))\n",
    "    for i, (q, name) in enumerate(zip(qs, labels)):\n",
    "        if i>2 and skip_ys: break\n",
    "        row, col = i % 3, i//3\n",
    "        im = tokens_spectrogram_image(q, mark_batches=True)\n",
    "        newsize = (np.array(im.size) *800/im.size[0]).astype(int)\n",
    "        im.resize(newsize)\n",
    "        ax[row].imshow(im)\n",
    "        ax[row].axis('off')\n",
    "        ax[row].set_title(labels[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "ys, ymix, ysum = archive['ys'], archive['ymix'], archive['ysum']\n",
    "diff = ysum - ymix\n",
    "qs      = [ ymix,   ysum,  diff, ys[0], ys[1]]\n",
    "labels =  ['ymix', 'ysum','diff := ysum - ymix', 'y0', 'y1', ]\n",
    "print(\"ymix.shape = \",ymix.shape)\n",
    "plot_emb_spectrograms(qs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0771e8",
   "metadata": {},
   "source": [
    "....So at least using the data I can see right now, ymix and ysum can differ by what looks to be 50% in places. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b60e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (q, name) in enumerate(zip(qs, labels)):\n",
    "    if i>2: break\n",
    "    print(f\"{name}:\")\n",
    "    show_pca_point_cloud(q, mode='lines+markers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b21d6",
   "metadata": {},
   "source": [
    "### Now the z's (note the model is untrained at this point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b64f2",
   "metadata": {},
   "source": [
    "### Reconstruction /demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b84a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "def aa_demo(given_model, aa_model, log_dict, zsum, zmix, step, demo_steps=35, sr=48000):\n",
    "    \"log decoded audio for zsum and zmix\"\n",
    "    with torch.no_grad():\n",
    "        for var,name in zip([zsum, zmix],['zsum','zmix']):\n",
    "            var = aa_model.decode(var)\n",
    "            fake_audio = given_model.decode_it(var, demo_steps=demo_steps)\n",
    "            filename = f'{name}_{step:08}.wav'\n",
    "            fake_audio = fake_audio.clamp(-1, 1).mul(32767).to(torch.int16).cpu()\n",
    "            torchaudio.save(filename, fake_audio, self.sample_rate)\n",
    "            log_dict[name] = wandb.Audio(filename, sample_rate=sr, caption=name)   \n",
    "            #log_dict[f'{name}_spec'] = wandb.Image( tokens_spectrogram_image(var.detach()) )\n",
    "    return log_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bc9f8c",
   "metadata": {},
   "source": [
    "### Define Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b6747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "mseloss = nn.MSELoss()\n",
    "\n",
    "#def rel_loss(y_pred: torch.Tensor, y: torch.Tensor, eps=1e-3) -> float:\n",
    "#    \"relative error loss   --- note we're never going to actually use this. it was just part of development\"\n",
    "#    e = torch.abs(y.view_as(y_pred) - y_pred) / ( torch.abs(y.view_as(y_pred)) + eps ) \n",
    "#    return torch.median(e)\n",
    "\n",
    "def vicreg_var_loss(z, gamma=1, eps=1e-4):\n",
    "    std_z = torch.sqrt(z.var(dim=0) + eps)\n",
    "    return torch.mean(F.relu(gamma - std_z))   # the relu gets us the max(0, ...)\n",
    "\n",
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "def vicreg_cov_loss(z):\n",
    "    \"the regularization term that is the sum of the off-diagaonal terms of the covariance matrix\"\n",
    "    num_features = z.shape[1]*z.shape[2]  # TODO: move this out for speed.\n",
    "    cov_z = torch.cov(rearrange(z, 'b c t -> ( c t ) b'))   \n",
    "    return off_diagonal(cov_z).pow_(2).sum().div(num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a1028",
   "metadata": {},
   "source": [
    "# Main run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f26fc4",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7605dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_aa_model(debug=False):   \n",
    "    \"train our aa projector, uses global variables, not sorry\"\n",
    "    global train_dl, given_model \n",
    "    \n",
    "    max_epochs = 40\n",
    "    lossinfo_every, viz_demo_every =   20, 10000000   # in units of steps\n",
    "    checkpoint_every = 10000\n",
    "    max_lr= 0.001\n",
    "    total_steps = len(train_set)//args.batch_size * max_epochs\n",
    "    print(\"total_steps =\",total_steps)  # for when I'm checking wandb\n",
    "\n",
    "    hprint(f\"Setting up AA model using device: {device}\")\n",
    "    #aa_model = AudioAlgebra(global_args, device, autoencoder, trivial=True)\n",
    "\n",
    "\n",
    "    \n",
    "    torch.manual_seed(seed) # chose this value because it shows of nice nonlinearity\n",
    "    aa_model  = AudioAlgebra(dims=emb_dims, hidden_dims=hidden_dims, use_bn=aa_use_bn, resid=aa_use_resid).to(device) \n",
    "    opt       = optim.Adam([*aa_model.parameters()], lr=5e-4)  # Adam optimizer\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=max_lr, total_steps=total_steps)\n",
    "    \n",
    "    aa_model, opt, train_dl, given_model, scheduler = accelerator.prepare(aa_model, opt, train_dl, given_model, scheduler)\n",
    "    \n",
    "    wandb.init(project='aa-mixer-vicreg')\n",
    "   \n",
    "    # training loop\n",
    "    train_iter = iter(train_dl) # this is only for use with get_stems_faders\n",
    "    epoch, step = 0, 0\n",
    "    torch.manual_seed(seed) # for reproducibility\n",
    "    while (epoch < max_epochs) or (max_epochs < 0):  # training loop\n",
    "        with tqdm(train_dl, unit=\"batch\", disable=not accelerator.is_main_process) as tepoch:\n",
    "            for batch in tepoch:   # train\n",
    "                opt.zero_grad()\n",
    "                log_dict = {}\n",
    "                batch = batch.to(device)\n",
    "\n",
    "                stems, faders, train_iter = get_stems_faders(batch, train_iter, train_dl)\n",
    "\n",
    "                # vicreg: 1. invariance\n",
    "                zsum, zmix, archive = do_mixing(stems, faders, accelerator.unwrap_model(given_model), accelerator.unwrap_model(aa_model), device, debug=debug)\n",
    "                mix_loss = mseloss(zsum, zmix)  \n",
    "\n",
    "                var_loss = (vicreg_var_loss(zsum) + vicreg_var_loss(zmix))/2    # vicreg: 2. variance\n",
    "                cov_loss = (vicreg_cov_loss(zsum) + vicreg_cov_loss(zmix))/2    # vicreg: 3. covariance\n",
    "\n",
    "\n",
    "                # reconstruction loss: inversion of aa map h^{-1}(z): z -> y,  i.e. train the aa decoder\n",
    "                y = accelerator.unwrap_model(given_model).encode(batch)\n",
    "                z, yrecon = accelerator.unwrap_model(aa_model).forward(y)       # (re)compute ys for one input batch (not stems&faders)\n",
    "                aa_recon_loss = mseloss(y, yrecon)     \n",
    "                aa_recon_loss = aa_recon_loss + mseloss(archive['ymix'], archive['ymix_recon'])  # also recon of the  mix ecoding\n",
    "                #aa_recon_loss  = aa_recon_loss +  mseloss(archive['ysum'], archive['yrecon_sum']) # Never use this:  ysum shouldn't matter / is poorly defined\n",
    "           \n",
    "                loss = mix_loss + var_loss + cov_loss + aa_recon_loss     # --- full loss function\n",
    "                \n",
    "                log_dict['train_loss'] = loss.detach()                    # --- this is the full loss \n",
    "                log_dict['mix_loss'] = mix_loss.detach() \n",
    "                log_dict['aa_recon_loss'] = aa_recon_loss.detach()\n",
    "                log_dict['var_loss'] = var_loss.detach() \n",
    "                log_dict['cov_loss'] = cov_loss.detach() \n",
    "                log_dict['learning_rate'] = opt.param_groups[0]['lr']\n",
    "                log_dict['epoch'] = epoch\n",
    "\n",
    "                if step % lossinfo_every == 0: \n",
    "                    tepoch.set_description(f\"Epoch {epoch+1}/{max_epochs}\")\n",
    "                    tepoch.set_postfix(loss=loss.item())         #  TODO: use EMA for loss display? \n",
    "\n",
    "                accelerator.backward(loss)  #loss.backward()\n",
    "                opt.step()  \n",
    "                \n",
    "                if accelerator.is_main_process:\n",
    "                    if  False and step % viz_demo_every == 0:\n",
    "                         log_dict = aa_demo(accelerator.unwrap_model(given_model), accelerator.unwrap_model(aa_model), log_dict, zsum, zmix, step)\n",
    "\n",
    "                    if False and step % checkpoint_every == 0:\n",
    "                        save_aa_checkpoint(aa_model, suffix=RUN_SUFFIX+f\"_{step}\")\n",
    "\n",
    "                    wandb.log(log_dict)\n",
    "\n",
    "                scheduler.step()   \n",
    "                step += 1\n",
    "\n",
    "        epoch += 1\n",
    "    #----- training loop finished\n",
    "    \n",
    "    save_aa_checkpoint(accelerator.unwrap_model(aa_model), suffix=RUN_SUFFIX+f\"_{step}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d1646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aa_model(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a7ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_wandb: wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61fdcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
