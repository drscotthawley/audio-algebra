{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe09744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b703b37d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| default_exp given_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b778b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# given_models\n",
    "> i.e. routines for setting up and using pretrained AutoEncoder models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9414fc4",
   "metadata": {},
   "source": [
    "Currently included:\n",
    "- Baselines: Spectrogram (i.e. complex STFT), MagSpectrogram, MagDPhaseSpectrogram, MelSpectrogram\n",
    "- Diffusion models: DVAE (unmasked)\n",
    "- VAEs: RAVE\n",
    "\n",
    "To be added: \n",
    "- Diffusion models: masked DVAE , archinet's\n",
    "- Others groups': JukeBox (as \"IceBox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a097942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb91a8ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations  # for type hints LAION code samples\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torchaudio import transforms as T\n",
    "import pytorch_lightning as pl\n",
    "import math\n",
    "import subprocess\n",
    "from copy import deepcopy\n",
    "import pytorch_lightning as pl\n",
    "from einops import rearrange\n",
    "\n",
    "try:\n",
    "    import rave\n",
    "    import gin\n",
    "    got_rave = True\n",
    "except:\n",
    "    print(\"  given_models: Warning: RAVE import failed\")\n",
    "    got_rave = False\n",
    "\n",
    "import laion_clap \n",
    "from laion_clap.training.data import get_audio_features\n",
    "\n",
    "from aeiou.core import batch_it_crazy\n",
    "from aeiou.hpc import freeze\n",
    "from autoencoders.models import AudioAutoencoder\n",
    "\n",
    "try:\n",
    "    from audio_algebra.DiffusionDVAE import DiffusionDVAE\n",
    "    from audio_algebra.DiffusionDVAE import sample as dvae_sample\n",
    "    got_dvae = True\n",
    "except:\n",
    "    print(\"  given_models: Warning: DVAE import failed\")\n",
    "    got_dvae = False\n",
    "\n",
    "# StackedDiffAE\n",
    "if False:  # incompatibility with StackedAE*Cond? \n",
    "    from audio_algebra.StackedDiffAE import LatentAudioDiffusionAutoencoder \n",
    "    from audio_algebra.StackedDiffAE import sample as stacked_sample\n",
    "    from autoencoders.models import AudioAutoencoder # audio-diffusion\n",
    "    \n",
    "    \n",
    "from audio_algebra.StackedAELatentDiffusionCond import LatentAudioDiffusionAutoencoder, StackedAELatentDiffusionCond\n",
    "from audio_algebra.StackedAELatentDiffusionCond import sample as ldc_sample\n",
    "from audio_algebra.StackedAELatentDiffusionCond import resample as ldc_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8537333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "# imports not needed for library itself, but used in notebook documentation/tests/demo\n",
    "import matplotlib.pyplot as plt\n",
    "from aeiou.viz import playable_spectrogram, audio_spectrogram_image, tokens_spectrogram_image, show_point_cloud\n",
    "from aeiou.core import load_audio, get_device\n",
    "from aeiou.datasets import Stereo\n",
    "from IPython.display import display, HTML, Audio  # just for displaying inside notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d869fd59",
   "metadata": {},
   "source": [
    "# Wrapper API: \"GivenModelClass\"\n",
    "We're going to make a bunch of wrappers that are PyTorch modules. The API for the wrappers will be as follows: \n",
    "- `.encode()`: encodes the (batch of) (raw) audio waveform(s) into encodings aka \"representations\" `reps`, where `reps` should have shape `([b,]c,d,n)` where `b` is an optional batch dimension (matching that of the waveform input), `c` may or may not correspond to actual audio channels (e.g. for DVAE, c=1 even for stereo). `d` and `n` are typically the \"dimensions\" of the embeddings and the time step/frame, respectively, but some models may not respect this.\n",
    "- `.decode()`: decodes the (batch of) encodings/representations from the encoder into \"reconstruction\" waveforms `recons`\n",
    "- `.forward()`: calls *both* `encode()` and `decode()` in succesion, returns tuple `(reps, recons)`\n",
    "- `.setup()`: an optional routine that will load checkpoints & do other 'init' stuff (but not done automatically in `init`)\n",
    "- `self.ckpt_info{}`: dict that includes URL and approved hash value for pretrained model checkpoint. Default is no info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8df9b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "class GivenModelClass(nn.Module):\n",
    "    \"This provides an (optional) 'shorthand' structure for (some) given_models\"\n",
    "    def __init__(self,\n",
    "        zero_pad=True,\n",
    "        make_sizes_match=True,\n",
    "        ckpt_info={'ckpt_path':'', 'ckpt_url':'','ckpt_hash':'', 'gdrive_path':''}, # info on pretrained checkpoints\n",
    "        **kwargs,  # these are so that some models can ignore kwargs needed by others\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.make_sizes_match, self.orig_shape, self.zero_pad, self.ckpt_info  = make_sizes_match, None, zero_pad, ckpt_info\n",
    "        self.name = self.__class__.__name__  # just a shorthand\n",
    "        self.ckpt_dir = os.path.expanduser('~/checkpoints')\n",
    "        if not os.path.exists(self.ckpt_dir): os.makedirs(self.ckpt_dir)\n",
    "    def setup(self, gdrive=True):\n",
    "        \"Setup can include things such as downloading checkpoints\"\n",
    "        pass  \n",
    "    def encode(self, waveform: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        return None\n",
    "    def decode(self, waveform: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        return None    \n",
    "    def forward(self, waveform: torch.Tensor)-> (torch.Tensor, torch.Tensor):\n",
    "        \"Calls .encode() and .decode() in succession, returns both results as tuple\"\n",
    "        reps = self.encode(waveform)\n",
    "        recons = self.decode(reps)\n",
    "        return (reps, recons)\n",
    "    \n",
    "    def get_checkpoint(self, gdrive=True):\n",
    "        \"This just ensures that the checkpoint file (if one is available) will be present on the local disk at self.ckpt_info['ckpt_path']\"\n",
    "        if self.ckpt_info=={} or all(x=='' for x in self.ckpt_info.values()): \n",
    "            print(\"No checkpoint info available.\")\n",
    "            return\n",
    "        #@title Mount or Download Checkpoint\n",
    "        on_colab = os.path.exists('/content')\n",
    "        if on_colab and gdrive:  # on colab, try to mount checkpoint from drive unless user prefers download\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive/') \n",
    "            ckpt_file = '/content/drive/'+self.ckpt_info['gdrive_path']\n",
    "            while not os.path.exists(ckpt_file):\n",
    "                print(f\"\\nPROBLEM: Expected to find the checkpoint file at {ckpt_file} but it's not there.\")\n",
    "                print(f\"Where is it? (Go to the File system in the left sidebar and find it)\")\n",
    "                ckpt_file = input('Enter location of checkpoint file: ')\n",
    "            self.ckpt_info['ckpt_path']= ckpt_file\n",
    "        else:\n",
    "            ckpt_file = os.path.expanduser(self.ckpt_info['ckpt_path']) #'checkpoint.ckpt'\n",
    "            if not os.path.exists(ckpt_file):\n",
    "                url = self.ckpt_info['ckpt_url']\n",
    "                if self.debug: print(f\"Can't find checkpoint file {ckpt_file}. Will try to download it from {url}\")\n",
    "                if 'drive.google.com' in url:\n",
    "                    # downloading large files from GDrive requires special treatment to bypass the dialog button it wants to throw up\n",
    "                    id = url.split('/')[-2]\n",
    "                    cmd = f\"gdown -O {ckpt_file} {id}\"\n",
    "                else:\n",
    "                    print(f\"Downloading to {ckpt_file}\")\n",
    "                    cmd = f\"curl -L {url} -o {ckpt_file}\"\n",
    "                    if self.debug: print(\"cmd = \",cmd)\n",
    "                subprocess.run(cmd, shell=True, check=True) \n",
    "                if self.ckpt_info['ckpt_hash'] != '': # check the hash if it was given\n",
    "                    print(f\"\\nSecurity: checking hash on downloaded checkpoint file...\")\n",
    "                    new_hash = subprocess.run(['shasum', '-a','256',ckpt_file], stdout=subprocess.PIPE).stdout.decode('utf-8').split(' ')[0]\n",
    "                    #new_hash = subprocess.run(['md5sum',ckpt_file], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "                    assert new_hash == self.ckpt_info['ckpt_hash'], \"Hashes don't match. STOP THE NOTEBOOK. DO NOT EXECUTE.\"\n",
    "                    print(\"Checkpoint hash checks out.\")\n",
    "            else:\n",
    "                print(\"Checkpoint found!\")\n",
    "    \n",
    "    def match_sizes(self, recon) -> torch.Tensor:\n",
    "        \"match recon size to original waveform size, if possible. Need to have set self.orig_shape earlier\"\n",
    "        if self.make_sizes_match and (self.orig_shape is not None) and (recon.shape != self.orig_shape):\n",
    "            if recon.shape[-1] > self.orig_shape[-1]:  # recon is longer\n",
    "                recon = recon[...,:self.orig_shape[-1]]\n",
    "            else: # recon is shorter\n",
    "                recon2 = torch.zeros(self.orig_shape)  # slow but what are you gonna do\n",
    "                recon2[...,:self.orig_shape[-1]] = recon\n",
    "                recon = recon2 \n",
    "            assert recon.shape == self.orig_shape, f\"Did not succeed in making size match. recon.shape ({recon.shape}) != self.orig_shape ({self.orig_shape})\"\n",
    "        return recon       \n",
    "    \n",
    "    #--- couple extra routines probably only useful for fourier-based AE's but no harm done including them here\n",
    "    def next_power_of_2(self, x:int) -> int:  \n",
    "        return 1 if x == 0 else 2**(x - 1).bit_length()\n",
    "\n",
    "    def zero_pad_po2(self, x:int) -> int:\n",
    "        \"useful for padding to nearest power of 2, useful for fourier-based transforms\"\n",
    "        new_shape = list(x.shape)\n",
    "        new_shape[-1] = self.next_power_of_2(new_shape[-1])\n",
    "        new_x = torch.zeros(new_shape).to(x.device)\n",
    "        new_x[...,:x.shape[-1]] = x\n",
    "        return new_x\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd4c371",
   "metadata": {},
   "source": [
    "# Baselines: STFT \"AutoEncoders\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba890cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "class SpectrogramAE(GivenModelClass):\n",
    "    \"Raw (complex) spectrogram. See torchaudio.Spectrogram & InverseSpectrogram for kwarg info\"\n",
    "    def __init__(self,\n",
    "        n_fft=1024,   \n",
    "        hop_length=256,\n",
    "        center=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = T.Spectrogram(power=None, n_fft=n_fft, hop_length=hop_length, center=center, **kwargs)\n",
    "        self.decoder = T.InverseSpectrogram(    n_fft=n_fft, hop_length=hop_length, center=center, **kwargs)\n",
    "        \n",
    "    def encode(self, waveform: torch.Tensor,**kwargs) -> torch.Tensor:\n",
    "        \"Note that this produces complex numbers by default\"\n",
    "        self.orig_shape = waveform.shape # can use this for matching output size later\n",
    "        return self.encoder(self.zero_pad_po2(waveform)) if self.zero_pad else self.encoder(waveform)\n",
    "\n",
    "    def decode(self, reps: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        \"this decoder offers perfect reconstruction\"\n",
    "        return self.match_sizes( self.decoder(reps) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564f7559",
   "metadata": {},
   "source": [
    "Let's test that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba4285f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "device = get_device()\n",
    "print(\"device =\",device)\n",
    "if torch.device('mps') == device: \n",
    "    print(\"Actually for spectrogram, MPS is going to cause 'TypeError: Trying to convert ComplexFloat to the MPS backend but it does not have support for that dtype', so you'll get cpu\")\n",
    "    device='cpu'\n",
    "\n",
    "data_path = '../aeiou/examples/'\n",
    "waveform = load_audio(data_path+'example.wav')\n",
    "#waveform = load_audio('/admin/home-shawley/jsontest.wav')\n",
    "stereo_op = Stereo()\n",
    "waveform = stereo_op(waveform)\n",
    "\n",
    "def nb_play(waveform):\n",
    "    display(Audio(waveform, rate=48000))\n",
    "    print(\"waveform.shape =\",waveform.shape)\n",
    "    #playable_spectrogram(waveform, output_type='live')\n",
    "    spec_graph = audio_spectrogram_image(waveform.cpu(), justimage=False, db=False, db_range=[-60,20])\n",
    "    display(spec_graph)\n",
    "    for c in range(waveform.shape[0]):\n",
    "        plt.plot(waveform[c].cpu().numpy(), label=f'channel {c}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "nb_play(waveform) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d058f13",
   "metadata": {},
   "source": [
    "Now show that \"`recon`\" results of the inverse transform are very close to the original waveform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2fceef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "given_model = SpectrogramAE().to(device)\n",
    "spec, recon = given_model.forward(waveform.to(device))\n",
    "print(\"spec.shape, recon.shape = \",spec.shape, recon.shape)\n",
    "print(\"spec.dtype = \",spec.dtype)\n",
    "diff = recon.cpu()-waveform\n",
    "\n",
    "#playable_spectrogram(diff, output_type='live') #plot/play the difference\n",
    "#spec_graph = audio_spectrogram_image(diff, justimage=False, db=False, db_range=[-60,20])\n",
    "#display(spec_graph)\n",
    "for c in range(waveform.shape[0]):\n",
    "    plt.plot(diff[c].cpu().numpy(), label=f'channel {c}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe46169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "class MagSpectrogramAE(GivenModelClass):\n",
    "    \"Magnitude spectrogram encoder, GriffinLim decoder\"\n",
    "    def __init__(self,\n",
    "        n_fft=1024,   \n",
    "        hop_length=256,\n",
    "        center=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = T.Spectrogram(power=2, n_fft=n_fft, hop_length=hop_length, center=center, **kwargs)\n",
    "        self.decoder = T.GriffinLim(          n_fft=n_fft, hop_length=hop_length, **kwargs)\n",
    "        \n",
    "    def encode(self, waveform: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        self.orig_shape = waveform.shape\n",
    "        return self.encoder(self.zero_pad_po2(waveform)) if self.zero_pad else self.encoder(waveform)\n",
    "\n",
    "    def decode(self, reps: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        \"Note that GriffinLim decoding *guesses* at the phase\"\n",
    "        return self.match_sizes( self.decoder(reps) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8415a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "# test that\n",
    "magspecfunc = MagSpectrogramAE().to(device)\n",
    "magspec, recon2 = magspecfunc.forward(waveform.to(device))\n",
    "print(\"magspec.shape, recon2.shape = \",magspec.shape, recon2.shape )\n",
    "print(\"magspec.dtype = \",magspec.dtype)\n",
    "recon2 = recon2.to('cpu')\n",
    "diff = recon2-waveform\n",
    "\n",
    "#playable_spectrogram(recon2, output_type='live') #plot/play the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b53028",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "class MagDPhaseSpectrogramAE(GivenModelClass):\n",
    "    \"Magnitude + PhaseChange spectrogram encoder, Exact decoder\"\n",
    "    def __init__(self,\n",
    "        n_fft=1024,   \n",
    "        hop_length=256,\n",
    "        center=True,   # used for fft argument\n",
    "        init='true',   # initial angle in decoder:'true'|'rand'|'zero'\n",
    "        use_cos=False, # use vector cosine rule to get angle\n",
    "        debug=False,\n",
    "        cheat=False,   # store original signal for comparison later\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = T.Spectrogram(power=None, n_fft=n_fft, hop_length=hop_length, center=center, **kwargs)\n",
    "        self.decoder = T.InverseSpectrogram(    n_fft=n_fft, hop_length=hop_length, center=center, **kwargs)\n",
    "        #self.gl = T.GriffinLim(          n_fft=n_fft, hop_length=hop_length, **kwargs)\n",
    "        self.use_cos, self.cheat, self.debug, self.init = use_cos, cheat, debug, init\n",
    "        self.pi = 3.141592653589\n",
    "        \n",
    "    def encode(self, waveform: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        self.orig_shape = waveform.shape\n",
    "        spec =  self.encoder(self.zero_pad_po2(waveform)) if self.zero_pad else self.encoder1(waveform)\n",
    "        mag, theta = torch.abs(spec), torch.angle(spec)\n",
    "        if self.debug: theta = torch.where(theta < 0, theta+2*self.pi, theta) # just to make it easier to compare later\n",
    "        if self.cheat: \n",
    "            self.spec_orig, self.mag_orig, self.theta = spec, mag, theta\n",
    "        if self.use_cos:  # doesn't sound as good imho\n",
    "            x, y = torch.real(spec), torch.imag(spec)\n",
    "            mag_tm1 = torch.roll(mag, 1, -1)  # previus timestep rolled forward for subtraction/multiplication\n",
    "            x_tm1, y_tm1 = torch.roll(x, 1, -1), torch.roll(y, 1, -1)\n",
    "            numerator, denominator = (x*x_tm1 + y*y_tm1), (mag*mag_tm1)\n",
    "            acos_arg = torch.where( denominator==0, 1, numerator/denominator) # aviod nans, div by 0\n",
    "            acos_arg =  torch.clip( acos_arg , -1, 1) # another bounds check\n",
    "            dtheta = torch.acos(acos_arg)  # could perhaps approximate acos by sqrt(2*(1-cos_arg)) when cos_arg is near 1\n",
    "        else:  # this is faster and sounds better too. \n",
    "            theta_tm1 = torch.roll(theta, 1, -1)\n",
    "            dtheta = theta-theta_tm1 # this can give bad vals when theta1/2 are on opposite sides of x=0 line\n",
    "            dtheta = torch.where(dtheta < 0, dtheta+2*self.pi, dtheta)  # force phase to be non-decreasing\n",
    "        dtheta[:,:,0] = theta[:,:,0]  # encode initial value of theta at first position, helps it sound better\n",
    "        return torch.concatenate((mag,dtheta))  #package mags together, dthetas together\n",
    "        \n",
    "        \n",
    "    def decode(self, reps: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        \"Note that GriffinLim decoding *guesses* at the phase\"\n",
    "        nc = reps.shape[-3] // 2 # number of audio channels\n",
    "        mag, dtheta = reps[0:nc,:,:], reps[nc:,:,:] # split the packaging\n",
    "        #return self.gl(mag**2)  # griffin lim cop-out\n",
    "        if self.cheat:\n",
    "            theta = self.theta.clone() # cheating\n",
    "        else:\n",
    "            theta = torch.zeros(dtheta.shape).to(reps.device)\n",
    "            if self.init=='true':\n",
    "                theta[:,:,0] = dtheta[:,:,0]  # initial value of theta, helps it sound better vs. random or zeros\n",
    "            elif self.init=='rand':\n",
    "                theta[:,:,0] = torch.rand(dtheta.shape[:-1])\n",
    "            for t in range(1,reps.shape[-1]):  # integrate theta along the time (last) dimension\n",
    "                theta[:,:,t] = theta[:,:,t-1] + dtheta[:,:,t]\n",
    "                theta[:,:,t] = torch.where( theta[:,:,t] < 2*self.pi, theta[:,:,t], theta[:,:,t] - 2*self.pi)            \n",
    "        spec = mag* ( torch.cos(theta) + 1j*torch.sin(theta) )\n",
    "        if self.debug:\n",
    "            self.mag_new, self.theta_new = torch.abs(spec), theta\n",
    "            self.spec_new = spec\n",
    "        return self.match_sizes( self.decoder(spec) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f052dbb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| eval: false  \n",
    "magdphasefunc = MagDPhaseSpectrogramAE(debug=True, init='true').to(device)\n",
    "magdtheta, recon3 = magdphasefunc.forward(waveform.to(device))\n",
    "recon3 = recon3.cpu()\n",
    "#playable_spectrogram(recon3, output_type='live') #plot/play the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5799e0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "diff = recon3 - waveform\n",
    "spec_graph = audio_spectrogram_image(recon3, justimage=False, db=False, db_range=[-60,20])\n",
    "display(spec_graph)\n",
    "for c in range(1): \n",
    "    print(f\"Channel {c}:\")\n",
    "    for thing, name in zip([waveform, recon3, diff], ['waveform', 'recon', 'diff']):\n",
    "        plt.plot(thing[c].cpu().numpy(), alpha=0.5, label=name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded8c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "class MelSpectrogramAE(GivenModelClass):\n",
    "    \"Mel spectrogram encoder, GriffinLim decoder\"\n",
    "    def __init__(self,\n",
    "        sample_rate=48000,\n",
    "        n_fft=1024,   \n",
    "        hop_length=256,\n",
    "        center=True,\n",
    "        **kwargs, # these are mainly just so that we can ignore kwargs that other models need\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential( T.MelSpectrogram(sample_rate=sample_rate, n_fft=n_fft, hop_length=hop_length, center=center, **kwargs))\n",
    "        self.inv_melscale_t = T.InverseMelScale(n_stft=n_fft // 2 + 1)\n",
    "        self.decoder = T.GriffinLim(n_fft=n_fft, hop_length=hop_length, **kwargs)\n",
    "        \n",
    "    def encode(self, waveform: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        self.orig_shape = waveform.shape\n",
    "        return self.encoder(self.zero_pad_po2(waveform)) if self.zero_pad else self.encoder(waveform)\n",
    "\n",
    "    def decode(self, melspec: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        spec = self.inv_melscale_t(melspec)\n",
    "        return self.match_sizes( self.decoder(spec) )\n",
    "    \n",
    "    def forward(self, waveform: torch.Tensor)-> (torch.Tensor, torch.Tensor):\n",
    "        \"Calls .encode() and .decode() in succession, returns both results as tuple\"\n",
    "        reps = self.encode(waveform)\n",
    "        recons = self.decode(reps)\n",
    "        return (reps, recons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "# test that\n",
    "i_love_slow_code = False\n",
    "if i_love_slow_code: \n",
    "    melspecfunc = MelSpectrogramAE().to(device)\n",
    "    melspec, recon4 = melspecfunc.forward(waveform.to(device))\n",
    "    print(\"melspec.device, melspec.shape, recon4.shape = \",melspec.device, melspec.shape, recon4.shape )\n",
    "    print(\"melspec.dtype =\",melspec.dtype)\n",
    "    recon4 = recon4.to('cpu')\n",
    "    #display(playable_spectrogram(recon4, output_type='live'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc2ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "if i_love_slow_code: \n",
    "    diff = recon4-waveform\n",
    "    #display(playable_spectrogram(diff, specs=\"waveform\", output_type='live')) #plot/play the difference\n",
    "    spec_graph = audio_spectrogram_image(recon4, justimage=False, db=False, db_range=[-60,20])\n",
    "    display(spec_graph)\n",
    "    c = 0\n",
    "    for thing, name in zip([waveform, recon4, diff],[\"waveform\", \"recon\", \"diff\"]):\n",
    "        plt.plot(thing[c].cpu().numpy(), alpha=0.5, label=name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe1cec",
   "metadata": {},
   "source": [
    "# Diffusion AutoEncoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa414cc",
   "metadata": {},
   "source": [
    "## 'DVAE' (old model) \n",
    "Wrapper for Zach's DVAE/diffAE model from September/October. This cannot be subclassed from the above `GivenModel` class if we want to be able to import the checkpoint files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d672371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DVAEWrapper(GivenModelClass):\n",
    "    \"Wrapper for (hawley's fork of) Zach's DiffusionDVAE\"\n",
    "    def __init__(self, \n",
    "        args_dict = {'num_quantizers':0, 'sample_size': 65536, 'demo_steps':50, 'sample_rate':48000, 'latent_dim': 64, 'pqmf_bands':1, 'ema_decay':0.995, 'num_quantizers':0},\n",
    "        debug=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if not got_dvae: return\n",
    "        class DictObj:\n",
    "            def __init__(self, in_dict:dict):\n",
    "                for key, val in in_dict.items():\n",
    "                    if isinstance(val, (list, tuple)):\n",
    "                        setattr(self, key, [DictObj(x) if isinstance(x, dict) else x for x in val])\n",
    "                    else:\n",
    "                        setattr(self, key, DictObj(val) if isinstance(val, dict) else val)\n",
    "        self.global_args = DictObj(args_dict)\n",
    "        self.model = DiffusionDVAE(self.global_args)\n",
    "        self.noise = None \n",
    "        self.demo_steps = self.global_args.demo_steps\n",
    "        self.demo_samples = self.global_args.sample_size \n",
    "        self.debug = debug\n",
    "        self.ckpt_info={'ckpt_url':'https://drive.google.com/file/d/1C3NMdQlmOcArGt1KL7pH32KtXVCOfXKr/view?usp=sharing',\n",
    "                        'ckpt_hash':'6a304c3e89ea3f7ca023f4c9accc5df8de0504595db41961cc7e8b0d07876ef5',\n",
    "                        'gdrive_path':'MyDrive/AI/checkpoints/DiffusionDVAE.ckpt',\n",
    "                        'ckpt_path':'~/checkpoints/dvae_checkpoint.ckpt'}\n",
    "    \n",
    "    def encode_it(self, demo_reals):\n",
    "        module = self.model\n",
    "        encoder_input = demo_reals\n",
    "\n",
    "        if module.pqmf_bands > 1:\n",
    "            encoder_input = module.pqmf(demo_reals).to(demo_reals.device)\n",
    "\n",
    "        noise = torch.randn([demo_reals.shape[0], 2, self.demo_samples]).to(encoder_input.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = module.encoder_ema(encoder_input)\n",
    "            if module.quantized:\n",
    "                if debug: print(\"Hey, did you know you're quantized? \")\n",
    "                #Rearrange for Memcodes\n",
    "                embeddings = rearrange(embeddings, 'b d n -> b n d')\n",
    "                embeddings, _= module.quantizer_ema(embeddings)\n",
    "                embeddings = rearrange(embeddings, 'b n d -> b d n')\n",
    "        \n",
    "        embeddings = torch.tanh(embeddings)\n",
    "        return embeddings, noise\n",
    "        \n",
    "    def encode(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        self.orig_shape = waveform.shape\n",
    "        self.demo_samples = waveform.shape[-1]\n",
    "        reps, self.noise = self.model.encode_it(waveform)\n",
    "        return reps\n",
    "\n",
    "    def decode(self, reps: torch.Tensor, demo_steps=None) -> torch.Tensor:\n",
    "        #print(\"reps.shape, self.noise.shape = \",reps.shape, self.noise.shape)\n",
    "        if demo_steps is None: demo_steps=self.demo_steps\n",
    "        fake_batches = dvae_sample(self.model.diffusion_ema, self.noise, demo_steps, 0, reps)\n",
    "        recon = rearrange(fake_batches, 'b d n -> d (b n)') # Put the demos together\n",
    "        return recon\n",
    "    \n",
    "    def setup(self, gdrive=True):  \n",
    "        ckpt_file = self.ckpt_info['ckpt_path']\n",
    "        print(f\"DVAE: attempting to load checkpoint {ckpt_file}\")\n",
    "        self.get_checkpoint(gdrive=gdrive)\n",
    "        try:\n",
    "            self.model = self.model.load_from_checkpoint(ckpt_file, global_args=self.global_args)\n",
    "        except Exception as e:\n",
    "            print(f\"Sorry, exception = {e}. Going with random weights\")\n",
    "        self.model.encode_it = self.encode_it\n",
    "        self.model.quantized = self.global_args.num_quantizers > 0 \n",
    "        self.model.eval() # disable randomness, dropout, etc...\n",
    "        freeze(self.model)  # freeze the weights for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e79027-9518-48b6-a34b-f1a3f5f72974",
   "metadata": {},
   "source": [
    "## Stacked DiffAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871a7f5-5500-42a9-a01e-6fd304235c16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ad212-6e9f-40fb-bf35-2a4cbbd4841d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export \n",
    "class StackedDiffAEWrapper(GivenModelClass):\n",
    "    \"Wrapper for (hawley's fork of) Zach's Stacked Latent DiffAE model\"\n",
    "    def __init__(self, \n",
    "        debug=True,\n",
    "        first_stage_config=None,\n",
    "        ckpt_info=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "\n",
    "        self.first_stage_config = first_stage_config if first_stage_config is not None else \\\n",
    "            {\"capacity\": 64, \"c_mults\": [2, 4, 8, 16, 32], \"strides\": [2, 2, 2, 2, 2], \"latent_dim\": 32}\n",
    "        self.first_stage_autoencoder = AudioAutoencoder( **self.first_stage_config ).requires_grad_(False)\n",
    "        self.model = LatentAudioDiffusionAutoencoder(autoencoder=self.first_stage_autoencoder)\n",
    "        self.latent_dim = self.model.latent_dim \n",
    "        self.latent_downsampling_ratio = self.model.latent_downsampling_ratio\n",
    "        \n",
    "        self.ckpt_info = ckpt_info if ckpt_info is not None else \\\n",
    "            {'ckpt_path':'~/checkpoints/stacked-diffae-more-310k.ckpt',\n",
    "            'ckpt_hash':'91f33839ecb6e3c41b1e89e1a9e0de0dac2ebe1795efa034797429c202600a58', # old PyL version btw\n",
    "            'ckpt_url':'', 'gdrive_path':''}\n",
    "        \n",
    "    def encode(self, reals: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model.encode(reals) # returns a (coarsest) single stage of reps\n",
    "    \n",
    "    def decode(self, reps: torch.Tensor, steps=100) -> torch.Tensor:\n",
    "        \"goes all the way from small embeddings (latents) to larger embeddings to audio waveform output\"\n",
    "        return self.model.decode(reps, steps=steps) \n",
    " \n",
    "    # hawley added the following two functions to 'get inside'\n",
    "    def decode_stage1to2(self, small_reps:torch.Tensor, steps=100)-> torch.Tensor:\n",
    "        \"takes output of encode (small embeddings) and produces next stage (large) embeddings\"\n",
    "        latents = small_reps \n",
    "        first_stage_latent_noise = torch.randn([latents.shape[0], self.latent_dim, latents.shape[2]*self.latent_downsampling_ratio]).to(small_reps.device)\n",
    "        first_stage_sampled = stacked_sample(self.model.diffusion, first_stage_latent_noise, steps, 0, cond=latents)\n",
    "        large_reps = first_stage_sampled\n",
    "        return large_reps\n",
    " \n",
    "    def decode_stage2(self, first_stage_sampled, steps=100) -> torch.Tensor: \n",
    "        \"goes from larger reps to audio output \"\n",
    "        return self.model.autoencoder.decode(first_stage_sampled)\n",
    "\n",
    "    def setup(self, gdrive=True):  \n",
    "        ckpt_file = self.ckpt_info['ckpt_path']\n",
    "        print(f\"{self.__class__.__name__}: attempting to load checkpoint {ckpt_file}\")\n",
    "        self.get_checkpoint(gdrive=gdrive)\n",
    "        try:\n",
    "            self.model = LatentAudioDiffusionAutoencoder.load_from_checkpoint(self.ckpt_info['ckpt_path'], \n",
    "                            autoencoder=self.first_stage_autoencoder, strict=True).requires_grad_(False)\n",
    "        except Exception as e:\n",
    "            print(f\"Sorry, exception = {e}. Going with random weights\")\n",
    "            \n",
    "        self.model.diffusion = self.model.diffusion_ema\n",
    "        self.model.latent_encoder = self.model.latent_encoder_ema\n",
    "        del self.model.diffusion_ema\n",
    "        del self.model.latent_encoder_ema\n",
    "        #self.model.encode_it = self.encode_it\n",
    "        #self.model.quantized = self.global_args.num_quantizers > 0 \n",
    "        self.model.eval() # disable randomness, dropout, etc...\n",
    "        freeze(self.model)  # freeze the weights for inference\n",
    "        print(f\"{self.__class__.__name__}: Setup completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7edf462-b638-435b-aa18-1c1e0154b10d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quick check\n",
    "device=get_device()\n",
    "sdae_model = StackedDiffAEWrapper()\n",
    "sdae_model.setup()\n",
    "sdae_model = sdae_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d7da6-77c8-4be9-a98c-f62e5b4a780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "print(\"waveform.shape =      \",waveform.shape)\n",
    "\n",
    "waveform_pad = given_model.zero_pad_po2(waveform)\n",
    "print(\"waveform_pad.shape =  \",waveform_pad.shape)\n",
    "\n",
    "# optional: to batch or not to batch?\n",
    "waveform_batch = waveform_pad.unsqueeze(0)\n",
    "print(f\"waveform_batch.shape = {waveform_batch.shape}, dtype = {waveform.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e8ba78-ff60-4cee-bd97-e23d12e04cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = sdae_model.encode(waveform_batch.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964eb7af-df1e-4959-ab99-095267a55a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "reps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5641d66b-e235-4374-9bdd-886cb0bf15c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.min(reps), torch.max(reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5883653-215a-4afa-8825-a98487d0a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon = sdae_model.decode(reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f925feb2-02ec-4bb8-a888-ab761199f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b19274",
   "metadata": {},
   "outputs": [],
   "source": [
    "reps2 = sdae_model.decode_stage1to2(reps)\n",
    "reps2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c8159",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon2 = sdae_model.decode_stage2(reps2)\n",
    "recon2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf675e-23ad-4bbc-84de-9c2f489e2b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon = recon.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eea4dd6-fb64-4e7d-83da-2a19530ee129",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ebf947-c024-4d15-a0ab-1a387d6e1c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(recon.cpu(), rate=48000))\n",
    "diff = waveform_pad - recon.cpu()\n",
    "for thing,name in zip([waveform, recon.cpu(), diff], ['input','recon','diff']):\n",
    "    plt.plot(thing[0,:].numpy(), alpha=0.5, label=name) # just left channel for now\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffae512",
   "metadata": {},
   "source": [
    "## @zaptrem's 1D diffusion unet\n",
    "\n",
    "Note that several of the routines imported require zaptrem's special fork of Flavio Schneider's repo(s): \n",
    "```bash\n",
    "pip install -U git+https://github.com/Sonauto/audio-diffusion-pytorch.git git+https://github.com/Sonauto/audio-encoders-pytorch.git https://github.com/Sonauto/a-unet/archive/tiled-attention.zip pyloudnorm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7774b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "\"\"\"\n",
    "# this is incompatible with the version of audio_diffusion_pytorch we need for Zach's utilities\n",
    "from audio_diffusion_pytorch.components import (\n",
    "    UNetV0,\n",
    "    LTPlugin,\n",
    ")\n",
    "from audio_diffusion_pytorch.models import DiffusionAE\n",
    "import pyloudnorm as pyln\n",
    "from torch import nn\n",
    "from audio_encoders_pytorch import TanhBottleneck, MelE1d\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c6fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DMAE1d(GivenModelClass):\n",
    "    def __init__(self, debug=False):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "        self.ckpt_info={'ckpt_url':'https://drive.google.com/file/d/1KKwPbM_Qmu5QvpAs3DdRaYlkaRTG-WJv/view?usp=share_link',\n",
    "                        'ckpt_path':'~/checkpoints/dmae1d_checkpoint.ckpt',\n",
    "                        'ckpt_hash':'a11a9c68e5962830b142202e25b3080f553a3a73cd944225b3c7d21fe8c631e9'}\n",
    "        self.resample_encode = T.Resample(48000, 44100)\n",
    "        self.resample_decode = T.Resample(44100, 48000)\n",
    "\n",
    "        UNet = LTPlugin(\n",
    "            UNetV0,\n",
    "            num_filters=128,\n",
    "            window_length=128,\n",
    "            stride=64,\n",
    "        )\n",
    "\n",
    "        self.model = DiffusionAE(\n",
    "            net_t=UNet,\n",
    "            dim=1, \n",
    "            in_channels=2,\n",
    "            channels=[256, 512, 512, 512, 1024, 1024, 1024],\n",
    "            factors=[1, 2, 2, 2, 2, 2, 2],\n",
    "            linear_attentions=[0, 1, 1, 1, 1, 1, 1],\n",
    "            attention_features=64,\n",
    "            attention_heads=8,\n",
    "            items=[1, 2, 2, 2, 2, 2, 2],\n",
    "            encoder=MelE1d(\n",
    "                in_channels=2,\n",
    "                channels=512,\n",
    "                multipliers=[1, 1, 1],\n",
    "                factors=[2, 2],\n",
    "                num_blocks=[4, 8],\n",
    "                mel_channels=80,\n",
    "                mel_sample_rate=44100,\n",
    "                mel_normalize_log=True,\n",
    "                out_channels=32,\n",
    "                bottleneck=TanhBottleneck()\n",
    "            ),\n",
    "            inject_depth=4\n",
    "        )\n",
    "\n",
    "    def forward(self, waveform_in, *args, **kwargs):\n",
    "        self.orig_shape = waveform_in.shape\n",
    "        waveform = self.zero_pad_po2(self.resample_encode(waveform_in))\n",
    "        waveform_out = self.model(waveform, *args, **kwargs)\n",
    "        return self.match_sizes(self.resample_decode(waveform_out))\n",
    "\n",
    "    def encode(self, waveform_in, *args, **kwargs):\n",
    "        self.orig_shape = waveform_in.shape\n",
    "        waveform = self.zero_pad_po2(self.resample_encode(waveform_in))\n",
    "        return self.model.encode(waveform, *args, **kwargs)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def decode(self, *args, **kwargs):\n",
    "        waveform_out = self.model.decode(*args, num_steps=50, show_progress=True, **kwargs)\n",
    "        return self.match_sizes(self.resample_decode(waveform_out))\n",
    "    \n",
    "    def setup(self, gdrive=True):  \n",
    "        ckpt_file = os.path.expanduser(self.ckpt_info['ckpt_path'])\n",
    "        print(f\"{self.__class__.__name__}: attempting to load checkpoint {ckpt_file}\")\n",
    "        self.get_checkpoint(gdrive=gdrive)\n",
    "        try:\n",
    "            #self.model = self.model.load_from_checkpoint(ckpt_file, global_args=self.global_args)\n",
    "            ae_checkpoint = torch.load(ckpt_file, map_location=torch.device('cpu'))\n",
    "            # Load the model and optimizer state from the checkpoint\n",
    "            self.load_state_dict(ae_checkpoint[\"model_state_dict\"], strict=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Sorry, exception = {e}. Going with random weights\")\n",
    "        self.model.eval() # disable randomness, dropout, etc...\n",
    "        freeze(self.model)  # freeze the weights for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603aea1",
   "metadata": {},
   "source": [
    "## Test our autoencoder options\n",
    "First prepare a waveform and instantiate the various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5796904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "print(\"waveform.shape =      \",waveform.shape)\n",
    "\n",
    "waveform_pad = given_model.zero_pad_po2(waveform)\n",
    "print(\"waveform_pad.shape =  \",waveform_pad.shape)\n",
    "\n",
    "# optional: to batch or not to batch?\n",
    "waveform_batch = waveform_pad.unsqueeze(0)\n",
    "print(f\"waveform_batch.shape = {waveform_batch.shape}, dtype = {waveform.dtype}\")\n",
    "\n",
    "given_models = [ StackedDiffAEWrapper(), SpectrogramAE(), MagSpectrogramAE(), MagDPhaseSpectrogramAE(),  DVAEWrapper()]\n",
    "                #, DMAE1d(),] # having trouble with missing keys in DMAE1d\n",
    "_ = [x.setup() for x in given_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfc2709",
   "metadata": {},
   "source": [
    "Now run the waveform through the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fec131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "for given_model in given_models:\n",
    "    display(HTML('<hr>'))\n",
    "    print(\"given_model.name = \",given_model.name)\n",
    "    given_model = given_model.to(device)\n",
    "    reps = given_model.encode(waveform_batch.to(device))\n",
    "    recon = given_model.decode(reps) \n",
    "    recon = recon.squeeze()\n",
    "    if len(reps.shape) < 4: reps = reps.unsqueeze(0) # for viz purposes\n",
    "    print(f\"For model {given_model.name}, reps.shape = {reps.shape} and dtype = {reps.dtype}. recon.shape = {recon.shape}\")\n",
    "    if given_model.name in ['DVAEWrapper', 'DMAE1d', 'StackedDiffAEWrapper']:\n",
    "        title, cmap = 'Embeddings', 'coolwarm'\n",
    "        vals = reps[:,0,:,:]\n",
    "    else:\n",
    "        title, cmap =f'{given_model.name}: 10Log10(abs(Embeddings)**2)', 'viridis'\n",
    "        vals = 10*torch.log10(torch.abs(reps[:,0,:,:])**2+1e-6)\n",
    "\n",
    "    #print(\"vals.shape =\",vals.shape)\n",
    "    display(Audio(recon.cpu(), rate=48000))\n",
    "    display(tokens_spectrogram_image(vals.cpu(), title=title, mark_batches=True, symmetric=False, cmap=cmap))\n",
    "    #THIS NEXT LINE MAKES notebook filesize huge: \n",
    "    #display(playable_spectrogram(recon.cpu(), specs=\"all\", output_type='live'))\n",
    "    diff = waveform_pad - recon.cpu()\n",
    "    for thing,name in zip([waveform, recon.cpu(), diff], ['input','recon','diff']):\n",
    "        plt.plot(thing[0,:].numpy(), alpha=0.5, label=name) # just left channel for now\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aabfa53",
   "metadata": {},
   "source": [
    "# Variational AutoEncoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9f534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RAVEWrapper(GivenModelClass):\n",
    "    \"Wrapper for RAVE\"\n",
    "    def __init__(self,\n",
    "        pretrained_name='',\n",
    "        checkpoint_file='percussion',\n",
    "        config_path='./v2.gin',  # this probably gets ignored\n",
    "        debug=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config_path = config_path\n",
    "        if not got_rave: return \n",
    "        if Path(checkpoint_file).suffix == '': checkpoint_file +='.ts'\n",
    "        self.debug = debug\n",
    "        self.ckpt_info={'ckpt_url':'https://play.forum.ircam.fr/rave-vst-api/get_model',\n",
    "                        'ckpt_hash':'',\n",
    "                        'gdrive_path':'',\n",
    "                        'ckpt_path':f'{self.ckpt_dir}/{checkpoint_file}'}  \n",
    "        self.ckpt_info['ckpt_url'] += \"/\"+Path(checkpoint_file).stem\n",
    "        gin.parse_config_file(self.config_path)\n",
    "        self.model = rave.RAVE()\n",
    "        self.model.eval()\n",
    "   \n",
    "    def setup(self, gdrive=False):\n",
    "        \"Setup can include things such as downloading checkpoints\"\n",
    "        self.get_checkpoint(gdrive=gdrive)\n",
    "        extension = Path(self.ckpt_info['ckpt_path']).suffix\n",
    "        if self.debug: print(\"extension =\",extension)\n",
    "        if extension == '.ts' or extension=='':\n",
    "            self.model = torch.jit.load(self.ckpt_info['ckpt_path'])\n",
    "        elif extension == '.ckpt':\n",
    "            self.model.load_state_dict(torch.load(self.ckpt_info['ckpt_path'])[\"state_dict\"])\n",
    "        else:\n",
    "            print(f\"Sorry, we don't know how to load {extension} checkpoint files. Weights will be uninitialized.\")\n",
    "\n",
    "    def encode(self, waveform: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            return self.model.encode(waveform)\n",
    "    def decode(self, reps: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            return self.model.decode(reps) \n",
    "    \n",
    "    def forward(self, waveform: torch.Tensor)-> (torch.Tensor, torch.Tensor):\n",
    "        \"Calls .encode() and .decode() in succession, returns both results as tuple\"\n",
    "        reps = self.encode(waveform)\n",
    "        recons = self.decode(reps)\n",
    "        return (reps, recons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209632ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "if got_rave: \n",
    "    given_model = RAVEWrapper(debug=True)\n",
    "    given_model.setup()\n",
    "\n",
    "    waveform = load_audio(data_path+'stereo_pewpew.mp3')\n",
    "    x = 1.5*waveform.clone()[0,:].unsqueeze(0).unsqueeze(0)\n",
    "    print(\"x.shape = \",x.shape)\n",
    "    playable_spectrogram(x[0],output_type='live')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64afe4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "if got_rave: \n",
    "    z = given_model.encode(x)\n",
    "    print(\"z.shape =\",z.shape)\n",
    "    tokens_spectrogram_image(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3364961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "if got_rave:\n",
    "    show_point_cloud(z, mode='lines+markers', method='umap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe01457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "if got_rave:\n",
    "    y = given_model.decode(z)\n",
    "    print(\"y.shape =\",y.shape)\n",
    "    playable_spectrogram(y[0], output_type='live')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1460c92-1b19-4ccc-9c77-d54f3dd67f4d",
   "metadata": {},
   "source": [
    "# CLAP embeddings & (lossy) decoding via diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990a822d-fd13-4732-ae08-95ecf5e90519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "class CLAPDAE(GivenModelClass):\n",
    "    \"the decoder side of this is the 'demo' side of train_stacked_latent_clap_audio_all_wds.py\"\n",
    "    def __init__(self, \n",
    "            clap_fusion=True, \n",
    "            clap_amodel='HTSAT-base', \n",
    "            device='cuda', \n",
    "            first_stage_config = {\"capacity\": 64, \"c_mults\": [2, 4, 8, 16, 32], \"strides\": [2, 2, 2, 2, 2], \"latent_dim\": 32},\n",
    "            sample_size = 1048576,\n",
    "            debug=True):\n",
    "        super().__init__()\n",
    "        self.device, self.debug  = device, debug\n",
    "        self.sample_size, self.demo_samples = sample_size, sample_size\n",
    "        # putting everything in setup(), below...\n",
    "        \n",
    "        #self.clap_ckpt_info={'ckpt_url':'',  # for decoder\n",
    "        #                'ckpt_path':'~/checkpoints/CLAP/properties_paths_base_epoch_90.pt',\n",
    "        #                'ckpt_hash':''}\n",
    "\n",
    "        #self.clap_module = laion_clap.CLAP_Module(enable_fusion=clap_fusion, device=device, amodel=clap_amodel).requires_grad_(False).eval()\n",
    "        #self.embedder, self.encoder = self.clap_module, self.clap_module # synonyms ;-) \n",
    "        \n",
    "        # Zach's stacked conditional latent diff ae\n",
    "        #self.latent_diffae_ckpt_info = {}\n",
    "        #self.first_stage_config = first_stage_config\n",
    "        #self.first_stage_autoencoder = AudioAutoencoder( **self.first_stage_config ).eval()  \n",
    "        #self.latent_diffae_ckpt_info = {'ckpt_path':'~/checkpoints/stacked-diffae/stacked-diffae-more-310k.ckpt'}\n",
    "        #self.latent_diffae = LatentAudioDiffusionAutoencoder(self.first_stage_autoencoder).eval()\n",
    "        \n",
    "        \n",
    "        #self.saeldc_ckpt_info = {\n",
    "        #    'ckpt_path':'~/checkpoints/longer_songlike_22s.ckpt',\n",
    "        #}\n",
    "        #self.latent_diffusion_model = StackedAELatentDiffusionCond(latent_ae=self.latent_diffae, clap_module=self.clap_module)\n",
    "\n",
    "\n",
    "        self.already_setup = False\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed(self, x, *args, **kwargs): # for audio embeddings\n",
    "        \"\"\"note that CLAP will embed audio of *arbitrary length* to a single 512-dim vector,\n",
    "           however our decoder is trained to produce a certain length, so we batchify on encoding\n",
    "        \"\"\"\n",
    "        module = self.latent_diffusion_model\n",
    "        \n",
    "        if isinstance(x, str):  # get text embeddings\n",
    "            print(\" embed: got text\") \n",
    "            embeddings = self.clap_module.get_text_embedding([x, \"\"], use_tensor=True)\n",
    "        else:                   # get audio embeddings\n",
    "            demo_reals = x\n",
    "            if self.debug: print(\"      demo_reals .shape, dtype =\",demo_reals.shape, demo_reals.dtype)\n",
    "            while len(demo_reals.shape) < 3: \n",
    "                demo_reals = demo_reals.unsqueeze(0) # add batch and/or channel dims \n",
    "            embeddings = module.embedder.get_audio_embedding_from_data(demo_reals.mean(dim=1).to(module.device), use_tensor=True).to(demo_reals.dtype)\n",
    "               \n",
    "        embeddings = embeddings.unsqueeze(1)  # the way my routines like it, so dims will be [1,1,512]    \n",
    "        return embeddings\n",
    "    \n",
    "    def encode(self, demo_reals, *args, **kwargs): # synonym for embed\n",
    "        return embed(demo_reals, *args, **kwargs)\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, \n",
    "               audio_embeddings, # outputs from 'encode'\n",
    "               cfg_scales=4,  \n",
    "               demo_steps=150,\n",
    "               init_audio=None,\n",
    "               init_strength=0.4,\n",
    "               batch_size=1,\n",
    "               **kwargs):\n",
    "        embeddings = audio_embeddings.to(self.device)\n",
    "        module = self.latent_diffusion_model\n",
    "        self.latent_noise = torch.randn([batch_size, module.latent_dim, self.demo_samples//module.downsampling_ratio], dtype=audio_embeddings.dtype, device=module.device) \n",
    "\n",
    "        latent_noise = self.latent_noise\n",
    "        fakes_list = []\n",
    "        if type(cfg_scales) != list: cfg_scales = [cfg_scales]\n",
    "        for cfg_scale in cfg_scales:\n",
    "            print(f\"Generating latents, CFG scale {cfg_scale}\")\n",
    "            if init_audio is not None:\n",
    "                fake_latents = ldc_resample(module.diffusion_ema, init_audio, demo_steps, 0, embedding=embeddings, embedding_scale=cfg_scale,  noise_level=(1.0-init_strength))\n",
    "            else:\n",
    "                fake_latents = ldc_sample(module.diffusion_ema, latent_noise, demo_steps, 0, embedding=embeddings, embedding_scale=cfg_scale)\n",
    "\n",
    "            fake_latents = fake_latents.clamp(-1, 1)\n",
    "            \n",
    "            print(f\"Decoding fake_latents of shape {fake_latents.shape} to audio length {self.sample_size}\")\n",
    "            fakes = module.decode(fake_latents, steps=100)# , init_audio=init_audio, init_strength=init_strength)\n",
    "            print(\"Rearranging demos\")\n",
    "            fakes = rearrange(fakes, 'b d n -> d (b n)')\n",
    "            print(\"   fakes.shape = \",fakes.shape)\n",
    "            fakes_list.append(fakes)\n",
    "        #return fakes_list if len(cfg_scales)>1 else fakes_list[0]\n",
    "        return fakes, fake_latents # quick hack\n",
    "    \n",
    "    def decode(self, *args, **kwargs): \n",
    "        return self.generate(*args, **kwargs) \n",
    "    \n",
    "    \n",
    "    def forward(self, waveform_in, *args, **kwargs):\n",
    "        embeddings = self.encode(waveform_in, *args, **kwargs) \n",
    "        fakes = self.decode(embeddings, **kwargs)\n",
    "        return fakes\n",
    "\n",
    "    @torch.no_grad()    \n",
    "    def setup(self, gdrive=True):  \n",
    "        if self.already_setup: return\n",
    "        \"\"\"wasn't working. trying this instead\n",
    "        # CLAP part\n",
    "        clap_ckpt_file = os.path.expanduser(self.clap_ckpt_info['ckpt_path'])\n",
    "        print(f\"{self.__class__.__name__}: attempting to load CLAP checkpoint {clap_ckpt_file}\")\n",
    "        if clap_ckpt_file:\n",
    "            try:\n",
    "                self.clap_module.load_ckpt(ckpt=clap_ckpt_file, verbose=False)\n",
    "            except TypeError:\n",
    "                print(\"  PRO TIP: For fewer loading messages, do `pip install git+https://github.com/drscotthawley/CLAP.git`\",flush=True)\n",
    "                self.clap_module.load_ckpt(ckpt=clap_ckpt_file)\n",
    "            except Exception as e:\n",
    "                print(\"Exception {e}. Unable to load checkpoint as specified. Starting from scratch\")\n",
    "                self.clap_module.load_ckpt(model_id=1)\n",
    "        else:\n",
    "            print(\"No checkpoint specified. Starting from scratch\")\n",
    "            self.clap_module.load_ckpt(model_id=1) # no checkpoint found\n",
    "        self.clap_module = self.clap_module.to(self.device)\n",
    "        freeze(self.clap_module)\n",
    "        \n",
    "        # diffusion part \n",
    "        latent_diffae_ckpt_path = self.latent_diffae_ckpt_info['ckpt_path']\n",
    "        print(f\"{self.__class__.__name__}: attempting to load latent_diffae checkpoint {latent_diffae_ckpt_path}\")\n",
    "        self.latent_diffae = LatentAudioDiffusionAutoencoder.load_from_checkpoint(latent_diffae_ckpt_path, autoencoder=self.first_stage_autoencoder, strict=False).eval()\n",
    "        self.latent_diffae = self.latent_diffae.to(self.device) \n",
    "        self.latent_diffae.diffusion = self.latent_diffae.diffusion_ema\n",
    "        del self.latent_diffae.diffusion_ema\n",
    "        self.latent_diffae.latent_encoder = self.latent_diffae.latent_encoder_ema\n",
    "        del self.latent_diffae.latent_encoder_ema\n",
    "    \n",
    "        latent_diffusion_model_ckpt_path = self.saeldc_ckpt_info['ckpt_path']\n",
    "        print(f\"{self.__class__.__name__}: attempting to load StackedAELatentDiffusionCond checkpoint {latent_diffusion_model_ckpt_path}\")\n",
    "        self.latent_diffusion_model = StackedAELatentDiffusionCond.load_from_checkpoint(\n",
    "                                        latent_diffusion_model_ckpt_path, \n",
    "                                        latent_ae=self.latent_diffae, \n",
    "                                        clap_module=self.clap_module, \n",
    "                                        strict=False ).eval()\n",
    "        self.latent_diffusion_model = self.latent_diffusion_model.to(self.device)\"\"\"\n",
    "        \n",
    "        ###### ----- pasting directy from training code and adding 'self'\n",
    "        \n",
    "        print(\"\\n =============  Setting up StackedAELatentCond using code pasted from train script... ===============\")\n",
    "        first_stage_config = {\"capacity\": 64, \"c_mults\": [2, 4, 8, 16, 32], \"strides\": [2, 2, 2, 2, 2], \"latent_dim\": 32}\n",
    "\n",
    "        self.first_stage_autoencoder = AudioAutoencoder(\n",
    "            **first_stage_config\n",
    "        ).eval()\n",
    "\n",
    "        # checkpoint for pretrained stage 1 autoencoder\n",
    "        pretrained_ckpt_path = \"/fsx/shawley/checkpoints/stacked-diffae/stacked-diffae-more-310k.ckpt\"\n",
    "        self.latent_diffae = LatentAudioDiffusionAutoencoder.load_from_checkpoint(pretrained_ckpt_path, autoencoder=self.first_stage_autoencoder, strict=False).eval()\n",
    "\n",
    "        self.latent_diffae.diffusion = self.latent_diffae.diffusion_ema\n",
    "        del self.latent_diffae.diffusion_ema\n",
    "\n",
    "        self.latent_diffae.latent_encoder = self.latent_diffae.latent_encoder_ema\n",
    "        del self.latent_diffae.latent_encoder_ema\n",
    "\n",
    "        # clap uses pretrained checkpoints\n",
    "        clap_fusion = True\n",
    "        clap_amodel = \"HTSAT-base\"\n",
    "        \n",
    "        self.clap_module = laion_clap.CLAP_Module(enable_fusion=clap_fusion, device=self.device, amodel=clap_amodel).requires_grad_(False).eval()\n",
    "        clap_ckpt_path = \"/fsx/shawley/checkpoints/CLAP/properties_paths_base_epoch_90.pt\"\n",
    "        if clap_ckpt_path:\n",
    "            self.clap_module.load_ckpt(ckpt=clap_ckpt_path, verbose=False)\n",
    "        else:\n",
    "            self.clap_module.load_ckpt(model_id=1, verbose=False)\n",
    "\n",
    "        ckpt_path = \"/fsx/shawley/checkpoints/longer_songlike_22s.ckpt\"\n",
    "        if ckpt_path:\n",
    "            print(f\"Loading StackedAELatentDiffusionCond from {ckpt_path}\")\n",
    "            self.latent_diffusion_model = StackedAELatentDiffusionCond.load_from_checkpoint(ckpt_path, latent_ae=self.latent_diffae, clap_module=self.clap_module, strict=False)\n",
    "        else:\n",
    "            self.latent_diffusion_model = StackedAELatentDiffusionCond(latent_ae=self.latent_diffae, clap_module=self.clap_module)\n",
    "        self.latent_diffusion_model = self.latent_diffusion_model.to(self.device)\n",
    "        \n",
    "        self.embedder, self.encoder = self.clap_module, self.clap_module # synonyms ;-) \n",
    "        print(f\"Success! All checkpoints loaded. {self.__class__.__name__} is ready to go.\")\n",
    "        self.already_setup = True\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1536697-99a1-434d-91b2-26e3c182df5a",
   "metadata": {},
   "source": [
    "Test that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040af32f-69b4-45e4-aa1f-7017bee673a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clapper = CLAPDAE(device=device)\n",
    "clapper.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6de602-312d-4e89-890c-2aa428a0867f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lets make sure audio will fit first \n",
    "waveform_in = waveform\n",
    "if True: # the encoder will expand and batchify if needed, but we'll do it ahead of time for comparison\n",
    "    sample_size = clapper.sample_size\n",
    "    waveform_in = torch.zeros(2, sample_size)\n",
    "    minlen = min(sample_size, waveform.shape[-1])\n",
    "    waveform_in[:, :minlen] = waveform[:, :minlen]\n",
    "nb_play(waveform_in) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d2c980-b0f4-49ef-ab51-91a88bb1a921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_embeds = clapper.encode(waveform_in)\n",
    "audio_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d52ce67-857d-4d6d-abba-96b8065fccc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "waveform_out = clapper.decode(audio_embeds).cpu()\n",
    "waveform_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b51552-f7f3-4455-bd9a-c102df736c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb_play(waveform_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94704e75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9763d28e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "aa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
